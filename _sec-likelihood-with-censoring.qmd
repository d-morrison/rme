{{< include macros.qmd >}}

If an event time $T$ is observed exactly as $T=t$, then the likelihood
of that observation is just its probability density function:

$$
\begin{aligned}
\Lik(t)
&= \pdf(T=t)\\
&\eqdef  \pdf_T(t)\\
&= \haz_T(t)\surv_T(t)\\
\ell(t)
&\eqdef \logf{\Lik(t)}\\
&= \logf{\haz_T(t) \surv_T(t)}\\
&= \logf{\haz_T(t)} + \logf{\surv_T(t)}\\
&= \logf{\haz_T(t)} - \cuhaz_T(t)\\
\end{aligned}
$$

---

If instead the event time $T$ is censored and only known to be after
time $y$, then the likelihood of that censored observation is instead
the survival function evaluated at the censoring time:

$$
\begin{aligned}
\Lik(y)
&=p_T(T>y)\\
&\eqdef  \surv_T(y)\\
\ell(y)
&\eqdef \logf{\Lik(y)}\\
&=\logf{\surv(y)}\\
&=-\cuhaz(y)\\
\end{aligned}
$$

---

::: notes
What's written above is incomplete. We also observed whether or not the
observation was censored. Let $C$ denote the time when censoring would
occur (if the event did not occur first); let $f_C(y)$ and $S_C(y)$ be
the corresponding density and survival functions for the censoring
event.

Let $Y$ denote the time when observation ended (either by censoring or
by the event of interest occurring), and let $D$ be an indicator
variable for the event occurring at $Y$ (so $D=0$ represents a censored
observation and $D=1$ represents an uncensored observation). In other
words, let $Y \eqdef  \min(T,C)$ and
$D \eqdef  \mathbb 1{\{T<=C\}}$.

Then the complete likelihood of the observed data $(Y,D)$ is:
:::

$$
\begin{aligned}
\Lik(y,d)
&= \p(Y=y, D=d)\\
&= \sb{\p(T=y,C> y)}^d \cdot \sb{\p(T>y,C=y)}^{1-d}\\
\end{aligned}
$$

---

::: notes
Typically, survival analyses assume that $C$ and $T$ are mutually
independent; this assumption is called "non-informative" censoring.

Then the joint likelihood $\p(Y,D)$ factors into the product
$\p(Y), \p(D)$, and the likelihood reduces to:
:::

$$
\begin{aligned}
\Lik(y,d)
&= \sb{\p(T=y,C> y)}^d\cdot
\sb{\p(T>y,C=y)}^{1-d}\\
&= \sb{\p(T=y)\p(C> y)}^d\cdot
\sb{\p(T>y)\p(C=y)}^{1-d}\\
&= \sb{\pdf_T(y)\surv_C(y)}^d\cdot
\sb{\surv(y)\pdf_C(y)}^{1-d}\\
&= \sb{\pdf_T(y)^d \surv_C(y)^d}\cdot
\sb{\surv_T(y)^{1-d} \pdf_C(y)^{1-d}}\\
&= \paren{\pdf_T(y)^d \cdot \surv_T(y)^{1-d}}\cdot
\paren{\pdf_C(y)^{1-d} \cdot \surv_C(y)^{d}}
\end{aligned}
$$

---

::: notes
The corresponding log-likelihood is:
:::

$$
\begin{aligned}
\ell(y,d)
&= \logf{\Lik(y,d)}\\
&= \logf{
\paren{f_T(y)^d \cdot S_T(y)^{1-d}}
\cdot
\paren{f_C(y)^{1-d} \cdot S_C(y)^{d}}
}
\\
&= \logf{f_T(y)^d \cdot S_T(y)^{1-d}}
+
\logf{f_C(y)^{1-d} \cdot S_C(y)^{d}}
\end{aligned}
$$ Let

-   $\theta_T$ represent the parameters of $p_T(t)$,
-   $\theta_C$ represent the parameters of $p_C(c)$,
-   $\theta = (\theta_T, \theta_C)$ be the combined vector of all
parameters.

---

::: notes
The corresponding score function is:
:::

$$
\begin{aligned}
\ell'(y,d)
&= \deriv{\theta}
\sb{
\logf{f_T(y)^d \cdot S_T(y)^{1-d}}
+
\logf{f_C(y)^{1-d} \cdot S_C(y)^{d}}
}
\\
&=
\paren{
\deriv{\theta}
\logf{
f_T(y)^d \cdot S_T(y)^{1-d}
}
}
+
\paren{
\deriv{\theta}
\logf{
f_C(y)^{1-d} \cdot S_C(y)^{d}
}
}
\end{aligned}
$$

---

::: notes
As long as $\theta_C$ and $\theta_T$ don't share any parameters, then if
censoring is non-informative, the partial derivative with respect to
$\theta_T$ is:
:::

$$
\begin{aligned}
\ell'_{\theta_T}(y,d)
&\eqdef  \deriv{\theta_T}\ell(y,d)\\
&=
\left(
\deriv{\theta_T}
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}
\right)
+
\left(
\deriv{\theta_T}
\text{log}\left\{
f_C(y)^{1-d} \cdot S_C(y)^{d}
\right\}
\right)\\
&=
\left(
\deriv{\theta_T}
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}
\right) + 0\\
&=
\deriv{\theta_T}
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}\\
\end{aligned}
$$

---

::: notes
Thus, the MLE for $\theta_T$ won't depend on $\theta_C$, and we can
ignore the distribution of $C$ when estimating the parameters of
$f_T(t)=p(T=t)$.
:::

Then:

$$
\begin{aligned}
\Lik(y,d)
&= f_T(y)^d \cdot S_T(y)^{1-d}\\
&= \left(h_T(y)^d  S_T(y)^d\right) \cdot S_T(y)^{1-d}\\
&= h_T(y)^d  \cdot S_T(y)^d \cdot S_T(y)^{1-d}\\
&= h_T(y)^d \cdot S_T(y)\\
&= S_T(y) \cdot h_T(y)^d \\
\end{aligned}
$$

::: notes
That is, if the event occurred at time $y$ (i.e., if $d=1$), then the
likelihood of $(Y,D) = (y,d)$ is equal to the hazard function at $y$
times the survival function at $y$. Otherwise, the likelihood is equal
to just the survival function at $y$.
:::

---

::: notes
The corresponding log-likelihood is:
:::

$$
\begin{aligned}
\ell(y,d)
&=\text{log}\left\{\Lik(y,d)\right\}\\
&= \text{log}\left\{S_T(y) \cdot h_T(y)^d\right\}\\
&= \text{log}\left\{S_T(y)\right\} + \text{log}\left\{h_T(y)^d\right\}\\
&= \text{log}\left\{S_T(y)\right\} + d\cdot \text{log}\left\{h_T(y)\right\}\\
&= -H_T(y) + d\cdot \text{log}\left\{h_T(y)\right\}\\
\end{aligned}
$$

::: notes
In other words, the log-likelihood contribution from a single
observation $(Y,D) = (y,d)$ is equal to the negative cumulative hazard
at $y$, plus the log of the hazard at $y$ if the event occurred at time
$y$.
:::
