
### The Probability Density Function (PDF)  {.smaller}

{{< include _sec-pdf.qmd >}}

### The Cumulative Distribution Function (CDF)

{{< include _sec-CDF.qmd >}}

### The Survival Function

{{< include _sec-survf.qmd >}}

### The Hazard Function

{{< include _sec-haz.qmd >}}

### The Cumulative Hazard Function

{{< include _sec-cuhaz.qmd >}}

### Some Key Mathematical Relationships among Survival Concepts

#### Diagram:

$$
\haz(t) \xrightarrow[]{\int_{u=0}^t \haz(u)du} \cuhaz(t)
\xrightarrow[]{\exp{-\cuhaz(t)}} \surv(t)
\xrightarrow[]{1-\surv(t)} \cdf(t)
$$

$$
\haz(t) \xleftarrow[\deriv{t}\cuhaz(t)]{} \cuhaz(t)
\xleftarrow[-\log{\surv(t)}]{} \surv(t)
\xleftarrow[1-\cdf(t)]{} \cdf(t)
$$


---

#### Identities:

$$
\begin{aligned}
\surv(t) &= 1 - \cdf(t)\\
&= \expf{-\cuhaz(t)}\\
\surv'(t) &= -f(t)\\
\cuhaz(t) &= -\logf{\surv(t)}\\
\cuhaz'(t) &= \haz(t)\\
\haz(t) &= \frac{\pdf(t)}{\surv(t)}\\
&= -\deriv{t}\log{\surv(t)} \\
f(t) &= \haz(t)\cdot \surv(t)\\
\end{aligned}
$$

---

Some proofs (others left as exercises):

$$
\begin{aligned}
S'(t) &= \deriv{t}(1-F(t))\\
&= -F'(t)\\
&= -f(t)\\
\end{aligned}
$$

---

$$
\begin{aligned}
\deriv{t}\log{\surv(t)}
&= \frac{S'(t)}{\surv(t)}\\
&= -\frac{f(t)}{\surv(t)}\\
&= -\haz(t)\\
\end{aligned}
$$

---

$$
\begin{aligned}
\cuhaz(t)
&\eqdef \int_{u=0}^t h(u) du\\
&= \int_0^t -\deriv{u}\text{log}\left\{S(u)\right\} du\\
&= \left[-\text{log}\left\{S(u)\right\}\right]_{u=0}^{u=t}\\
&= \left[\text{log}\left\{S(u)\right\}\right]_{u=t}^{u=0}\\
&= \logf{\surv(0)} - \logf{\surv(t)}\\
&= \logf{1} - \logf{\surv(t)}\\
&= 0 - \logf{\surv(t)}\\
&=-\logf{\surv(t)}
\end{aligned}
$$

---

Corollary:

$$\surv(t) = \text{exp}\left\{-\cuhaz(t)\right\}$$

---

#### Example: Time to death the US in 2004

The first day is the most dangerous:

```{r}
#| fig-cap: "Daily Hazard Rates in 2004 for US Females"
#| fig-pos: "H"
#| fig-height: 6
#| label: fig-haz-female-US
#| echo: true

# download `survexp.rda` from:
# paste0(
# "https://github.com/therneau/survival/raw/",
# "f3ac93704949ff26e07720b56f2b18ffa8066470/",
# "Data/survexp.rda")

# (newer versions of `survival` don't have the first-year breakdown; see:
# https://cran.r-project.org/web/packages/survival/news.html)

fs::path(
  here::here(),
  "Data",
  "survexp.rda"
) |>
  load()
s1 <- survexp.us[, "female", "2004"]
age1 <- c(
  0.5 / 365.25,
  4 / 365.25,
  17.5 / 365.25,
  196.6 / 365.25,
  1:109 + 0.5
)
s2 <- 365.25 * s1[5:113]
s2 <- c(s1[1], 6 * s1[2], 22 * s1[3], 337.25 * s1[4], s2)
cols <- rep(1, 113)
cols[1] <- 2
cols[2] <- 3
cols[3] <- 4

plot(age1, s1, type = "b", lwd = 2, xlab = "Age", ylab = "Daily Hazard Rate", col = cols)

text(10, .003, "First Day", col = 2)
text(18, .00030, "Rest of First Week", col = 3)
text(18, .00015, "Rest of First month", col = 4)
```

---

:::{#exr-compare-mf}
Hypothesize why the male and female hazard functions in @fig-haz-mf differ where they do?
:::

```{r}
#| fig-cap: "Daily Hazard Rates in 2004 for US Males and Females 1-40"
#| fig-pos: "H"
#| label: fig-haz-mf
#| echo: true
yrs <- 1:40
s1 <- survexp.us[5:113, "male", "2004"]
s2 <- survexp.us[5:113, "female", "2004"]

age1 <- 1:109

plot(age1[yrs], s1[yrs], type = "l", lwd = 2, xlab = "Age", ylab = "Daily Hazard Rate")
lines(age1[yrs], s2[yrs], col = 2, lwd = 2)
legend(5, 5e-6, c("Males", "Females"), col = 1:2, lwd = 2)
```

---

:::{#exr-surv-vs-haz}
Compare and contrast @fig-surv-US-females with @fig-haz-female-US.
:::

---

```{r}
#| fig-cap: "Survival Curve in 2004 for US Females"
#| label: fig-surv-US-females
#| fig-pos: "H"
#| echo: true

s1 <- survexp.us[, "female", "2004"]

s2 <- 365.25 * s1[5:113]
s2 <- c(s1[1], 6 * s1[2], 21 * s1[3], 337.25 * s1[4], s2)
cs2 <- cumsum(s2)
age2 <- c(1 / 365.25, 7 / 365.25, 28 / 365.25, 1:110)
plot(age2, exp(-cs2), type = "l", lwd = 2, xlab = "Age", ylab = "Survival")
```

---

### Likelihood with censoring

If an event time $T$ is observed exactly as $T=t$, then the likelihood
of that observation is just its probability density function:

$$
\begin{aligned}
\Lik(t)
&= p(T=t)\\
&\eqdef  f_T(t)\\
&= h_T(t)S_T(t)\\
\ell(t)
&\eqdef \text{log}\left\{\Lik(t)\right\}\\
&= \text{log}\left\{h_T(t)S_T(t)\right\}\\
&= \text{log}\left\{h_T(t)\right\} + \text{log}\left\{S_T(t)\right\}\\
&= \text{log}\left\{h_T(t)\right\} - H_T(t)\\
\end{aligned}
$$

---

If instead the event time $T$ is censored and only known to be after
time $y$, then the likelihood of that censored observation is instead
the survival function evaluated at the censoring time:

$$
\begin{aligned}
\Lik(y)
&=p_T(T>y)\\
&\eqdef  S_T(y)\\
\ell(y)
&\eqdef \text{log}\left\{\Lik(y)\right\}\\
&=\text{log}\left\{S(y)\right\}\\
&=-H(y)\\
\end{aligned}
$$

---

::: notes
What's written above is incomplete. We also observed whether or not the
observation was censored. Let $C$ denote the time when censoring would
occur (if the event did not occur first); let $f_C(y)$ and $S_C(y)$ be
the corresponding density and survival functions for the censoring
event.

Let $Y$ denote the time when observation ended (either by censoring or
by the event of interest occurring), and let $D$ be an indicator
variable for the event occurring at $Y$ (so $D=0$ represents a censored
observation and $D=1$ represents an uncensored observation). In other
words, let $Y \eqdef  \min(T,C)$ and
$D \eqdef  \mathbb 1{\{T<=C\}}$.

Then the complete likelihood of the observed data $(Y,D)$ is:
:::

$$
\begin{aligned}
\Lik(y,d)
&= p(Y=y, D=d)\\
&= \left[p(T=y,C> y)\right]^d \cdot
\left[p(T>y,C=y)\right]^{1-d}\\
\end{aligned}
$$

---

::: notes
Typically, survival analyses assume that $C$ and $T$ are mutually
independent; this assumption is called "non-informative" censoring.

Then the joint likelihood $p(Y,D)$ factors into the product
$p(Y), p(D)$, and the likelihood reduces to:
:::

$$
\begin{aligned}
\Lik(y,d)
&= \left[p(T=y,C> y)\right]^d\cdot
\left[p(T>y,C=y)\right]^{1-d}\\
&= \left[p(T=y)p(C> y)\right]^d\cdot
\left[p(T>y)p(C=y)\right]^{1-d}\\
&= \left[f_T(y)S_C(y)\right]^d\cdot
\left[S(y)f_C(y)\right]^{1-d}\\
&= \left[f_T(y)^d S_C(y)^d\right]\cdot
\left[S_T(y)^{1-d}f_C(y)^{1-d}\right]\\
&= \left(f_T(y)^d \cdot S_T(y)^{1-d}\right)\cdot
\left(f_C(y)^{1-d} \cdot S_C(y)^{d}\right)
\end{aligned}
$$

---

::: notes
The corresponding log-likelihood is:
:::

$$
\begin{aligned}
\ell(y,d)
&= \text{log}\left\{\Lik(y,d) \right\}\\
&= \text{log}\left\{
\left(f_T(y)^d \cdot S_T(y)^{1-d}\right)\cdot
\left(f_C(y)^{1-d} \cdot S_C(y)^{d}\right)
\right\}\\
&= \text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}
+
\text{log}\left\{
f_C(y)^{1-d} \cdot S_C(y)^{d}
\right\}\\
\end{aligned}
$$ Let

-   $\theta_T$ represent the parameters of $p_T(t)$,
-   $\theta_C$ represent the parameters of $p_C(c)$,
-   $\theta = (\theta_T, \theta_C)$ be the combined vector of all
parameters.

---

::: notes
The corresponding score function is:
:::

$$
\begin{aligned}
\ell'(y,d)
&= \deriv{\theta}
\left[
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}
+
\text{log}\left\{
f_C(y)^{1-d} \cdot S_C(y)^{d}
\right\}
\right]\\
&=
\left(
\deriv{\theta}
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}
\right)
+
\left(
\deriv{\theta}
\text{log}\left\{
f_C(y)^{1-d} \cdot S_C(y)^{d}
\right\}
\right)\\
\end{aligned}
$$

---

::: notes
As long as $\theta_C$ and $\theta_T$ don't share any parameters, then if
censoring is non-informative, the partial derivative with respect to
$\theta_T$ is:
:::

$$
\begin{aligned}
\ell'_{\theta_T}(y,d)
&\eqdef  \deriv{\theta_T}\ell(y,d)\\
&=
\left(
\deriv{\theta_T}
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}
\right)
+
\left(
\deriv{\theta_T}
\text{log}\left\{
f_C(y)^{1-d} \cdot S_C(y)^{d}
\right\}
\right)\\
&=
\left(
\deriv{\theta_T}
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}
\right) + 0\\
&=
\deriv{\theta_T}
\text{log}\left\{
f_T(y)^d \cdot S_T(y)^{1-d}
\right\}\\
\end{aligned}
$$

---

::: notes
Thus, the MLE for $\theta_T$ won't depend on $\theta_C$, and we can
ignore the distribution of $C$ when estimating the parameters of
$f_T(t)=p(T=t)$.
:::

Then:

$$
\begin{aligned}
\Lik(y,d)
&= f_T(y)^d \cdot S_T(y)^{1-d}\\
&= \left(h_T(y)^d  S_T(y)^d\right) \cdot S_T(y)^{1-d}\\
&= h_T(y)^d  \cdot S_T(y)^d \cdot S_T(y)^{1-d}\\
&= h_T(y)^d \cdot S_T(y)\\
&= S_T(y) \cdot h_T(y)^d \\
\end{aligned}
$$

::: notes
That is, if the event occurred at time $y$ (i.e., if $d=1$), then the
likelihood of $(Y,D) = (y,d)$ is equal to the hazard function at $y$
times the survival function at $y$. Otherwise, the likelihood is equal
to just the survival function at $y$.
:::

---

::: notes
The corresponding log-likelihood is:
:::

$$
\begin{aligned}
\ell(y,d)
&=\text{log}\left\{\Lik(y,d)\right\}\\
&= \text{log}\left\{S_T(y) \cdot h_T(y)^d\right\}\\
&= \text{log}\left\{S_T(y)\right\} + \text{log}\left\{h_T(y)^d\right\}\\
&= \text{log}\left\{S_T(y)\right\} + d\cdot \text{log}\left\{h_T(y)\right\}\\
&= -H_T(y) + d\cdot \text{log}\left\{h_T(y)\right\}\\
\end{aligned}
$$

::: notes
In other words, the log-likelihood contribution from a single
observation $(Y,D) = (y,d)$ is equal to the negative cumulative hazard
at $y$, plus the log of the hazard at $y$ if the event occurred at time
$y$.
:::
