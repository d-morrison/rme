{
  "hash": "bc9a8928dc284970be44d8f461a59fa8",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"Logistic regression and variations\"\n---\n\n\n\n\n\n\n\n\n# Models for Binary Outcomes {#sec-Bernoulli-models}\n\n---\n\n\n<!-- ::: {.content-hidden when-format=\"revealjs\"} -->\n\n---\n\n### Configuring R {.unnumbered}\n\nFunctions from these packages will be used throughout this document:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%>%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\n```\n:::\n\n\n\n\n\n\n\nHere are some R settings I use in this document:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9\n```\n:::\n\n\n\n\n\n\n\n<!-- ::: -->\n\n\n\n\\providecommand{\\cbl}[1]{\\left\\{#1\\right.}\n\\providecommand{\\cb}[1]{\\left\\{#1\\right\\}}\n\\providecommand{\\paren}[1]{\\left(#1\\right)}\n\\providecommand{\\sb}[1]{\\left[#1\\right]}\n\\def\\pr{\\text{p}}\n\\def\\am{\\arg \\max}\n\\def\\argmax{\\arg \\max}\n\\def\\p{\\text{p}}\n\\def\\P{\\text{P}}\n\\def\\ph{\\hat{\\text{p}}}\n\\def\\hp{\\hat{\\text{p}}}\n\\def\\ga{\\alpha}\n\\def\\b{\\beta}\n\\providecommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor}\n\\providecommand{\\ceiling}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\providecommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\def\\Ber{\\text{Ber}}\n\\def\\Bernoulli{\\text{Bernoulli}}\n\\def\\Pois{\\text{Pois}}\n\\def\\Poisson{\\text{Poisson}}\n\\def\\Gaus{\\text{Gaussian}}\n\\def\\Normal{\\text{N}}\n\\def\\NB{\\text{NegBin}}\n\\def\\NegBin{\\text{NegBin}}\n\\def\\vbeta{\\vec \\beta}\n\\def\\vb{\\vec \\b}\n\\def\\v0{\\vec{0}}\n\\def\\gb{\\beta}\n\\def\\gg{\\gamma}\n\\def\\gd{\\delta}\n\\def\\eps{\\varepsilon}\n\\def\\om{\\omega}\n\\def\\m{\\mu}\n\\def\\s{\\sigma}\n\\def\\l{\\lambda}\n\\def\\gs{\\sigma}\n\\def\\gm{\\mu}\n\\def\\M{\\text{M}}\n\\def\\gM{\\text{M}}\n\\def\\Mu{\\text{M}}\n\\def\\cd{\\cdot}\n\\def\\cds{\\cdots}\n\\def\\lds{\\ldots}\n\\def\\eqdef{\\stackrel{\\text{def}}{=}}\n\\def\\defeq{\\stackrel{\\text{def}}{=}}\n\\def\\hb{\\hat \\beta}\n\\def\\hl{\\hat \\lambda}\n\\def\\hy{\\hat y}\n\\def\\yh{\\hat y}\n\\def\\V{{\\text{Var}}}\n\\def\\hs{\\hat \\sigma}\n\\def\\hsig{\\hat \\sigma}\n\\def\\hS{\\hat \\Sigma}\n\\def\\hSig{\\hat \\Sigma}\n\\def\\hSigma{\\hat \\Sigma}\n\\def\\hSurv{\\hat{S}}\n\\providecommand{\\hSurvf}[1]{\\hat{S}\\paren{#1}}\n\\def\\dist{\\ \\sim \\ }\n\\def\\ddist{\\ \\dot{\\sim} \\ }\n\\def\\dsim{\\ \\dot{\\sim} \\ }\n\\def\\za{z_{1 - \\frac{\\alpha}{2}}}\n\\def\\cirad{\\za \\cdot \\hse{\\hb}}\n\\def\\ci{\\hb {\\color{red}\\pm} \\cirad}\n\\def\\th{\\theta}\n\\def\\Th{\\Theta}\n\\def\\xbar{\\bar{x}}\n\\def\\hth{\\hat\\theta}\n\\def\\hthml{\\hth_{\\text{ML}}}\n\\def\\ba{\\begin{aligned}}\n\\def\\ea{\\end{aligned}}\n\\def\\ind{тлл}\n\\def\\indpt{тлл}\n\\def\\all{\\forall}\n\\def\\iid{\\text{iid}}\n\\def\\ciid{\\text{ciid}}\n\\def\\simind{\\ \\sim_{\\ind}\\ }\n\\def\\siid{\\ \\sim_{\\iid}\\ }\n\\def\\simiid{\\siid}\n\\def\\distiid{\\siid}\n\\def\\tf{\\therefore}\n\\def\\Lik{\\mathcal{L}}\n\\def\\llik{\\ell}\n\\providecommand{\\llikf}[1]{\\llik \\paren{#1}}\n\\def\\score{\\ell'}\n\\providecommand{\\scoref}[1]{\\score \\paren{#1}}\n\\def\\hess{\\ell''}\n\\def\\hessian{\\ell''}\n\\providecommand{\\hessf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\hessianf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\starf}[1]{#1^*}\n\\def\\lik{\\ell}\n\\providecommand{\\est}[1]{\\widehat{#1}}\n\\providecommand{\\esttmp}[1]{{\\widehat{#1}}^*}\n\\def\\esttmpl{\\esttmp{\\lambda}}\n\\def\\cR{\\mathcal{R}}\n\\def\\range{\\mathcal{R}}\n\\def\\Range{\\mathcal{R}}\n\\providecommand{\\rangef}[1]{\\cR(#1)}\n\\def\\~{\\approx}\n\\def\\dapp{\\dot\\approx}\n\\providecommand{\\red}[1]{{\\color{red}#1}}\n\\providecommand{\\deriv}[1]{\\frac{\\partial}{\\partial #1}}\n\\providecommand{\\derivf}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\providecommand{\\blue}[1]{{\\color{blue}#1}}\n\\providecommand{\\green}[1]{{\\color{green}#1}}\n\\providecommand{\\hE}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hExp}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hmu}[1]{\\hat{\\mu}\\sb{#1}}\n\\def\\Expp{\\mathbb{E}}\n\\def\\Ep{\\mathbb{E}}\n\\def\\expit{\\text{expit}}\n\\providecommand{\\expitf}[1]{\\expit\\cb{#1}}\n\\providecommand{\\dexpitf}[1]{\\expit'\\cb{#1}}\n\\def\\logit{\\text{logit}}\n\\providecommand{\\logitf}[1]{\\logit\\cb{#1}}\n\\providecommand{\\E}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Ef}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Exp}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Expf}[1]{\\mathbb{E}\\sb{#1}}\n\\def\\Varr{\\text{Var}}\n\\providecommand{\\var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\varf}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Varf}[1]{\\text{Var}\\paren{#1}}\n\\def\\Covt{\\text{Cov}}\n\\providecommand{\\covh}[1]{\\widehat{\\text{Cov}}\\paren{#1}}\n\\providecommand{\\Cov}[1]{\\Covt \\paren{#1}}\n\\providecommand{\\Covf}[1]{\\Covt \\paren{#1}}\n\\def\\varht{\\widehat{\\text{Var}}}\n\\providecommand{\\varh}[1]{\\varht\\paren{#1}}\n\\providecommand{\\varhf}[1]{\\varht\\paren{#1}}\n\\providecommand{\\vc}[1]{\\boldsymbol{#1}}\n\\providecommand{\\sd}[1]{\\text{sd}\\paren{#1}}\n\\providecommand{\\SD}[1]{\\text{SD}\\paren{#1}}\n\\providecommand{\\hSD}[1]{\\widehat{\\text{SD}}\\paren{#1}}\n\\providecommand{\\se}[1]{\\text{se}\\paren{#1}}\n\\providecommand{\\hse}[1]{\\hat{\\text{se}}\\paren{#1}}\n\\providecommand{\\SE}[1]{\\text{SE}\\paren{#1}}\n\\providecommand{\\HSE}[1]{\\widehat{\\text{SE}}\\paren{#1}}\n\\renewcommand{\\log}[1]{\\text{log}\\cb{#1}}\n\\providecommand{\\logf}[1]{\\text{log}\\cb{#1}}\n\\def\\dlog{\\text{log}'}\n\\providecommand{\\dlogf}[1]{\\dlog \\cb{#1}}\n\\renewcommand{\\exp}[1]{\\text{exp}\\cb{#1}}\n\\providecommand{\\expf}[1]{\\exp{#1}}\n\\def\\dexp{\\text{exp}'}\n\\providecommand{\\dexpf}[1]{\\dexp \\cb{#1}}\n\\providecommand{\\e}[1]{\\text{e}^{#1}}\n\\providecommand{\\ef}[1]{\\text{e}^{#1}}\n\\providecommand{\\inv}[1]{\\paren{#1}^{-1}}\n\\providecommand{\\invf}[1]{\\paren{#1}^{-1}}\n\\def\\oinf{I}\n\\def\\Nat{\\mathbb{N}}\n\\providecommand{\\oinff}[1]{\\oinf\\paren{#1}}\n\\def\\einf{\\mathcal{I}}\n\\providecommand{\\einff}[1]{\\einf\\paren{#1}}\n\\def\\heinf{\\hat{\\einf}}\n\\providecommand{\\heinff}[1]{\\heinf \\paren{#1}}\n\\providecommand{\\1}[1]{\\mathbb{1}_{#1}}\n\\providecommand{\\set}[1]{\\cb{#1}}\n\\providecommand{\\pf}[1]{\\p \\paren{#1}}\n\\providecommand{\\Bias}[1]{\\text{Bias}\\paren{#1}}\n\\providecommand{\\bias}[1]{\\text{Bias}\\paren{#1}}\n\\def\\ss{\\sigma^2}\n\\providecommand{\\ssqf}[1]{\\sigma^2\\paren{#1}}\n\\providecommand{\\mselr}[1]{\\text{MSE}\\paren{#1}}\n\\providecommand{\\maelr}[1]{\\text{MAE}\\paren{#1}}\n\\providecommand{\\abs}[1]{\\left|#1\\right|}\n\\providecommand{\\sqf}[1]{\\paren{#1}^2}\n\\providecommand{\\sq}{^2}\n\\def\\err{\\eps}\n\\providecommand{\\erf}[1]{\\err\\paren{#1}}\n\\renewcommand{\\vec}[1]{\\tilde{#1}}\n\\providecommand{\\v}[1]{\\vec{#1}}\n\\providecommand{\\matr}[1]{\\mathbf{#1}}\n\\def\\mX{\\matr{X}}\n\\def\\mx{\\matr{x}}\n\\def\\vx{\\vec{x}}\n\\def\\vX{\\vec{X}}\n\\def\\vy{\\vec{y}}\n\\def\\vY{\\vec{Y}}\n\\def\\vpi{\\vec{\\pi}}\n\\providecommand{\\mat}[1]{\\mathbf{#1}}\n\\providecommand{\\dsn}[1]{#1_1, \\ldots, #1_n}\n\\def\\X1n{\\dsn{X}}\n\\def\\Xin{\\dsn{X}}\n\\def\\x1n{\\dsn{x}}\n\\def\\'{^{\\top}}\n\\def\\dpr{\\cdot}\n\\def\\Xx1n{X_1=x_1, \\ldots, X_n = x_n}\n\\providecommand{\\dsvn}[2]{#1_1=#2_1, \\ldots, #1_n = #2_n}\n\\providecommand{\\sumn}[1]{\\sum_{#1=1}^n}\n\\def\\sumin{\\sum_{i=1}^n}\n\\def\\sumi1n{\\sum_{i=1}^n}\n\\def\\prodin{\\prod_{i=1}^n}\n\\def\\prodi1n{\\prod_{i=1}^n}\n\\providecommand{\\lp}[2]{#1 \\' \\beta}\n\\def\\odds{\\omega}\n\\def\\OR{\\text{OR}}\n\\def\\logodds{\\eta}\n\\def\\oddst{\\text{odds}}\n\\def\\probst{\\text{probs}}\n\\def\\probt{\\text{probt}}\n\\def\\probit{\\text{probit}}\n\\providecommand{\\oddsf}[1]{\\oddst\\cb{#1}}\n\\providecommand{\\doddsf}[1]{{\\oddst}'\\cb{#1}}\n\\def\\oddsinv{\\text{invodds}}\n\\providecommand{\\oddsinvf}[1]{\\oddsinv\\cb{#1}}\n\\def\\invoddsf{\\oddsinvf}\n\\providecommand{\\doddsinvf}[1]{{\\oddsinv}'\\cb{#1}}\n\\def\\dinvoddsf{\\doddsinvf}\n\\def\\haz{h}\n\\def\\cuhaz{H}\n\\def\\incidence{\\bar{\\haz}}\n\\def\\phaz{\\Expf{\\haz}}\n\n\n\n\n\n\n\n\n\n```{=html}\n<style>\n.quarto-figure-center > figure {\ntext-align: center;\n}\n</style>\n```\n\n\n\n\n\n\n\n\n\n### Acknowledgements {.unnumbered}\n\nThis content is adapted from:\n\n-   @dobson4e, Chapter 7\n-   @vittinghoff2e, Chapter 5\n- [David Rocke](https://dmrocke.ucdavis.edu/)'s materials from the [2021 edition of Epi 204](https://dmrocke.ucdavis.edu/Class/EPI204-Spring-2021/EPI204-Spring-2021.html)\n-   @NahhasIRMPHR [Chapter 6](https://www.bookdown.org/rwnahhas/RMPH/blr.html)\n\n## Introduction\n\n### What is logistic regression?\n\n**Logistic regression** is a framework for modeling [binary](probability.qmd#def-binary) outcomes, conditional on one or more *predictors* (a.k.a. *covariates*).\n\n---\n\n::: {#exr-binary-examples}\n\n#### Examples of binary outcomes\n\nWhat are some examples of binary outcomes in the health sciences?\n:::\n\n----\n\n:::: {.solution}\n\n\nExamples of binary outcomes include:\n\n* exposure (exposed vs unexposed)\n* disease (diseased vs healthy)\n* recovery (recovered vs unrecovered)\n* relapse (relapse vs remission)\n* return to hospital (returned vs not)\n* vital status (dead vs alive)\n\n\n\n\n\n::::\n\n---\n\nLogistic regression uses the [Bernoulli](probability.qmd#def-bernoulli) distribution to model the outcome variable, conditional on one or more covariates. \n\n---\n\n::: {#exr-def-bernoulli}\n\nWrite down a mathematical definition of the Bernoulli distribution.\n:::\n\n---\n\n::::{.solution}\n\n\nThe **Bernoulli distribution** family for a random variable $X$ is defined as:\n\n$$\n\\ba\n\\Pr(X=x) &= \\1{x\\in \\set{0,1}}\\pi^x(1-\\pi)^{1-x}\\\\\n&= \\cbl{{\\pi, x=1}\\atop{1-\\pi, x=0}}\n\\ea\n$$\n\n\n\n::::\n\n---\n\n### Logistic regression versus linear regression\n\nLogistic regression differs from linear regression, which uses the Gaussian (\"normal\") distribution to model the outcome variable, conditional on the covariates.\n\n---\n\n:::: {#exr-linear}\n\nRecall: what kinds of outcomes is linear regression used for?\n::::\n\n---\n\n::: {.solution}\n\nLinear regression is typically used for numerical outcomes that aren't event counts or waiting times for an event. Examples of outcomes that are often analyzed using linear regression include include weight, height, and income.\n\n:::\n\n## Risk Estimation and Prediction\n\n::: notes\nIn Epi 203, you have already seen methods for modeling binary outcomes using one covariate that is also binary (such as exposure/non-exposure).\nIn this section, we review one-covariate analyses, with a special focus on risk ratios and odds ratios, which are important concepts for interpreting logistic regression.\n:::\n\n---\n\n:::::{#exm-oc-mi}\n### Oral Contraceptive Use and Heart Attack\n\n* Research question: how does oral contraceptive (OC) use affect the risk of myocardial infarction (MI; a.k.a. heart attack)?\n\n:::{.notes}\nThis was an issue when oral contraceptives were first developed, because the original formulations used higher concentrations of hormones. Modern OCs don't have this issue.\n\n@tbl-oc-mi contains simulated data for an imaginary follow-up (a.k.a. *prospective*) study in which two groups are identified, one using OCs and another not using OCs, and both groups are tracked for three years to determine how many in each groups have MIs.\n\n:::\n\n\n\n\n\n\n\n\n::: {#tbl-oc-mi .cell tbl-cap='Simulated data from study of oral contraceptive use and heart attack risk'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\noc_mi = \n  tribble(\n    ~OC, ~MI, ~Total,\n    \"OC use\", 13, 5000,\n    \"No OC use\", 7, 10000\n  ) |> \n  mutate(`No MI` = Total - MI) |> \n  relocate(`No MI`, .after = MI)\n\ntotals = \n  oc_mi |> \n  summarize(across(c(MI, `No MI`, Total), sum)) |> \n  mutate(OC = \"Total\")\n\ntbl_oc_mi = bind_rows(oc_mi, totals)\n\ntbl_oc_mi\n#> # A tibble: 3 x 4\n#>   OC           MI `No MI` Total\n#>   <chr>     <dbl>   <dbl> <dbl>\n#> 1 OC use       13    4987  5000\n#> 2 No OC use     7    9993 10000\n#> 3 Total        20   14980 15000\n```\n:::\n\n\n\n\n\n\n\n\n:::::\n\n---\n\n::::{#exr-probs}\n\nReview: estimate the probabilities of MI for OC users and non-OC users in @exm-oc-mi.\n::::\n\n----\n\n:::{.solution}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$\\ph(MI|OC) = \\frac{13}{5000} = 0.0026$$\n\n$$\\ph(MI|\\neg OC) = \\frac{7}{10000} = \\ensuremath{7\\times 10^{-4}}$$\n\n:::\n\n---\n\n#### Controls\n\n::::{.callout-note}\n##### Two meanings of \"controls\"\n\nDepending on context, \"controls\" can mean either individuals who don't experience an *exposure* of interest, or individuals who don't experience an *outcome* of interest.\n\n::::\n\n---\n\n:::{#def-cases-retrospective}\n##### cases and controls in retrospective studies\nIn *retrospective studies*, participants who experience the outcome of interest are called **cases**, while participants who don't experience that outcome are called **controls**. \n:::\n\n---\n\n:::{#def-cases-prospective}\n##### treatment groups and control groups in prospective studies\nIn *prospective studies*, the group of participants who experience the treatment or exposure of interest is called the **treatment group**, while the participants who receive the baseline or comparison treatment  (for example, clinical trial participants who receive a placebo or a standard-of-care treatment rather than an experimental treatment) are called **controls**.\n\n:::\n\n## Comparing Probabilities\n\n### Risk differences\n\n::: notes\nThe simplest comparison of two probabilities, $\\pi_1$, and $\\pi_2$, is the difference of their values:\n:::\n\n:::{#def-RD}\n#### Risk difference\nThe **risk difference** of two probabilities, $\\pi_1$, and $\\pi_2$, is the difference of their values: \n$$\\delta(\\pi_1,\\pi_2) \\eqdef \\pi_1 - \\pi_2$$\n:::\n\n---\n\n:::{#exm-RD}\n\n#### Difference in MI risk\n\nIn @exm-oc-mi, the maximum likelihood estimate of the difference in MI risk between OC users and OC non-users is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$\n\\begin{aligned}\n\\hat\\delta(\\pi(OC), \\pi(\\neg OC))\n&= \\delta(\\hat\\pi(OC), \\hat\\pi(\\neg OC))\\\\\n&= \\hat\\pi(OC) - \\hat\\pi(\\neg OC)\\\\\n&= 0.0026 - \\ensuremath{7\\times 10^{-4}}\\\\\n&= 0.0019\n\\end{aligned}\n$$\n\n:::\n\n---\n\n### Risk ratios\n\n:::{#def-RR}\n### Relative risk ratios\n\n:::: notes\nThe **relative risk** of probability $\\pi_1$ compared to another probability $\\pi_2$, also called the **risk ratio**, **relative risk ratio**, **probability ratio**, or **rate ratio**, is the ratio of those probabilities:\n::::\n\n$$\\rho(\\pi_1,\\pi_2) = \\frac{\\pi_1}{\\pi_2}$$\n:::\n\n---\n\n:::{#exm-RR}\n\n:::: notes\nAbove, we estimated that:\n::::\n\n$$\\ph(MI|OC) = 0.0026$$\n\n$$\\ph(MI|\\neg OC) = \\ensuremath{7\\times 10^{-4}}$$\n\n:::: notes\nSo we might estimate that the *relative risk* of MI for OC versus non-OC is:\n::::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr = (13/5000)/(7/10000)\n```\n:::\n\n\n\n\n\n\n\n\n$$\n\\begin{aligned}\n\\hat\\rho(OC, \\neg OC)\n&=\\frac{\\ph(MI|OC)}{\\ph(MI|\\neg OC)}\\\\\n&= \\frac{0.0026}{\\ensuremath{7\\times 10^{-4}}}\\\\\n&= 3.7143\n\\end{aligned}\n$$\n\n:::: notes\nWe might summarize this result by saying that \"the estimated probability of MI among OC users was 3.7143 as high as the estimated probability among OC non-users. \n::::\n\n:::\n\n---\n\n### Relative risk differences\n\n:::{#def-RRD}\n\n#### Relative risk difference\n\n:::: notes\nSometimes, we divide the risk difference by the comparison probability; the result is called the **relative risk difference**:\n::::\n\n$$\\xi(\\pi_1,\\pi_2) \\eqdef \\frac{\\delta(\\pi_1,\\pi_2)}{\\pi_2}$$\n\n:::\n\n---\n\n:::{#thm-rrd-vs-rr}\n#### Relative risk difference equals risk ratio minus 1\n\n$$\\xi(\\pi_1,\\pi_2) = \\rho(\\pi_1,\\pi_2) - 1$$\n:::\n\n---\n\n::: proof\n$$\n\\begin{aligned}\n\\xi(\\pi_1,\\pi_2) \n&\\eqdef \\frac{\\delta(\\pi_1,\\pi_2)}{\\pi_2}\n\\\\&= \\frac{\\pi_1-\\pi_2}{\\pi_2}\n\\\\&= \\frac{\\pi_1}{\\pi_2} - 1\n\\\\&= \\rho(\\pi_1,\\pi_2) - 1\n\\end{aligned}\n$$\n:::\n\n---\n\n### Changing reference groups in risk comparisons\n\n:::: notes\nRisk differences, risk ratios, and relative risk differences are defined by two probabilities, plus a choice of which probability is the **baseline** or **reference** probability (i.e., which probability is the subtrahend of the risk difference or the denominator of the risk ratio).\n:::\n\n$$\\delta(\\pi_2,\\pi_1) = -\\delta(\\pi_1, \\pi_2)$$\n\n$$\\rho(\\pi_2,\\pi_1) = \\inv{\\rho(\\pi_1,\\pi_2)}$$\n$$\\xi(\\pi_2,\\pi_1) = \\inv{\\xi(\\pi_2,\\pi_1) + 1} - 1$$\n\n:::{#exr-change-ref-group}\n\nProve the relationships above.\n\n:::\n\n---\n\n:::{#exm-ref}\n\n#### Switching the reference group in a risk ratio\n\nAbove, we estimated that the risk ratio of OC versus non-OC is:\n\n$$\n\\begin{aligned}\n\\rho(OC, \\neg OC)\n&= 3.7143\n\\end{aligned}\n$$\n\nIn comparison, the risk ratio for non-OC versus OC is:\n\n$$\n\\begin{aligned}\n\\rho(\\neg OC, OC)\n&=\\frac{\\ph(MI|\\neg OC)}{\\ph(MI|OC)}\\\\\n&= \\frac{\\ensuremath{7\\times 10^{-4}}}{0.0026}\\\\\n&= 0.2692\\\\\n&= \\frac{1}{\\rho(OC, \\neg OC)}\n\\end{aligned}\n$$\n\n:::\n\n## Odds and Odds Ratios\n\n### Odds and probabilities\n\n::: notes\nIn logistic regression, we will make use of a mathematically-convenient transformation of probability, called *odds*.\n:::\n\n:::{#def-odds}\n#### Odds\nThe **odds** of an outcome $A$, which we will represent using $\\odds$ (\"omega\"), is the probability that the outcome occurs, divided by the probability that it doesn't occur:\n\n$$\\odds(A) \\eqdef \\frac{\\Pr(A)}{\\Pr(\\neg A)}$$\n:::\n\n---\n\n:::{#thm-prob-to-odds}\nIf the probability of an outcome $A$ is $\\Pr(A)=\\pi$, \nthen the corresponding odds of $A$ is:\n\n$$\\oddsf{\\pi} = \\frac{\\pi}{1-\\pi}$$ {#eq-odds-probs}\n\n:::\n\n---\n\n:::{.proof}\n$$\n\\ba\n\\Pr(\\neg A) &= 1 - \\Pr(A)\n\\\\ &= 1 - \\pi\n\\ea\n$$\n\n$$\n\\ba\n\\tf \\odds(A) &\\eqdef \\frac{\\Pr(A)}{\\Pr(\\neg A)} \n\\\\ &= \\frac{\\pi}{1-\\pi}\n\\ea\n$$\n\n:::\n\n---\n\n::: notes\n[Function @eq-odds-probs], which transforms probabilities into odds, can be called the **odds function**. @fig-odds-probs graphs the shape of this function.\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nodds = function(pi) pi / (1 - pi)\nlibrary(ggplot2)\nggplot() +\n  geom_function(\n    fun = odds,\n    arrow = arrow(ends = \"last\"),\n    mapping = aes(col = \"odds function\")\n  ) +\n  xlim(0, .99) +\n  xlab(\"Probability\") +\n  ylab(\"Odds\") +\n  geom_abline(aes(\n    intercept = 0,\n    slope = 1,\n    col = \"y=x\"\n  )) +\n  theme_bw() +\n  labs(colour = \"\") +\n  theme(legend.position = \"bottom\")\n\n```\n\n::: {.cell-output-display}\n![Odds versus probability](logistic-regression_files/figure-pdf/fig-odds-probs-1.pdf){#fig-odds-probs}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n:::{#exm-odds}\n\n#### Computing odds from probabilities\n\nIn @exr-probs, we estimated that the probability of MI, given OC use, is $\\pi(OC) \\eqdef \\Pr(MI|OC) = 0.0026$. If this estimate is correct, then the odds of MI, given OC use, is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$\n\\begin{aligned}\n\\odds(OC) \n&\\eqdef \\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\\\\n&=\\frac{\\Pr(MI|OC)}{1-\\Pr(MI|OC)}\\\\\n&=\\frac{\\pi(OC)}{1-\\pi(OC)}\\\\\n&=\\frac{0.0026}{1-0.0026}\\\\\n&\\approx 0.002607\n\\end{aligned}\n$$\n\n:::\n\n---\n\n:::{#exr-odds}\n\n#### Computing odds from probabilities\n\nEstimate the odds of MI, for non-OC users.\n\n::::{.solution}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$\n\\odds(\\neg OC) = \\ensuremath{7.0049\\times 10^{-4}}\n$$\n\n::::\n\n:::\n\n---\n\n:::{#thm-est-odds}\n#### One-sample MLE for odds\n\nLet $X_1,...X_n$ be a set of $n$ $\\iid$ Bernoulli trials, and let $X = \\sumin X_i$ be their sum. \n\nThen the maximum likelihood estimate of the odds of $X=1$, $\\odds$, is:\n\n$$\n\\hat{\\odds}= \\frac{x}{n-x}\n$$\n\n:::\n\n---\n\n::: proof\n\n$$\n\\begin{aligned}\n1-\\hat\\pi \n&= 1-\\frac{x}{n}\\\\\n&= \\frac{n}{n} - \\frac{x}{n}\\\\\n&= \\frac{n - x}{n}\n\\end{aligned}\n$$\n\nThus, the estimated odds is:\n\n$$\n\\begin{aligned}\n\\frac{\\hat\\pi}{1-\\hat\\pi}\n&= \\frac{\\left(\\frac{x}{n}\\right)}{\\left(\\frac{n-x}{n}\\right)}\\\\\n&= \\frac{x}{n-x}\n\\end{aligned}\n$$\n\n:::: notes\nThat is, odds can be calculated directly as \"# events\" divided by \"# nonevents\" (without needing to calculate $\\hat\\pi$ and $1-\\hat\\pi$ first).\n::::\n\n:::\n\n---\n\n::::{#exm-odds-shortcut}\n\n#### calculating odds using the shortcut\nIn @exm-odds, we calculated \n$$\n\\begin{aligned}\n\\odds(OC) \n&=0.0026\n\\end{aligned}\n$$\n\nLet's recalculate this result using our shortcut.\n\n::::\n\n---\n\n::::{#sol-odds-shortcut}\n$$\n\\begin{aligned}\n\\odds(OC) \n&=\\frac{13}{5000-13}\\\\\n&=0.0026\n\\end{aligned}\n$$\n\nSame answer as in @exm-odds!\n\n::::\n\n---\n\n:::{#thm-odds-simplified}\n\n#### Simplified expression for odds function\n\n::: notes\nAn equivalent expression for the odds function is \n:::\n\n$$\n\\oddsf{\\pi} = \\invf{\\invf{\\pi}-1)}\n$$ {#eq-odds-reduced}\n\n:::\n\n---\n\n:::{#exr-odds2}\nProve that @eq-odds-reduced is equivalent to @def-odds.\n:::\n\n---\n\n:::{#thm-deriv-odds}\n#### Derivative of odds function\n\n$$\\doddsf{\\pi} = \\frac{1}{\\sqf{1-\\pi}}$$\n:::\n\n---\n\n::: proof\n$$\n\\ba\n\\doddsf{\\pi}\n   &= \\deriv{\\pi}\\paren{\\frac{\\pi}{1-\\pi}}\n\\\\ &=  \\frac {\\deriv{\\pi}\\pi} {1-\\pi} - \n       \\paren{\\frac{\\pi}{\\sqf{1-\\pi}} \\cd \\deriv{\\pi}\\paren{1-\\pi}}\n\\\\ &=  \\frac{1}{1-\\pi} - \\frac{\\pi}{\\sqf{1-\\pi}} \\cd (-1)\n\\\\ &=  \\frac{1}{1-\\pi} + \\frac{\\pi}{\\sqf{1-\\pi}}\n\\\\ &=  \\frac{1-\\pi}{\\sqf{1-\\pi}} + \\frac{\\pi}{\\sqf{1-\\pi}}\n\\\\ &=  \\frac{1-\\pi + \\pi}{\\sqf{1-\\pi}}\n\\\\ &=  \\frac{1}{\\sqf{1-\\pi}}\n\\ea\n$$\n\n:::\n\n---\n\n#### Odds of rare events\n\nFor rare events (small $\\pi$), odds and probabilities are nearly equal, because $1-\\pi \\approx 1$ (see @fig-odds-probs).\n\nFor example, in @exm-odds, the probability and odds differ by $\\ensuremath{6.7776\\times 10^{-6}}$.\n\n---\n\n:::{#exr-odds-probs}\n\nWhat odds value corresponds to the probability $\\pi = 0.2$, and what is the numerical difference between these two values?\n\n:::\n\n---\n\n::::{.solution}\n$$\n\\odds = \\frac{\\pi}{1-\\pi} \n=\\frac{.2}{.8}\n= .25\n$$\n::::\n\n---\n\n:::{#thm-odds-minus-probs}\n\nLet $\\odds = \\frac{\\pi}{1-\\pi}$. Then:\n\n$$\\odds - \\pi =  \\odds \\cd \\pi$$\n\n:::\n\n---\n\n:::{#exr-odds-minus-probs}\n\nProve @thm-odds-minus-probs.\n\n:::\n\n---\n\n::: solution\n\n$$\n\\ba\n\\odds - \\pi \n&= \\frac{\\pi}{1-\\pi} - \\pi\n\\\\ &= \\frac{\\pi}{1-\\pi} - \\frac{\\pi(1-\\pi)}{1-\\pi}\n\\\\ &= \\frac{\\pi}{1-\\pi} - \\frac{\\pi - \\pi^2}{1-\\pi}\n\\\\ &= \\frac{\\pi - (\\pi - \\pi^2)}{1-\\pi}\n\\\\ &= \\frac{\\pi - \\pi + \\pi^2}{1-\\pi}\n\\\\ &= \\frac{\\pi^2}{1-\\pi}\n\\\\ &= \\frac{\\pi}{1-\\pi} \\pi\n\\\\ &= \\odds \\pi\n\\ea\n$$\n\n:::\n\n---\n\n:::{#lem-odds-neg}\n#### Odds of a non-event\n\nIf $\\pi$ is the odds of event $A$ \nand $\\odds$ is the corresponding odds of $A$,\nthen the odds of $\\neg A$ are:\n\n$$\n\\odds(\\neg A) = \\frac{1-\\pi}{\\pi}\n$$\n\n:::\n\n::: proof\nLeft to the reader.\n::: \n\n---\n\n### The inverse odds function\n\n:::{#def-inv-odds}\n\n##### inverse odds function\nThe **inverse odds function**, \n\n$$\\invoddsf{\\odds} \\eqdef \\frac{\\odds}{1 + \\odds}$$\nconverts odds into their corresponding probabilities (@fig-inv-odds).\n:::\n\n::: notes\nThe inverse-odds function takes an odds as input and produces a probability as output. Its domain of inputs is $[0,\\infty)$ and its range of outputs is $[0,1]$.\n\nI haven't seen anyone give the inverse-odds function a concise name; maybe $\\text{prob}()$?\n:::\n\n---\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodds_inv = function(omega) (1 + omega^-1)^-1\nggplot() +\n  geom_function(fun = odds_inv, aes(col = \"inverse-odds\")) +\n  xlab(\"Odds\") +\n  ylab(\"Probability\") +\n  xlim(0,5) +\n  ylim(0,1) +\n  geom_abline(aes(intercept = 0, slope = 1, col = \"x=y\"))\n```\n\n::: {.cell-output-display}\n![The inverse odds function, $\\invoddsf{\\odds}$](logistic-regression_files/figure-pdf/fig-inv-odds-1.pdf){#fig-inv-odds}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n:::{#exr-odds-probs}\n\nWhat probability corresponds to an odds of $\\odds = 1$, and what is the numerical difference between these two values?\n\n:::\n\n---\n\n::::{.solution}\n$$\n\\pi(1) = \\frac{1}{1+1} \n=\\frac{1}{2}\n= .5\n$$\n$$\n1 - \\pi(1) = 1 - .5 = .5\n$$\n\n::::\n\n---\n\n:::{#lem-invodds-simplified}\n\n##### Simplified expression for inverse odds function\n\n::: notes\nAn equivalent expression for the inverse odds function is \n:::\n\n$$\n\\pi(\\odds) = (1+\\odds^{-1})^{-1}\n$$ {#eq-inv-odds-reduced}\n\n:::\n\n---\n\n:::{#exr-inv-odds2}\nProve that @eq-inv-odds-reduced is equivalent to @def-inv-odds.\n:::\n\n---\n\n\n:::{#lem-one-minus-odds-inv}\n#### One minus inverse-odds\n\n\n$$1 - \\oddsinvf{\\odds} = \\frac{1}{1+\\odds}$$\n:::\n\n---\n\n::: {.proof}\n\n$$\n\\ba\n1 - \\oddsinvf{\\odds} &= 1 - \\frac{\\odds}{1 + \\odds}\n\\\\ &= \\frac{\\red{1+\\odds}}{1 + \\odds} - \\frac{\\blue{\\odds}}{1 + \\odds}\n\\\\ &= \\frac{\\red{(1+\\odds)} - \\blue{\\odds}}{1 + \\odds}\n\\\\ &= \\frac{1 + \\odds - \\odds}{1 + \\odds}\n\\\\ &= \\frac{1}{1 + \\odds}\n\\ea\n$$\n:::\n\n---\n\n:::{#thm-inverse-odds-nonevent}\n\nIf $\\odds$ is the odds of event $A$, \nthen the probability that $A$ does not occur is:\n\n$$\\Pr(\\neg A) = \\frac{1}{1+\\odds}$$\n:::\n\n---\n\n::: proof\n\n:::: notes\nUse @lem-one-minus-odds-inv:\n::::\n\n$$\n\\ba\n\\Pr(\\neg A) \n&= 1 - \\Pr(A)\n\\\\ &= 1 - \\oddsinvf{\\odds}\n\\\\ &= \\frac{1}{1 + \\odds}\n\\ea\n$$\n:::\n\n---\n\n:::{#thm-deriv-invodds}\n\n##### Derivative of inverse odds function\n$$\\doddsinvf{\\odds} = \\frac{1}{\\sqf{1+\\odds}}$$\n:::\n\n---\n\n::: proof\n\n:::: notes\nUse the quotient rule:\n::::\n\n$$\n\\ba\n{\\oddsinv}'(\\odds) \n&= \\deriv{\\odds} \\oddsinvf{\\odds}\n\\\\ &= \\deriv{\\odds} \\frac{\\odds}{1+\\odds}\n\\\\ &= \\frac{\\deriv{\\odds}\\odds}{1+\\odds} - \\frac{\\odds}{\\sqf{1+\\odds}} \\cd \\deriv{\\odds}(1+\\odds) \n\\\\ &= \\frac{1}{1+\\odds} - \\frac{\\odds}{\\sqf{1+\\odds}} \\cd 1\n\\\\ &= \\frac{1}{1+\\odds} - \\frac{\\odds}{\\sqf{1+\\odds}}\n\\\\ &= \\frac{1+\\odds}{\\sqf{1+\\odds}} - \\frac{\\odds}{\\sqf{1+\\odds}}\n\\\\ &= \\frac{1+\\odds - \\odds}{\\sqf{1+\\odds}}\n\\\\ &= \\frac{1}{\\sqf{1+\\odds}}\n\\ea\n$$\n\n:::\n\n---\n\n\n:::{#cor-deriv-invodds}\n\n$$\\doddsinvf{\\odds} = \\sqf{1 - \\oddsinvf{\\odds}}$$\n:::\n\n---\n\n### Odds ratios\n\n::: notes\n\nNow that we have defined odds, we can introduce another way of comparing event probabilities: odds ratios.\n\n:::\n\n:::{#def-OR}\n##### Odds ratio\nThe **odds ratio** for two odds $\\odds_1$, $\\odds_2$ is their ratio:\n\n$$\\theta(\\odds_1, \\odds_2) = \\frac{\\odds_1}{\\odds_2}$$\n\n:::\n\n---\n\n:::{#exm-OR}\n#### Calculating odds ratios\nIn @exm-oc-mi, the odds ratio for OC users versus OC-non-users is:\n\n$$\n\\begin{aligned}\n\\theta(\\odds(OC), \\odds(\\neg OC))\n&= \\frac{\\odds(OC)}{\\odds(\\neg OC)}\\\\\n&= \\frac{0.0026}{\\ensuremath{7\\times 10^{-4}}}\\\\\n&= 3.7143\\\\\n\\end{aligned}\n$$\n:::\n\n---\n\n#### A shortcut for calculating odds ratio estimates {.smaller}\n\n::: notes\nThe general form of a two-by-two table is shown in @tbl-2x2-generic. \n\n\n|               | Event | Non-Event | Total\n|-------------- | ------ | ---------- | ----- \n|Exposed        | a      | b | a+b\n|Non-exposed    | c      | d | c+d\n|Total         | a+c   | b+d | a+b+c+d\n\n: A generic 2x2 table {#tbl-2x2-generic}\n\n:::\n\n::: notes\nFrom this table, we have:\n:::\n\n* $\\hat\\pi(Event|Exposed) = a/(a+b)$\n\n* $\\hat\\pi(\\neg Event|Exposed) = b/(a+b)$\n\n* $\\hat\\odds(Event|Exposed) = \\frac{\\left(\\frac{a}{a+b}\\right)}{\\left(\\frac{b}{a+b}\\right)}=\\frac{a}{b}$\n\n* $\\hat\\odds(Event|\\neg Exposed) = \\frac{c}{d}$\n(see @exr-odds-generic)\n\n* $\\theta(Exposed,\\neg Exposed) = \\frac{\\frac{a}{b}}{\\frac{c}{d}} = \\frac{ad}{bc}$\n\n---\n\n:::{#exr-odds-generic}\nGiven @tbl-2x2-generic, show that $\\hat\\odds(Event|\\neg Exposed) = \\frac{c}{d}$.\n:::\n\n---\n\n#### Properties of odds ratios {#sec-OR-props}\n\n:::: notes\nOdds ratios have a special property: we can swap a covariate with the outcome, and the odds ratio remains the same.\n::::\n\n:::{#thm-or-swap}\n##### Odds ratios are reversible\n\n\nFor any two events $A$, $B$:\n\n$$\\theta(A|B) = \\theta(B|A)$$\n\n:::\n\n---\n\n::: {.proof}\n\n\n$$\n\\ba\n\\theta(A|B) &\\eqdef\\frac{\\odds(A|B)}{\\odds(A|\\neg B)}\n\\\\ &= \\frac\n{\\paren{\\frac{\\p(A|B)}{\\p(\\neg A|B)}}}\n{\\paren{\\frac{\\p(A|\\neg B)}{\\p(\\neg A| \\neg B)}}}\n\\\\ &= \n\\paren{\\frac{\\p(A|B)}{\\p(\\neg A|B)}}\n\\inv{\\frac{\\p(A|\\neg B)}{\\p(\\neg A| \\neg B)}}\n\\\\ &= \n\\paren{\\frac{\\p(A|B)}{\\p(\\neg A|B)}}\n\\paren{\\frac{\\p(\\neg A| \\neg B)}{\\p(A|\\neg B)}}\n\\\\ &= \n\\paren{\\frac{\\p(A|B)}{\\p(\\neg A|B)} \\cdot \\frac{\\p(B)}{\\p(B)}}\n\\paren{\\frac{\\p(\\neg A| \\neg B)}{\\p(A|\\neg B)} \\cdot \\frac{\\p(\\neg B)}{\\p(\\neg B)}}\n\\\\ &= \n\\paren{\\frac{\\p(A,B)}{\\p(\\neg A,B)}}\n\\paren{\\frac{\\p(\\neg A, \\neg B)}{\\p(A, \\neg B)}}\n\\\\ &= \n\\paren{\\frac{\\p(B,A)}{\\red{\\p(B,\\neg A)}}}\n\\paren{\\frac{\\p(\\neg B, \\neg A)}{\\blue{\\p(\\neg B, A)}}}\n\\\\ &= \n\\paren{\\frac{\\p(B,A)}{\\blue{\\p(\\neg B, A)}}}\n\\paren{\\frac{\\p(\\neg B, \\neg A)}{\\red{\\p(B,\\neg A)}}}\n\\\\ &= \\text{[reverse the preceding steps]}\n\\\\ &= \\theta(B|A)\n\\ea\n$$\n\n\n\n:::\n\n---\n\n:::{#exm-or-inv-MI}\n\nIn @exm-oc-mi, we have:\n\n$$\n\\begin{aligned}\n\\theta(MI; OC) \n&\\eqdef\n\\frac{\\odds(MI|OC)}{\\odds(MI|\\neg OC)}\\\\\n&\\eqdef \\frac\n{\\left(\\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\right)}\n{\\left(\\frac{\\Pr(MI|\\neg OC)}{\\Pr(\\neg MI|\\neg OC)}\\right)}\\\\\n&= \\frac\n{\\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)}\n{\\left(\\frac{\\Pr(MI,\\neg OC)}{\\Pr(\\neg MI,\\neg OC)}\\right)}\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(MI,\\neg OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(MI,\\neg OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(\\neg MI,OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC,MI)}{\\Pr(\\neg OC,MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC,\\neg MI)}{\\Pr(OC,\\neg MI)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC|\\neg MI)}{\\Pr(OC|\\neg MI)}\\right)\\\\\n&= \\frac{\\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)}\n{\\left(\\frac{\\Pr(OC|\\neg MI)}{\\Pr(\\neg OC|\\neg MI)}\\right)}\\\\\n&\\eqdef \\frac{\\odds(OC|MI)}\n{\\odds(OC|\\neg MI)}\\\\\n&\\eqdef \\theta(OC; MI)\n\\end{aligned}\n$$\n:::\n\n---\n\n:::{#exr-2x2-generic}\nFor @tbl-2x2-generic, show that $\\hat\\theta(Exposed, Unexposed) = \\hat\\theta(Event, \\neg Event)$.\n:::\n\n---\n\n:::: notes\nConditional odds ratios have the same reversibility property:\n::::\n\n:::{#thm-conditional-OR-swap}\n##### Conditional odds ratios are reversible\n\n\nFor any three events $A$, $B$, $C$:\n\n$$\\theta(A|B,C) = \\theta(B|A,C)$$\n:::\n\n---\n\n:::{.proof}\nApply the same steps as for @thm-or-swap, \ninserting $C$ into the conditions (RHS of $|$) of every expression.\n:::\n\n---\n\n#### Odds Ratios vs Probability (Risk) Ratios {#sec-OR-RR}\n\n::: notes\nWhen the outcome is rare (i.e., its probability is small) for both groups being compared in an odds ratio, the odds of the outcome will be similar to the probability of the outcome, and thus the risk ratio will be similar to the odds ratio.\n:::\n\n##### Case 1: rare events\n\nFor rare events, odds ratios and probability (a.k.a. risk, a.k.a.\nprevalence) ratios will be close:\n\n$\\pi_1 = .01$ $\\pi_2 = .02$\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi1 = .01\npi2 = .02\npi2/pi1\n#> [1] 2\nodds(pi2)/odds(pi1)\n#> [1] 2.02\n```\n:::\n\n\n\n\n\n\n\n\n---\n\n:::{#exm-or-rr-OC-MI}\nIn @exm-oc-mi, the outcome is rare for both OC and non-OC participants, so the odds for both groups are similar to the corresponding probabilities, and the odds ratio is similar the risk ratio.\n:::\n\n---\n\n##### Case 2: frequent events\n\n$\\pi_1 = .4$ $\\pi_2 = .5$\n\nFor more frequently-occurring outcomes, this won't be the case:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi1 = .4\npi2 = .5\npi2/pi1\n#> [1] 1.25\nodds(pi2)/odds(pi1)\n#> [1] 1.5\n```\n:::\n\n\n\n\n\n\n\n\n---\n\n#### Odds Ratios in Case-Control Studies\n\n::: notes\n@tbl-oc-mi simulates a follow-up study in which two populations were followed and the number of MIтАЩs was observed. \nThe risks are $P(MI|OC)$ and $P(MI|\\neg OC)$ and we can estimate these risks from the data.\n\nBut suppose we had a case-control study in which we had 100 women with MI and selected a comparison group of 100 women without MI (matched as groups on age, etc.). \nThen MI is not random, and we cannot compute P(MI|OC) and we cannot compute the risk ratio. \nHowever, the odds ratio however can be computed.\n\nThe disease odds ratio is the odds for the disease in the exposed group divided by the odds for the disease in the unexposed group, and we cannot validly compute and use these separate parts.\n\nWe can still validly compute and use the exposure odds ratio, \nwhich is the odds for exposure in the disease group divided by the odds for exposure in the non-diseased group \n(because exposure can be treated as random):\n\n:::\n\n$$\n\\hth(OC|MI) = \n\\frac{\\hat{\\odds}(OC|MI)}{\\hat{\\odds}(OC|\\neg MI)}\n$$\n\n::: notes\nAnd these two odds ratios, $\\hth(MI|OC)$ and $\\hth(OC|MI)$,\nare mathematically equivalent, as we saw in @sec-OR-props:\n:::\n\n$$\\hth(MI|OC) = \\hth(OC|MI)$$\n\n---\n\n:::{#exr-or-rev}\nCalculate the odds ratio of MI with respect to OC use, \nassuming that @tbl-oc-mi comes from a case-control study. \nConfirm that the result is the same as in @exm-OR.\n\n---\n\n::::{.solution}\n\n::: {.content-visible when-format=\"revealjs\"}\n\n\n\n\n\n\n\n\n::: {#tbl-oc-mi2 .cell tbl-cap='Simulated data from study of oral contraceptive use and heart attack risk'}\n\n```{.r .cell-code}\ntbl_oc_mi\n#> # A tibble: 3 x 4\n#>   OC           MI `No MI` Total\n#>   <chr>     <dbl>   <dbl> <dbl>\n#> 1 OC use       13    4987  5000\n#> 2 No OC use     7    9993 10000\n#> 3 Total        20   14980 15000\n```\n:::\n\n\n\n\n\n\n\n:::\n\n* $\\odds(OC|MI) = P(OC|MI)/(1 тАУ P(OC|MI) = \\frac{13}{7} = 1.8571$\n\n* $\\odds(OC|\\neg MI) = P(OC|\\neg MI)/(1 тАУ P(OC|\\neg MI) = \\frac{4987}{9993} = 0.499$\n\n* $\\theta(OC,MI) = \\frac{\\odds(OC|MI)}{\\odds(OC|\\neg MI)} = \\frac{13/7}{4987/9993} = 3.7214$\n\n::: notes\nThis is the same estimate we calculated in @exm-OR.\n:::\n\n::::\n\n:::\n\n---\n\n#### Odds Ratios in Cross-Sectional Studies\n\n* If a cross-sectional study is a probability sample of a population (which it rarely is) then we can estimate risks.\n\n* If it is a sample, but not an unbiased probability sample, then we need to treat it in the same way as a case-control study.\n\n* We can validly estimate odds ratios in either case.\n\n* But we can usually not validly estimate risks and risk ratios.\n\n## The logit and expit functions\n\n### The logit function\n\n:::{#def-logit}\n\n#### logit function\n\n::: notes\nThe **logit function**  of a probability $\\pi$ is \nthe natural logarithm of the odds function of $\\pi$:\n:::\n\n$$\\logit(\\pi) \\eqdef \\log{\\odds(\\pi)}$$\n\n:::\n\n---\n\n::: notes\n@fig-logit shows the shape of the $\\logit()$ function.\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlogit = function(p) log(odds(p))\n\nlogit_plot = \n  ggplot() + \n  geom_function(\n    fun = logit,\n    arrow = arrow(ends = \"both\")) + \n  xlim(.001, .999) + \n  ylab(\"logit(p)\") +\n  xlab(\"p\") +\n  theme_bw()\nprint(logit_plot)\n```\n\n::: {.cell-output-display}\n![The logit function](logistic-regression_files/figure-pdf/fig-logit-1.pdf){#fig-logit}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n:::{#thm-logit-function}\n\n$$\\logit(\\pi) = \\log{\\frac{\\pi}{1-\\pi}}$${#eq-logit}\n\n:::\n\n---\n\n:::{#exr-prove-eq-logit}\n\n#### Compose the logit function\n\nProve @thm-logit-function.\n:::\n\n---\n\n::: proof\nApply @def-logit and then @def-odds (details left to the reader).\n:::\n\n---\n\n:::{#thm-deriv-logit}\n\n#### Derivative of logit function\n\n$$\\logit'(\\pi) = \\frac{1}{(\\pi)(1-\\pi)}$$\n:::\n\n--- \n\n::: proof\n\n$$\n\\ba\n\\logit'(\\pi)\n   &= \\deriv{\\pi}\\logit(\\pi)\n\\\\ &= \\deriv{\\pi}\\log{\\odds(\\pi)}\n\\\\ &= \\frac{\\odds'(\\pi)}{\\odds(\\pi)}\n\\\\ &= \\odds'(\\pi) \\frac{1}{\\odds(\\pi)}\n\\\\ &= \\frac{1}{\\sqf{1-\\pi}}\\frac{1-\\pi}{\\pi}\n\\\\ &= \\frac{1}{(\\pi)(1-\\pi)}\n\\ea\n$$\n\n:::\n\n---\n\n### The expit function\n\n:::{#def-expit}\n\n#### expit, logistic, inverse-logit\n\nThe **expit function** (@fig-expit-plot) of a log-odds $\\logodds$, also known as the **inverse-logit function** or **logistic function**, is the inverse-odds of the exponential of $\\logodds$:\n\n$$\\expit(\\logodds) \\eqdef \\oddsinvf{\\expf{\\logodds}}$$\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nexpit = function(eta)\n  exp(eta) / (1 + exp(eta))\nlibrary(ggplot2)\nexpit_plot =\n  ggplot() +\n  geom_function(fun = expit,\n                arrow = arrow(ends = \"both\")) +\n  xlim(-8, 8) +\n  ylim(0, 1) +\n  ylab(expression(expit(eta))) +\n  xlab(expression(eta)) +\n  theme_bw()\nprint(expit_plot)\n```\n\n::: {.cell-output-display}\n![The expit function](logistic-regression_files/figure-pdf/fig-expit-plot-1.pdf){#fig-expit-plot}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n:::{#thm-logit-expit}\n#### logit and expit are each others' inverses\n\n$$\\logitf{\\expitf{\\logodds}} = \\logodds$$\n\n$$\\expitf{\\logitf{\\pi}} = \\pi$$\n\n:::\n\n---\n\n:::{.proof}\nLeft to the reader.\n:::\n\n---\n\n:::{#thm-expit}\n\n#### Expressions for expit function\n\n$$\n\\ba\n\\expit(\\logodds) \n   &= \\frac{\\exp{\\logodds}}{1+\\exp{\\logodds}}\n\\\\ &= (1 + \\exp{-\\logodds})^{-1}\n\\ea\n$$\n:::\n\n---\n\n::: proof\nApply definitions and @lem-invodds-simplified.\nDetails left to the reader.\n:::\n\n---\n\n\n:::{#lem-one-minus-expit}\n$$1-\\expitf{\\logodds} = \\inv{1+\\exp{\\logodds}}$$\n:::\n\n---\n\n::: {.proof}\n\nUsing @lem-one-minus-odds-inv:\n\n$$\n\\ba\n1 - \\expitf{\\logodds} &= 1 - \\oddsinvf{\\expf{\\logodds}}\n\\\\ &= \\frac{1}{1+\\expf{\\logodds}}\n\\\\ &= \\inv{1 + \\exp{\\logodds}}\n\\ea\n$$\n:::\n\n---\n\n:::{#lem-deriv-expit}\n$$\\dexpitf{\\logodds} = (\\expitf{\\logodds}) (1 - \\expitf{\\logodds})$$\n:::\n\n---\n\n::: proof\n\nUsing @thm-deriv-invodds:\n\n$$\n\\ba\n\\dexpitf{\\logodds}\n   &= \\deriv{\\logodds} \\expitf{\\logodds}   \n\\\\ &= \\deriv{\\logodds} \\invoddsf{\\expf{\\logodds}}\n\\\\ &= \\dinvoddsf{\\expf{\\logodds}}  \\deriv{\\logodds}\\expf{\\logodds}\n\\\\ &=  \\frac{1}{\\sqf{1 + \\expf{\\logodds}}} \\expf{\\logodds}\n\\\\ &=  \\frac{\\expf{\\logodds}}{\\sqf{1 + \\expf{\\logodds}}} \n\\\\ &=  \\frac{\\expf{\\logodds}}{1 + \\expf{\\logodds}} \\frac{1}{1 + \\expf{\\logodds}} \n\\\\ &=  \\expitf{\\logodds} (1-\\expitf{\\logodds})\n\\ea\n$$\n\n:::\n\n---\n\n::: proof\n\nAlternatively, we can use @thm-expit:\n\n$$\n\\ba\n\\dexpitf{\\logodds}\n   &= \\deriv{\\logodds} \\expitf{\\logodds}\n\\\\ &= \\deriv{\\logodds} (1 + \\exp{-\\logodds})^{-1}\n\\\\ &= -(1 + \\exp{-\\logodds})^{-2} \\deriv{\\logodds} (1 + \\exp{-\\logodds})\n\\\\ &= -(1 + \\exp{-\\logodds})^{-2} (-\\exp{-\\logodds})\n\\\\ &= (1 + \\exp{-\\logodds})^{-2} (\\exp{-\\logodds})\n\\\\ &= (1 + \\exp{-\\logodds})^{-1} \\frac{\\exp{-\\logodds}}{1 + \\exp{-\\logodds}}\n\\\\ &= (1 + \\exp{-\\logodds})^{-1} \\frac{1}{1 + \\exp{\\logodds}}\n\\\\ &= \\expitf{\\logodds} (1-\\expitf{\\logodds})\n\\ea\n$$\n:::\n\n### Diagram of expit and logit {#sec-expit-logit-diagram}\n\n::: {.content-visible when-format=\"pdf\"}\n$$\n\\left[ \\pi  \\eqdef\\Pr(Y=1)\\right]\n\\underbrace{\n\\overbrace{\n\\underset{\n\\xleftarrow[ \\frac{\\odds}{1+\\odds}]{}\n}\n{\n\\xrightarrow{\\frac{\\pi}{1-\\pi}}\n}\n\\left[\\odds \\eqdef \\text{odds}(Y=1)\\right]\n\\underset{\n\\xleftarrow[\\exp{\\logodds}]{}\n}\n{\n\\xrightarrow{\\log{\\odds}}\n}\n}^{\\logit(\\pi)}\n}_{\\expit(\\logodds)}\n\\left[\\logodds \\eqdef \\text{log-odds}(Y=1)\\right]\n$$\n:::\n\n::: {.content-hidden when-format=\"pdf\"}\n\n$$\n\\underbrace{\\pi}_{\\atop{\\Pr(Y=1)} }\n\\overbrace{\n\\underbrace{\n\\underset{\n\\xleftarrow[\\frac{\\odds}{1+\\odds}]{}\n}\n{\n\\xrightarrow{\\frac{\\pi}{1-\\pi}}\n}\n\\underbrace{\\odds}_{\\text{odds}(Y=1)}\n\\underset{\n\\xleftarrow[\\exp{\\logodds}]{}\n}\n{\n\\xrightarrow{\\log{\\odds}}\n}\n}_{\\expit(\\logodds)}\n}^{\\logit(\\pi)}\n\\underbrace{\\logodds}_{\\atop{\\text{log-odds}(Y=1)}}\n$$\n\n:::\n\n## Introduction to logistic regression\n\n::: notes\n\n* In @exm-oc-mi, we estimated the risk and the odds of MI for two groups, \ndefined by oral contraceptive use.\n\n* If the predictor is quantitative (dose) or there is more than one predictor, the task becomes more difficult.\n\n* In this case, we will use logistic regression, which is a generalization of the linear regression models you have been using that can account for a binary response instead of a continuous one.\n\n:::\n\n### Binary outcomes models - one group, no covariates {.smaller}\n\n$$\n\\begin{aligned}\n\\P(Y=1) &= \\pi\\\\\n\\P(Y=0) &= 1-\\pi\\\\\n\\P(Y=y) &= \\pi^y (1-\\pi)^{1-y}\\\\\n\\mathbf y  &= (y_1, ..., y_n)\\\\\n\\mathcal L(\\pi;\\mathbf y) &= \\pi^{\\sum y_i} (1-\\pi)^{n - \\sum y_i}\\\\\n\\ell(\\pi, \\mathbf y) &= \\left({\\sum y_i}\\right) \\log{\\pi} + \\left(n - \\sum y_i\\right) \\log{1-\\pi}\\\\\n&= \\left({\\sum y_i}\\right) \\left(\\log{\\pi} - \\log{1-\\pi}\\right) + n \\cdot \\log{1-\\pi}\\\\\n&= \\left({\\sum y_i}\\right) \\log{\\frac{\\pi}{ 1-\\pi}} + n \\cdot \\log{1-\\pi}\n\\\\ &= \\left({\\sum y_i}\\right) \\logit(\\pi) + n \\cdot \\log{1-\\pi}\n\\end{aligned}\n$$\n\n### Binary outcomes - general  {.smaller}\n\n$$\n\\ba\n\\P(Y_i=1) &= \\pi_i\n\\\\ \\P(Y_i=0) &= 1-\\pi_i\n\\ea\n$$\n\n$$\\P(Y_i=y_i) = (\\pi_i)^y_i (1-\\pi_i)^{1-y_i}$$\n\n$$\\Lik_i(\\pi_i) = \\P(Y_i=y_i)$$\n\n$$\n\\ba\n\\ell_i(\\pi_i) \n   &= \\log{\\Lik_i(\\pi_i)}\n\\\\ &= y_i \\log{\\pi_i} + (1-y_i) \\log{1-\\pi_i}\n\\ea\n$$\n\n---\n\nFor $\\iid$ data $\\vec y  = (y_1, ..., y_n)$:\n\n$$\n\\ba\n\\Lik(\\pi;\\vec y) &= \\P(\\dsvn{Y}{y})\n\\\\ &= \\prod_{i=1}^n \\P(Y_i=y_i)\n\\\\ &= \\prod_{i=1}^n \\Lik_i(\\pi_i) \n\\ea\n$$\n\n### Modeling $\\pi_i$ as a function of $X_i$\n\nIf there are only a few distinct $X_i$ values, we can model $\\pi_i$\nseparately for each value of $X_i$.\n\nOtherwise, we need regression.\n\n$$\n\\begin{aligned}\n\\pi(x) &\\equiv \\text{E}(Y=1|X=x)\\\\\n&= f(x^\\top\\beta)\n\\end{aligned}\n$$\n\nTypically, we use the $\\expit$ inverse-link: \n\n$$\\pi(\\vec x) = \\expit(\\vx'\\beta)$${#eq-logistic-link}\n\n\n### Meet the beetles\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmx)\n\ndata(BeetleMortality, package = \"glmx\")\nbeetles = BeetleMortality |>\n  mutate(\n    pct = died/n,\n    survived = n - died\n  )\n\nplot1 = \n  beetles |> \n  ggplot(aes(x = dose, y = pct)) +\n  geom_point(aes(size = n)) +\n  xlab(\"Dose (log mg/L)\") +\n  ylab(\"Mortality rate (%)\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_size(range = c(1,2)) +\n  theme_bw(base_size = 18)\n\nprint(plot1)\n```\n\n::: {.cell-output-display}\n![Mortality rates of adult flour beetles after five hours' exposure to gaseous carbon disulphide (Bliss 1935)](logistic-regression_files/figure-pdf/fig-beetles_1a-1.pdf){#fig-beetles_1a}\n:::\n:::\n\n\n\n\n\n\n\n\n### Why don't we use linear regression?\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeetles_long = beetles  |> \n   reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))) |> \n  as_tibble()\n\nlm1 = beetles_long |> lm(formula = outcome ~ dose)\nf.linear = function(x) predict(lm1, newdata = data.frame(dose = x))\n\nrange1 = range(beetles$dose) + c(-.2, .2)\n\nplot2 = \n  plot1 + \n  geom_function(\n    fun = f.linear, \n    aes(col = \"Straight line\")) +\n  labs(colour=\"Model\", size = \"\")\n\nplot2 |> print()\n\n```\n\n::: {.cell-output-display}\n![Mortality rates of adult flour beetles after five hours' exposure to gaseous carbon disulphide (Bliss 1935)](logistic-regression_files/figure-pdf/fig-beetles_2-1.pdf){#fig-beetles_2}\n:::\n:::\n\n\n\n\n\n\n\n\n### Zoom out\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(plot2 + expand_limits(x = c(1.6, 2))) |> print()\n```\n\n::: {.cell-output-display}\n![Mortality rates of adult flour beetles after five hours' exposure to gaseous carbon disulphide (Bliss 1935)](logistic-regression_files/figure-pdf/fig-beetles_3-1.pdf){#fig-beetles_3}\n:::\n:::\n\n\n\n\n\n\n\n\n### log transformation of dose?\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm2 = beetles_long |> lm(formula = outcome ~ log(dose))\nf.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))\n\nplot3 = plot2 + \n  expand_limits(x = c(1.6, 2)) +\n  geom_function(fun = f.linearlog, aes(col = \"Log-transform dose\"))\n(plot3 + expand_limits(x = c(1.6, 2))) |> print()\n\n```\n\n::: {.cell-output-display}\n![Mortality rates of adult flour beetles after five hours' exposure to gaseous carbon disulphide (Bliss 1935)](logistic-regression_files/figure-pdf/fig-beetles_4-1.pdf){#fig-beetles_4}\n:::\n:::\n\n\n\n\n\n\n\n\n### Logistic regression\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n#| label: fig-beetles_5\n#| fig-cap: \"Mortality rates of adult flour beetles after five hours' exposure to gaseous carbon disulphide (Bliss 1935)\"\n\nbeetles_glm_grouped = beetles |>\n  glm(formula = cbind(died, survived) ~ dose, family = \"binomial\")\nf = function(x)\n  beetles_glm_grouped |>\n  predict(newdata = data.frame(dose = x), type = \"response\")\n\nplot4 = plot3 + geom_function(fun = f, aes(col = \"Logistic regression\"))\nplot4 |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-19-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n### Three parts to regression models\n\n-   What distribution does the outcome have for a specific subpopulation\ndefined by covariates? (outcome model)\n\n-   How does the combination of covariates relate to the mean? (link\nfunction)\n\n-   How do the covariates combine? (linear predictor, interactions)\n\n---\n\n### Logistic regression in R\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbeetles_glm_grouped = \n  beetles |> \n  glm(\n    formula = cbind(died, survived) ~ dose, \n    family = \"binomial\")\n\nlibrary(parameters)\nbeetles_glm_grouped |> \n  parameters() |> \n  print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter   | Log-Odds |   SE |           95% CI |      z |      p |\n|:-----------|:--------:|:----:|:----------------:|:------:|:------:|\n|(Intercept) |   -60.72 | 5.18 | (-71.44, -51.08) | -11.72 | < .001 |\n|dose        |    34.27 | 2.91 |   (28.85, 40.30) |  11.77 | < .001 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\nFitted values:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.values(beetles_glm_grouped)\n#>      1      2      3      4      5      6      7      8 \n#> 0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790\npredict(beetles_glm_grouped, type = \"response\")\n#>      1      2      3      4      5      6      7      8 \n#> 0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790\npredict(beetles_glm_grouped, type = \"link\")\n#>       1       2       3       4       5       6       7       8 \n#> -2.7766 -1.6286 -0.5662  0.4277  1.3564  2.2337  3.0596  3.8444\n\nfit_y = beetles$n * fitted.values(beetles_glm_grouped)\n```\n:::\n\n\n\n\n\n\n\n\n---\n\n### Individual observations\n\n\n\n\n\n\n\n\n::: {#tbl-beetles-long .cell tbl-cap='`beetles` data in long format'}\n\n```{.r .cell-code}\nbeetles_long\n#> # A tibble: 481 x 6\n#>     dose  died     n   pct survived outcome\n#>    <dbl> <int> <int> <dbl>    <int>   <dbl>\n#>  1  1.69     6    59 0.102       53       1\n#>  2  1.69     6    59 0.102       53       1\n#>  3  1.69     6    59 0.102       53       1\n#>  4  1.69     6    59 0.102       53       1\n#>  5  1.69     6    59 0.102       53       1\n#>  6  1.69     6    59 0.102       53       1\n#>  7  1.69     6    59 0.102       53       0\n#>  8  1.69     6    59 0.102       53       0\n#>  9  1.69     6    59 0.102       53       0\n#> 10  1.69     6    59 0.102       53       0\n#> # i 471 more rows\n```\n:::\n\n\n\n\n\n\n\n\n---\n\n\n\n\n\n\n\n\n::: {#tbl-beetles-model-ungrouped .cell tbl-cap='logistic regression model for beetles data with individual Bernoulli data'}\n\n```{.r .cell-code}\nbeetles_glm_ungrouped = \n  beetles_long |> \n  glm(\n    formula = outcome ~ dose, \n    family = \"binomial\")\n\nbeetles_glm_ungrouped |> parameters() |> print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter   | Log-Odds |   SE |           95% CI |      z |      p |\n|:-----------|:--------:|:----:|:----------------:|:------:|:------:|\n|(Intercept) |   -60.72 | 5.18 | (-71.44, -51.08) | -11.72 | < .001 |\n|dose        |    34.27 | 2.91 |   (28.85, 40.30) |  11.77 | < .001 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n::: notes\nHere's the previous version again:\n:::\n\n\n\n\n\n\n\n\n::: {#tbl-beetles-model-grouped .cell tbl-cap='logistic regression model for beetles data with grouped (binomial) data'}\n\n```{.r .cell-code}\nbeetles_glm_grouped |> parameters() |> print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter   | Log-Odds |   SE |           95% CI |      z |      p |\n|:-----------|:--------:|:----:|:----------------:|:------:|:------:|\n|(Intercept) |   -60.72 | 5.18 | (-71.44, -51.08) | -11.72 | < .001 |\n|dose        |    34.27 | 2.91 |   (28.85, 40.30) |  11.77 | < .001 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n::: notes\nThey seem the same! But not quite:\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nlogLik(beetles_glm_grouped)\n#> 'log Lik.' -18.72 (df=2)\nlogLik(beetles_glm_ungrouped)\n#> 'log Lik.' -186.2 (df=2)\n```\n:::\n\n\n\n\n\n\n\n\n::: notes\nThe difference is due to the binomial coefficient\n$\\left(n\\atop x \\right)$ which isn't included in the\nindividual-observations (Bernoulli) version of the model.\n:::\n\n## Multiple logistic regression\n\n### Coronary heart disease (WCGS) study data\n\n::: notes\nLet's use the data from the Western Collaborative Group Study (WCGS) \n(@rosenman1975coronary) to explore multiple logistic regression:\n:::\n\n*From @vittinghoff2e:*\n\n\"The **Western Collaborative Group Study (WCGS)** was a large\nepidemiological study designed to investigate the association between\nthe \"type A\" behavior pattern and coronary heart disease (CHD)\".\n\n*From Wikipedia, \"Type A and Type B personality theory\":*\n\n\"The hypothesis describes Type A individuals as outgoing, ambitious,\nrigidly organized, highly status-conscious, impatient, anxious,\nproactive, and concerned with time management....\n\nThe hypothesis describes Type B individuals as a contrast to those of\nType A. Type B personalities, by definition, are noted to live at lower\nstress levels. They typically work steadily and may enjoy achievement,\nalthough they have a greater tendency to disregard physical or mental\nstress when they do not achieve.\"\n\n---\n\n#### Study design\n\n*from `?faraway::wcgs`:*\n\n3154 healthy young men aged 39-59 from the San Francisco area were\nassessed for their personality type. All were free from coronary heart\ndisease at the start of the research. Eight and a half years later\nchange in CHD status was recorded.\n\n*Details (from `faraway::wcgs`)*\n\nThe WCGS began in 1960 with 3,524 male volunteers who were employed by\n11 California companies. Subjects were 39 to 59 years old and free of\nheart disease as determined by electrocardiogram. After the initial\nscreening, the study population dropped to 3,154 and the number of\ncompanies to 10 because of various exclusions. The cohort comprised both\nblue- and white-collar employees.\n\n---\n\n### Baseline data collection\n\nsocio-demographic characteristics:\n\n-   age\n-   education\n-   marital status\n-   income\n-   occupation\n-   physical and physiological including:\n-   height\n-   weight\n-   blood pressure\n-   electrocardiogram\n-   corneal arcus;\n\n---\n\nbiochemical measurements:\n\n-   cholesterol and lipoprotein fractions;\n-   medical and family history and use of medications;\n\n---\n\nbehavioral data:\n\n-   Type A interview,\n-   smoking,\n-   exercise\n-   alcohol use.\n\n---\n\nLater surveys added data on: \n\n* anthropometry \n* triglycerides \n* Jenkins Activity Survey \n* caffeine use\n\nAverage follow-up continued for 8.5 years with repeat examinations.\n\n\n### Load the data\n\nHere, I load the data:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n### load the data directly from a UCSF website:\nlibrary(haven)\nurl = paste0(\n    # I'm breaking up the url into two chunks for readability\n    \"https://regression.ucsf.edu/sites/g/files/\",\n    \"tkssra6706/f/wysiwyg/home/data/wcgs.dta\")\nwcgs = haven::read_dta(url)\n\n```\n:::\n\n\n\n::: {#tbl-wcgs .cell tbl-cap='`wcgs` data'}\n\n```{.r .cell-code}\nwcgs |> head()\n#> # A tibble: 6 x 22\n#>     age arcus behpat   bmi chd69  chol   dbp dibpat height    id lnsbp lnwght\n#>   <dbl> <lgl> <fct>  <dbl> <fct> <dbl> <dbl> <fct>   <dbl> <dbl> <dbl>  <dbl>\n#> 1    50 TRUE  A1      31.3 No      249    90 Type A     67  2343  4.88   5.30\n#> 2    51 FALSE A1      25.3 No      194    74 Type A     73  3656  4.79   5.26\n#> 3    59 TRUE  A1      28.7 No      258    94 Type A     70  3526  5.06   5.30\n#> 4    51 TRUE  A1      22.1 No      173    80 Type A     69 22057  4.84   5.01\n#> 5    44 FALSE A1      22.3 No      214    80 Type A     71 12927  4.84   5.08\n#> 6    47 FALSE A1      27.1 No      206    76 Type A     64 16029  4.75   5.06\n#> # i 10 more variables: ncigs <dbl>, sbp <dbl>, smoke <fct>, t1 <dbl>,\n#> #   time169 <dbl>, typchd69 <fct>, uni <dbl>, weight <dbl>, wghtcat <fct>,\n#> #   agec <fct>\n```\n:::\n\n\n\n\n\n\n\n\n\n### Data cleaning\n\n::: notes\nNow let's do some data cleaning\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(arsenal) # provides `set_labels()`\nlibrary(forcats) # provides `as_factor()`\nlibrary(haven)\nlibrary(plotly)\nwcgs = wcgs |> \n  mutate(\n    age = age |> \n      arsenal::set_labels(\"Age (years)\"),\n    \n    arcus = \n      arcus |> \n      as.logical() |> \n      arsenal::set_labels(\"Arcus Senilis\"),\n    \n    time169 = \n      time169 |> \n      as.numeric() |> \n      arsenal::set_labels(\"Observation (follow up) time (days)\"),\n    \n    dibpat =\n      dibpat |> \n      as_factor() |> \n      relevel(ref = \"Type B\") |> \n      arsenal::set_labels(\"Behavioral Pattern\"),\n    \n    typchd69 = typchd69 |> \n      labelled(\n        label = \"Type of CHD Event\",\n        labels = \n          c(\n            \"None\" = 0, \n            \"infdeath\" = 1,\n            \"silent\" = 2,\n            \"angina\" = 3)),\n    \n    # turn stata-style labelled variables in to R-style factors:\n    across(\n      where(is.labelled), \n      haven::as_factor)\n  )\n\n```\n:::\n\n\n\n\n\n\n\n\n### What's in the data {.smaller}\n\nHere's a table of the data:\n\n\n\n\n\n\n\n\n\n```{.r .cell-code}\nwcgs |>\n  select(-c(id, uni, t1)) |>\n  tableby(chd69 ~ ., data = _) |>\n  summary(\n    pfootnote = TRUE,\n    title =\n      \"Baseline characteristics by CHD status at end of follow-up\")\n```\n\n\n\nTable: Baseline characteristics by CHD status at end of follow-up\n\n|                                        |    No (N=2897)     |    Yes (N=257)     |   Total (N=3154)   |    p value|\n|:---------------------------------------|:------------------:|:------------------:|:------------------:|----------:|\n|**Age (years)**                         |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |   46.082 (5.457)   |   48.490 (5.801)   |   46.279 (5.524)   |           |\n|&nbsp;&nbsp;&nbsp;Range                 |  39.000 - 59.000   |  39.000 - 59.000   |  39.000 - 59.000   |           |\n|**Arcus Senilis**                       |                    |                    |                    | < 0.001^2^|\n|&nbsp;&nbsp;&nbsp;N-Miss                |         0          |         2          |         2          |           |\n|&nbsp;&nbsp;&nbsp;FALSE                 |    2058 (71.0%)    |    153 (60.0%)     |    2211 (70.1%)    |           |\n|&nbsp;&nbsp;&nbsp;TRUE                  |    839 (29.0%)     |    102 (40.0%)     |    941 (29.9%)     |           |\n|**Behavioral Pattern**                  |                    |                    |                    | < 0.001^2^|\n|&nbsp;&nbsp;&nbsp;A1                    |     234 (8.1%)     |     30 (11.7%)     |     264 (8.4%)     |           |\n|&nbsp;&nbsp;&nbsp;A2                    |    1177 (40.6%)    |    148 (57.6%)     |    1325 (42.0%)    |           |\n|&nbsp;&nbsp;&nbsp;B3                    |    1155 (39.9%)    |     61 (23.7%)     |    1216 (38.6%)    |           |\n|&nbsp;&nbsp;&nbsp;B4                    |    331 (11.4%)     |     18 (7.0%)      |    349 (11.1%)     |           |\n|**Body Mass Index (kg/m2)**             |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |   24.471 (2.561)   |   25.055 (2.579)   |   24.518 (2.567)   |           |\n|&nbsp;&nbsp;&nbsp;Range                 |  11.191 - 37.653   |  19.225 - 38.947   |  11.191 - 38.947   |           |\n|**Total Cholesterol**                   |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;N-Miss                |         12         |         0          |         12         |           |\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |  224.261 (42.217)  |  250.070 (49.396)  |  226.372 (43.420)  |           |\n|&nbsp;&nbsp;&nbsp;Range                 | 103.000 - 400.000  | 155.000 - 645.000  | 103.000 - 645.000  |           |\n|**Diastolic Blood Pressure**            |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |   81.723 (9.621)   |  85.315 (10.311)   |   82.016 (9.727)   |           |\n|&nbsp;&nbsp;&nbsp;Range                 |  58.000 - 150.000  |  64.000 - 122.000  |  58.000 - 150.000  |           |\n|**Behavioral Pattern**                  |                    |                    |                    | < 0.001^2^|\n|&nbsp;&nbsp;&nbsp;Type B                |    1486 (51.3%)    |     79 (30.7%)     |    1565 (49.6%)    |           |\n|&nbsp;&nbsp;&nbsp;Type A                |    1411 (48.7%)    |    178 (69.3%)     |    1589 (50.4%)    |           |\n|**Height (inches)**                     |                    |                    |                    |   0.290^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |   69.764 (2.539)   |   69.938 (2.410)   |   69.778 (2.529)   |           |\n|&nbsp;&nbsp;&nbsp;Range                 |  60.000 - 78.000   |  63.000 - 77.000   |  60.000 - 78.000   |           |\n|**Ln of Systolic Blood Pressure**       |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |   4.846 (0.110)    |   4.900 (0.125)    |   4.850 (0.112)    |           |\n|&nbsp;&nbsp;&nbsp;Range                 |   4.585 - 5.438    |   4.605 - 5.298    |   4.585 - 5.438    |           |\n|**Ln of Weight**                        |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |   5.126 (0.123)    |   5.155 (0.118)    |   5.128 (0.123)    |           |\n|&nbsp;&nbsp;&nbsp;Range                 |   4.357 - 5.670    |   4.868 - 5.768    |   4.357 - 5.768    |           |\n|**Cigarettes per day**                  |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |  11.151 (14.329)   |  16.665 (15.657)   |  11.601 (14.518)   |           |\n|&nbsp;&nbsp;&nbsp;Range                 |   0.000 - 99.000   |   0.000 - 60.000   |   0.000 - 99.000   |           |\n|**Systolic Blood Pressure**             |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |  128.034 (14.746)  |  135.385 (17.473)  |  128.633 (15.118)  |           |\n|&nbsp;&nbsp;&nbsp;Range                 |  98.000 - 230.000  | 100.000 - 200.000  |  98.000 - 230.000  |           |\n|**Current smoking**                     |                    |                    |                    | < 0.001^2^|\n|&nbsp;&nbsp;&nbsp;No                    |    1554 (53.6%)    |     98 (38.1%)     |    1652 (52.4%)    |           |\n|&nbsp;&nbsp;&nbsp;Yes                   |    1343 (46.4%)    |    159 (61.9%)     |    1502 (47.6%)    |           |\n|**Observation (follow up) time (days)** |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             | 2775.158 (562.205) | 1654.700 (859.297) | 2683.859 (666.524) |           |\n|&nbsp;&nbsp;&nbsp;Range                 | 238.000 - 3430.000 | 18.000 - 3229.000  | 18.000 - 3430.000  |           |\n|**Type of CHD Event**                   |                    |                    |                    |           |\n|&nbsp;&nbsp;&nbsp;None                  |      0 (0.0%)      |      0 (0.0%)      |      0 (0.0%)      |           |\n|&nbsp;&nbsp;&nbsp;infdeath              |   2897 (100.0%)    |      0 (0.0%)      |    2897 (91.9%)    |           |\n|&nbsp;&nbsp;&nbsp;silent                |      0 (0.0%)      |    135 (52.5%)     |     135 (4.3%)     |           |\n|&nbsp;&nbsp;&nbsp;angina                |      0 (0.0%)      |     71 (27.6%)     |     71 (2.3%)      |           |\n|&nbsp;&nbsp;&nbsp;4                     |      0 (0.0%)      |     51 (19.8%)     |     51 (1.6%)      |           |\n|**Weight (lbs)**                        |                    |                    |                    | < 0.001^1^|\n|&nbsp;&nbsp;&nbsp;Mean (SD)             |  169.554 (21.010)  |  174.463 (21.573)  |  169.954 (21.096)  |           |\n|&nbsp;&nbsp;&nbsp;Range                 |  78.000 - 290.000  | 130.000 - 320.000  |  78.000 - 320.000  |           |\n|**Weight Category**                     |                    |                    |                    | < 0.001^2^|\n|&nbsp;&nbsp;&nbsp;< 140                 |     217 (7.5%)     |     15 (5.8%)      |     232 (7.4%)     |           |\n|&nbsp;&nbsp;&nbsp;140-170               |    1440 (49.7%)    |     98 (38.1%)     |    1538 (48.8%)    |           |\n|&nbsp;&nbsp;&nbsp;170-200               |    1049 (36.2%)    |    122 (47.5%)     |    1171 (37.1%)    |           |\n|&nbsp;&nbsp;&nbsp;> 200                 |     191 (6.6%)     |     22 (8.6%)      |     213 (6.8%)     |           |\n|**RECODE of age (Age)**                 |                    |                    |                    | < 0.001^2^|\n|&nbsp;&nbsp;&nbsp;35-40                 |    512 (17.7%)     |     31 (12.1%)     |    543 (17.2%)     |           |\n|&nbsp;&nbsp;&nbsp;41-45                 |    1036 (35.8%)    |     55 (21.4%)     |    1091 (34.6%)    |           |\n|&nbsp;&nbsp;&nbsp;46-50                 |    680 (23.5%)     |     70 (27.2%)     |    750 (23.8%)     |           |\n|&nbsp;&nbsp;&nbsp;51-55                 |    463 (16.0%)     |     65 (25.3%)     |    528 (16.7%)     |           |\n|&nbsp;&nbsp;&nbsp;56-60                 |     206 (7.1%)     |     36 (14.0%)     |     242 (7.7%)     |           |\n1. Linear Model ANOVA\n2. Pearson's Chi-squared test\n\n\n\n\n\n\n\n\n### Data by age and personality type\n\nFor now, we will look at the interaction between age and personality\ntype (`dibpat`). To make it easier to visualize the data, we summarize\nthe event rates for each combination of age:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchd_grouped_data = \n  wcgs |> \n  summarize(\n    .by = c(age, dibpat),\n    n = sum(chd69 %in% c(\"Yes\", \"No\")),\n    x = sum(chd69 == \"Yes\")) |> \n  mutate(\n    `n - x` = n - x,\n    `p(chd)` = (x / n) |> \n      labelled(label = \"CHD Event by 1969\"),\n    `odds(chd)` = `p(chd)` / (1 - `p(chd)`), \n    `logit(chd)` = log(`odds(chd)`)\n  )\n\nchd_grouped_data\n#> # A tibble: 42 x 8\n#>      age dibpat     n     x `n - x` `p(chd)`  `odds(chd)` `logit(chd)`\n#>    <dbl> <fct>  <int> <int>   <int> <dbl+lbl>       <dbl>        <dbl>\n#>  1    50 Type A    76     8      68 0.105          0.118         -2.14\n#>  2    51 Type A    67    11      56 0.164          0.196         -1.63\n#>  3    59 Type A    30     7      23 0.233          0.304         -1.19\n#>  4    44 Type A   113     9     104 0.0796         0.0865        -2.45\n#>  5    47 Type A    72     7      65 0.0972         0.108         -2.23\n#>  6    40 Type A   133     9     124 0.0677         0.0726        -2.62\n#>  7    41 Type A   108     7     101 0.0648         0.0693        -2.67\n#>  8    43 Type A    97     7      90 0.0722         0.0778        -2.55\n#>  9    54 Type A    53     7      46 0.132          0.152         -1.88\n#> 10    48 Type A    80    12      68 0.15           0.176         -1.73\n#> # i 32 more rows\n```\n:::\n\n\n\n\n\n\n\n\n### Graphical exploration\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ggeasy)\nlibrary(scales)\nchd_plot_probs = \n  chd_grouped_data |> \n  ggplot(\n    aes(\n      x = age, \n      y = `p(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"P(CHD Event by 1969)\") +\n  scale_y_continuous(\n    labels = scales::label_percent(),\n    sec.axis = sec_axis(\n      ~ odds(.),\n      name = \"odds(CHD Event by 1969)\")) +\n  ggeasy::easy_labs() +\n  theme(legend.position = \"bottom\")\n\nprint(chd_plot_probs)\n```\n\n::: {.cell-output-display}\n![CHD rates by age group, probability scale](logistic-regression_files/figure-pdf/fig-chd-graph-prob-scale-1.pdf){#fig-chd-graph-prob-scale}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n#### Odds scale\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrans_odds = trans_new(\n  name = \"odds\", \n  transform = odds, \n  inverse = odds_inv)\n\nchd_plot_odds = chd_plot_probs + \n  scale_y_continuous(\n    trans = trans_odds, # this line changes the vertical spacing\n    name = chd_plot_probs$labels$y,\n    sec.axis = sec_axis(\n      ~ odds(.),\n      name = \"odds(CHD Event by 1969)\"))\n\nprint(chd_plot_odds)\n```\n\n::: {.cell-output-display}\n![CHD rates by age group, odds spacing](logistic-regression_files/figure-pdf/fig-chd-odds-scale-1.pdf){#fig-chd-odds-scale}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n#### Log-odds (logit) scale\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrans_logit = trans_new(\n  name = \"logit\", \n  transform = logit, \n  inverse = expit)\n\nchd_plot_logit = \n  chd_plot_probs + \n  scale_y_continuous(\n    trans = trans_logit, # this line changes the vertical spacing\n    name = chd_plot_probs$labels$y,\n    breaks = c(seq(.01, .1, by = .01), .15, .2),\n    minor_breaks = NULL,\n    sec.axis = sec_axis(\n      ~ logit(.),\n      name = \"log(odds(CHD Event by 1969))\"))\n\nprint(chd_plot_logit)\n```\n\n::: {.cell-output-display}\n![CHD data (logit-scale)](logistic-regression_files/figure-pdf/fig-chd_plot_logit-1.pdf){#fig-chd_plot_logit}\n:::\n:::\n\n\n\n\n\n\n\n\n### Logistic regression models for CHD data {.smaller}\n\nHere, we fit stratified models for CHD by personality type.\n\n\n\n\n\n\n\n\n::: {#tbl-chd-strat .cell tbl-cap='CHD model, stratified parametrization'}\n\n```{.r .cell-code}\nchd_glm_strat = glm(\n  \"formula\" = chd69 == \"Yes\" ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat |> parameters() |> print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter             | Log-Odds |   SE |         95% CI |     z |      p |\n|:---------------------|:--------:|:----:|:--------------:|:-----:|:------:|\n|dibpat (Type B)       |    -5.80 | 0.98 | (-7.73, -3.90) | -5.95 | < .001 |\n|dibpat (Type A)       |    -5.50 | 0.67 | (-6.83, -4.19) | -8.18 | < .001 |\n|dibpat (Type B) ├Ч age |     0.06 | 0.02 |   (0.02, 0.10) |  3.01 | 0.003  |\n|dibpat (Type A) ├Ч age |     0.07 | 0.01 |   (0.05, 0.10) |  5.24 | < .001 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nWe can get the corresponding odds ratios ($e^{\\beta}$s) by passing\n`exponentiate = TRUE` to `parameters()`:\n\n\n\n\n\n\n\n\n::: {#tbl-chd-glm-strat-OR .cell tbl-cap='Odds ratio estimates for CHD model'}\n\n```{.r .cell-code}\nchd_glm_strat |> \n  parameters(exponentiate = TRUE) |> \n  print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter             | Odds Ratio |       SE |           95% CI |     z |      p |\n|:---------------------|:----------:|:--------:|:----------------:|:-----:|:------:|\n|dibpat (Type B)       |   3.02e-03 | 2.94e-03 | (4.40e-04, 0.02) | -5.95 | < .001 |\n|dibpat (Type A)       |   4.09e-03 | 2.75e-03 | (1.08e-03, 0.02) | -8.18 | < .001 |\n|dibpat (Type B) ├Ч age |       1.06 |     0.02 |     (1.02, 1.11) |  3.01 | 0.003  |\n|dibpat (Type A) ├Ч age |       1.07 |     0.01 |     (1.05, 1.10) |  5.24 | < .001 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n### Models superimposed on data\n\nWe can graph our fitted models on each scale (probability, odds,\nlog-odds).\n\n---\n\n#### probability scale\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ncurve_type_A = function(x) \n{\n  chd_glm_strat |> predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type A\"))\n}\n\ncurve_type_B = function(x) \n{\n  chd_glm_strat |> predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type B\"))\n}\n\nchd_plot_probs_2 =\n  chd_plot_probs +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\nprint(chd_plot_probs_2)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-37-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n#### odds scale\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# curve_type_A = function(x) \n# {\n#   chd_glm_strat |> predict(\n#     type = \"link\",\n#     newdata = tibble(age = x, dibpat = \"Type A\")) |> exp()\n# }\n# curve_type_B = function(x) \n# {\n#   chd_glm_strat |> predict(\n#     type = \"link\",\n#     newdata = tibble(age = x, dibpat = \"Type B\")) |> exp()\n# }\n\nchd_plot_odds_2 =\n  chd_plot_odds +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\nprint(chd_plot_odds_2)\n\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/fig-fitted-odds-chd-1.pdf){#fig-fitted-odds-chd}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n#### log-odds (logit) scale\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchd_plot_logit_2 =\n  chd_plot_logit +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\nprint(chd_plot_logit_2)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/fig-fitted-log-odds-1.pdf){#fig-fitted-log-odds}\n:::\n:::\n\n\n\n\n\n\n\n\n### reference-group and contrast parametrization\n\nWe can also use the corner-point parametrization (with reference groups\nand contrasts):\n\n\n\n\n\n\n\n\n::: {#tbl-model-corner-point .cell tbl-cap='CHD model (corner-point parametrization)'}\n\n```{.r .cell-code}\nchd_glm_contrasts = \n  wcgs |> \n  glm(\n    \"data\" = _,\n    \"formula\" = chd69 == \"Yes\" ~ dibpat*I(age - 50), \n    \"family\" = binomial(link = \"logit\")\n  )\n\nchd_glm_contrasts |> \n  parameters() |> \n  print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter                  | Log-Odds |   SE |         95% CI |      z |      p |\n|:--------------------------|:--------:|:----:|:--------------:|:------:|:------:|\n|(Intercept)                |    -2.73 | 0.13 | (-2.98, -2.49) | -21.45 | < .001 |\n|dibpat (Type A)            |     0.82 | 0.15 |   (0.53, 1.13) |   5.42 | < .001 |\n|age - 50                   |     0.06 | 0.02 |   (0.02, 0.10) |   3.01 | 0.003  |\n|dibpat (Type A) ├Ч age - 50 |     0.01 | 0.02 |  (-0.04, 0.06) |   0.42 | 0.674  |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n::: notes\nCompare with @tbl-chd-glm-strat-OR.\n:::\n\n::: {.content-visible when-format=\"revealjs\"}\n\n\n\n\n\n\n\n\n::: {#tbl-chd-glm-strat-OR-v2 .cell tbl-cap='Odds ratio estimates for CHD model'}\n\n```{.r .cell-code}\nchd_glm_strat |> \n  parameters() |> \n  print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter             | Log-Odds |   SE |         95% CI |     z |      p |\n|:---------------------|:--------:|:----:|:--------------:|:-----:|:------:|\n|dibpat (Type B)       |    -5.80 | 0.98 | (-7.73, -3.90) | -5.95 | < .001 |\n|dibpat (Type A)       |    -5.50 | 0.67 | (-6.83, -4.19) | -8.18 | < .001 |\n|dibpat (Type B) ├Ч age |     0.06 | 0.02 |   (0.02, 0.10) |  3.01 | 0.003  |\n|dibpat (Type A) ├Ч age |     0.07 | 0.01 |   (0.05, 0.10) |  5.24 | < .001 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n:::\n\n---\n\n:::{#exr-strat-to-contrast}\nIf I give you model 1, how would you get the coefficients of model 2?\n:::\n\n---\n\n:::{#thm-logistic-OR}\n\nFor the logistic regression model:\n\n- $Y_i|\\vX_i \\simind \\Ber(\\pi(\\vX_i))$\n- $\\pi(\\vx) = \\expitf{\\vx'\\vb}$\n\nConsider two covariate patterns, $\\vx$ and $\\vec{x^*}$.\n\nThe odds ratio comparing these covariate patterns is:\n\n$$\n\\omega(\\vx,\\vec{x^*}) = \\exp{(\\vx-\\vec{x^*})\\' \\vb}\n$$\n\n:::\n\n---\n\n::: proof\n$$\n\\ba\n\\omega(\\vx,\\vec{x^*}) \n&= \\frac {\\odds(Y=1 | \\vX = \\vx)} {\\odds(Y=1 | \\vX = \\vec{x^*})}\n\\\\ &= \\frac{\\exp{\\vx\\'\\vb}}{\\exp{{\\vec{x^*}}\\' \\vb}}\n\\\\ &= \\exp{\\vx\\'\\vb - {\\vec{x^*}}\\' \\vb}\n\\\\ &= \\exp{(\\vx\\' - {\\vec{x^*}}\\') \\vb}\n\\\\ &= \\exp{{(\\vx - \\vec{x^*})}\\' \\vb}\n\\ea\n$$\n:::\n\n## Fitting logistic regression models\n\n### Maximum likelihood estimation for $\\ciid$ data\n\nAssume:\n\n- $Y_i|\\vX_i \\simind \\Ber(\\pi(X_i))$\n- $\\pi(\\vx) = \\expitf{\\vx'\\vb}$\n\n---\n\n#### log-likelihood function\n\n$$\n\\ba\n\\ell(\\vb, \\vy) \n   &= \\log{\\Lik(\\vb, \\vec y) }\n\\\\ &= \\sumin \\red{\\ell_i}(\\pi(\\vx_i))\n\\ea\n$$ {#eq-loglik-bernoulli-iid}\n\n---\n\n$$\n\\ba\n\\red{\\ell_i}(\\pi)\n   &= y_i \\log{\\pi} + (1 - y_i) \\log{1-\\pi}\n\\\\ &= y_i \\log{\\pi} + (1 \\cd \\log{1-\\pi} - y_i \\cd \\log{1-\\pi})\n\\\\ &= y_i \\log{\\pi} + (\\log{1-\\pi} - y_i \\log{1-\\pi})\n\\\\ &= y_i \\log{\\pi} + \\log{1-\\pi} - y_i \\log{\\blue{1-\\pi}}\n\\\\ &= y_i \\log{\\pi} - y_i \\log{\\blue{1-\\pi}} + \\log{1-\\pi}\n\\\\ &= (y_i \\log{\\pi} - y_i \\log{\\blue{1-\\pi}}) + \\log{1-\\pi}\n\\\\ &= y_i (\\log{\\red{\\pi}} - \\log{\\blue{1-\\pi}}) + \\log{1-\\pi}\n\\\\ &= y_i \\paren{\\log{\\frac{\\red{\\pi}}{\\blue{1-\\pi}}}} + \\log{1-\\pi}\n\\\\ &= y_i (\\logit(\\pi)) + \\log{1-\\pi}\n\\ea\n$$\n\n\n---\n\n#### score function\n\n$$\n\\ba\n\\ell'(\\vb) \n   &\\eqdef \\deriv{\\vb} \\ell(\\vb)\n\\\\ &=      \\deriv{\\vb} \\sumin \\ell_i(\\vb)\n\\\\ &=      \\sumin \\deriv{\\vb} \\ell_i(\\vb)\n\\\\ &=      \\sumin \\ell'_i(\\vb)\n\\ea\n$$\n\n---\n\n$$\n\\ba\n\\ell_i'(\\vb) \n   &= \\deriv{\\vb} y_i \\paren{\\logitf{\\pi_i}} + \\log{1-\\pi_i}\n\\\\ &= \\deriv{\\vb}\\cb{y_i \\paren{\\vx_i'\\vb} + \\log{1-\\pi_i}}\n\\\\ &= \\cb{y_i \\deriv{\\vb}\\paren{\\vx_i'\\vb} + \\deriv{\\vb}\\log{1-\\pi_i}}\n\\\\ &= \\cb{\\vx_i y_i + \\deriv{\\vb}\\log{1-\\expit(\\vx_i'\\vb)}}\n\\\\ &= \\cb{\\vx_i y_i + \\deriv{\\vb}\\log{\\inv{1+\\exp{\\vx_i'\\vb}}}}\n\\\\ &= \\cb{\\vx_i y_i - \\deriv{\\vb}\\log{1+\\exp{\\vx_i'\\vb}}}\n\\ea\n$$\n\n---\n\nNow we need to apply the [chain rule](math-prereqs.qmd#thm-chain-rule):\n\n$$\n\\deriv{\\beta}\\log{1+\\exp{\\vx_i'\\beta}} = \n\\frac{1}{1+\\exp{\\vx_i'\\beta}} \\deriv{\\beta}\\cb{1+\\exp{\\vx_i'\\beta}}\n$$\n\n$$\n\\ba\n\\deriv{\\beta}\\cb{1+\\exp{\\vx_i'\\beta}}\n   &= \\exp{\\vx_i'\\beta} \\deriv{\\beta}\\vx_i'\\beta\n\\\\ &= \\vx_i \\exp{\\vx_i'\\beta} \n\\ea\n$$\n\nSo:\n\n$$\n\\ba\n\\deriv{\\beta}\\log{1+\\exp{\\vx_i'\\beta}} \n   &= \\frac{1}{1+\\exp{\\vx_i'\\beta}} \\exp{\\vx_i'\\beta} \\vx_i\n\\\\ &= \\frac{\\exp{\\vx_i'\\beta}}{1+\\exp{\\vx_i'\\beta}}  \\vx_i\n\\\\ &= \\vx_i \\expitf{\\vx_i'\\beta}\n\\ea\n$$\n\n---\n\nSo:\n\n$$\n\\ba\n\\llik_i'(\\vb) \n&= \\vx_i y_i - \\vx_i \\expitf{\\vx_i'\\beta} \n\\\\ &= \\vx_i (y_i - \\expitf{\\vx_i'\\beta})\n\\\\ &= \\vx_i (y_i - \\pi_i)\n\\\\ &= \\vx_i (y_i - \\Expp[Y_i|\\vX_i=\\vx_i])\n\\\\ &= \\vx_i \\ \\err(y_i|\\vX_i=\\vx_i)\n\\ea\n$$\n\n::: notes\n\nThis last expression is essentially the same as we found in [linear regression](Linear-models-overview.qmd#eq-scorefun-linreg).\n:::\n\n---\n\nPutting the pieces of $\\llik'(\\vb)$ back together, we have:\n\n$$\n\\llik'(\\vb) = \\sumin \\cb{\\vx_i(y_i - \\expitf{\\vx_i'\\beta}) }\n$$\n\nSetting $\\ell'(\\vb; \\vy) = 0$ gives us:\n\n\n$$\\sumin \\cb{\\vx_i(y_i - \\expitf{\\vx_i'\\beta}) } = 0$$ {#eq-score-logistic}\n\n\n---\n\n::: notes\n\nIn general, the estimating equation $\\ell'(\\vb; \\vy) = 0$ cannot\nbe solved analytically.\n\nInstead, we can use the [Newton-Raphson method](intro-MLEs.qmd#sec-newton-raphson):\n\n:::\n\n$$\n\\esttmp{\\theta} \n\\leftarrow \\esttmp{\\theta} - \\inv{\\hessf{\\vec y;\\esttmp{\\theta}}} \n\\scoref{\\vec y;\\esttmp{\\theta}}\n$$\n\n::: notes\n\nWe make an iterative series of\nguesses, and each guess helps us make the next guess better (i.e., higher\nlog-likelihood). You can see some information about this process like so:\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\noptions(digits = 8)\ntemp = \n  wcgs |> \n  glm(\n    control = glm.control(trace = TRUE),\n    data = _,\n    formula = chd69 == \"Yes\" ~ dibpat*age, \n    family = binomial(link = \"logit\")\n  )\n#> Deviance = 1775.7899 Iterations - 1\n#> Deviance = 1708.5396 Iterations - 2\n#> Deviance = 1704.0434 Iterations - 3\n#> Deviance = 1703.9833 Iterations - 4\n#> Deviance = 1703.9832 Iterations - 5\n#> Deviance = 1703.9832 Iterations - 6\n```\n:::\n\n\n\n\n\n\n\n\n::: notes\n\nAfter each iteration of the fitting procedure, the deviance\n($2(\\ell_{\\text{full}} - \\ell(\\hat\\beta))$ ) is printed. You can see\nthat the algorithm took six iterations to converge to a solution where\nthe likelihood wasn't changing much anymore.\n\n:::\n\n## Model comparisons for logistic models {#sec-gof}\n\n### Deviance test\n\nWe can compare the maximized log-likelihood of our model,\n$\\ell(\\hat\\beta; \\mathbf x)$, versus the log-likelihood of the full\nmodel (aka saturated model aka maximal model), $\\ell_{\\text{full}}$,\nwhich has one parameter per covariate pattern. With enough data,\n$2(\\ell_{\\text{full}} - \\ell(\\hat\\beta; \\mathbf x)) \\dot \\sim \\chi^2(N - p)$,\nwhere $N$ is the number of distinct covariate patterns and $p$ is the\nnumber of $\\beta$ parameters in our model. A significant p-value for\nthis **deviance** statistic indicates that there's some detectable\npattern in the data that our model isn't flexible enough to catch.\n\n::: callout-caution\nThe deviance statistic needs to have a large amount of data **for each\ncovariate pattern** for the $\\chi^2$ approximation to hold. A guideline\nfrom Dobson is that if there are $q$ distinct covariate patterns\n$x_1...,x_q$, with $n_1,...,n_q$ observations per pattern, then the\nexpected frequencies $n_k \\cdot \\pi(x_k)$ should be at least 1 for every\npattern $k\\in 1:q$.\n:::\n\nIf you have covariates measured on a continuous scale, you may not be\nable to use the deviance tests to assess goodness of fit.\n\n### Hosmer-Lemeshow test\n\nIf our covariate patterns produce groups that are too small, a\nreasonable solution is to make bigger groups by merging some of the\ncovariate-pattern groups together.\n\nHosmer and Lemeshow (1980) proposed that we group the patterns by their\npredicted probabilities according to the model of interest. For example,\nyou could group all of the observations with predicted probabilities of\n10% or less together, then group the observations with 11%-20%\nprobability together, and so on; $g=10$ categories in all.\n\nThen we can construct a statistic\n$$X^2 = \\sum_{c=1}^g \\frac{(o_c - e_c)^2}{e_c}$$ where $o_c$ is the\nnumber of events *observed* in group $c$, and $e_c$ is the number of\nevents expected in group $c$ (based on the sum of the fitted values\n$\\hat\\pi_i$ for observations in group $c$).\n\nIf each group has enough observations in it, you can compare $X^2$ to a\n$\\chi^2$ distribution; by simulation, the degrees of freedom has been\nfound to be approximately $g-2$.\n\nFor our CHD model, this procedure would be:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwcgs = \n  wcgs |> \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |> fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |> \n      cut(breaks = seq(0, 1, by = .1), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |> \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nlibrary(pander)\nHL_table |> pander()\n```\n\n::: {.cell-output-display}\n\n---------------------------------------\n pred_prob_cats1     n      o      e   \n----------------- ------- ----- -------\n    (0.1,0.2]       785    116    108  \n\n    (0.2,0.3]       64     12    13.77 \n\n     [0,0.1]       2,305   129   135.2 \n---------------------------------------\n\n\n:::\n\n```{.r .cell-code}\n\nX2 = HL_table |> \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |> \n  pull(`X^2`)\nprint(X2)\n#> [1] 1.1102871\n\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n```\n:::\n\n\n\n\n\n\n\n\nOur statistic is $X^2 = 1.11028711$; $p(\\chi^2(1) > 1.11028711) = 0.29201955$,\nwhich is our p-value for detecting a lack of goodness of fit.\n\nUnfortunately that grouping plan left us with just three categories with\nany observations, so instead of grouping by 10% increments of predicted\nprobability, typically analysts use deciles of the predicted\nprobabilities:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwcgs = \n  wcgs |> \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |> fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |> \n      cut(breaks = quantile(pred_probs_glm1, seq(0, 1, by = .1)), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |> \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nHL_table |> pander()\n```\n\n::: {.cell-output-display}\n\n------------------------------------\n pred_prob_cats1    n    o      e   \n----------------- ----- ---- -------\n  (0.114,0.147]    275   48   36.81 \n\n  (0.147,0.222]    314   51   57.19 \n\n (0.0774,0.0942]   371   27   32.56 \n\n (0.0942,0.114]    282   30   29.89 \n\n (0.0633,0.069]    237   17   15.97 \n\n (0.069,0.0774]    306   20   22.95 \n\n (0.0487,0.0633]   413   27   24.1  \n\n (0.0409,0.0487]   310   14   14.15 \n\n [0.0322,0.0363]   407   16   13.91 \n\n (0.0363,0.0409]   239   7    9.48  \n------------------------------------\n\n\n:::\n\n```{.r .cell-code}\n\nX2 = HL_table |> \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |> \n  pull(`X^2`)\n\nprint(X2)\n#> [1] 6.7811383\n\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n```\n:::\n\n\n\n\n\n\n\n\nNow we have more evenly split categories. The p-value is $0.56041994$,\nstill not significant.\n\nGraphically, we have compared:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nHL_plot = \n  HL_table |> \n  ggplot(aes(x = pred_prob_cats1)) + \n  geom_line(aes(y = e, x = pred_prob_cats1, group = \"Expected\", col = \"Expected\")) +\n  geom_point(aes(y = e, size = n, col = \"Expected\")) +\n  geom_point(aes(y = o, size = n, col = \"Observed\")) +\n  geom_line(aes(y = o, col = \"Observed\", group = \"Observed\")) +\n  scale_size(range = c(1,4)) +\n  theme_bw() +\n  ylab(\"number of CHD events\") +\n  theme(axis.text.x = element_text(angle = 45))\n```\n:::\n\n\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplotly(HL_plot)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-46-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(HL_plot)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-47-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n### Comparing models\n\n-   AIC = $-2 * \\ell(\\hat\\theta) + 2 * p$ \\[lower is better\\]\n-   BIC = $-2 * \\ell(\\hat\\theta) + p * \\text{log}(n)$ \\[lower is\nbetter\\]\n-   likelihood ratio \\[higher is better\\]\n\n## Residual-based diagnostics\n\n### Logistic regression residuals only work for grouped data\n\nResiduals only work if there is more than one observation for most\ncovariate patterns.\n\nHere we will create the grouped-data version of our CHD model from the\nWCGS study:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nwcgs_grouped = \n  wcgs |> \n  summarize(\n    .by = c(dibpat, age),\n    n = n(),\n    chd = sum(chd69 == \"Yes\"),\n    `!chd` = sum(chd69 == \"No\")\n  )\n\nchd_glm_strat_grouped = glm(\n  \"formula\" = cbind(chd, `!chd`) ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs_grouped,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat_grouped |> parameters() |> print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter             | Log-Odds |   SE |         95% CI |     z |      p |\n|:---------------------|:--------:|:----:|:--------------:|:-----:|:------:|\n|dibpat (Type B)       |    -5.80 | 0.98 | (-7.73, -3.90) | -5.95 | < .001 |\n|dibpat (Type A)       |    -5.50 | 0.67 | (-6.83, -4.19) | -8.18 | < .001 |\n|dibpat (Type B) ├Ч age |     0.06 | 0.02 |   (0.02, 0.10) |  3.01 | 0.003  |\n|dibpat (Type A) ├Ч age |     0.07 | 0.01 |   (0.05, 0.10) |  5.24 | < .001 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n### (Response) residuals\n\n$$e_k \\eqdef \\bar y_k - \\hat{\\pi}(x_k)$$\n\n($k$ indexes the covariate patterns)\n\nWe can graph these residuals $e_k$ against the fitted values\n$\\hat\\pi(x_k)$:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nwcgs_grouped = \n  wcgs_grouped |> \n  mutate(\n    fitted = chd_glm_strat_grouped |> fitted(),\n    fitted_logit = fitted |> logit(),\n    response_resids = \n      chd_glm_strat_grouped |> resid(type = \"response\")\n  )\n\nwcgs_response_resid_plot = \n  wcgs_grouped |> \n  ggplot(\n    mapping = aes(\n      x = fitted,\n      y = response_resids\n    )\n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n  geom_smooth( #<1>\n    se = TRUE,  #<1>\n    method.args = list( #<1>\n      span=2/3, #<1>\n      degree=1, #<1>\n      family=\"symmetric\", #<1>\n      iterations=3), #<1>\n    method = stats::loess) #<1>\n\n```\n:::\n\n\n\n\n\n\n\n\n1.  Don't worry about these options for now; I chose them to match\n`autoplot()` as closely as I can. `plot.glm` and `autoplot` use\n`stats::lowess` instead of `stats::loess`; `stats::lowess` is older,\nhard to use with `geom_smooth`, and hard to match exactly with\n`stats::loess`; see https://support.bioconductor.org/p/2323/.\\]\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwcgs_response_resid_plot |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-50-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"html\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwcgs_response_resid_plot |> ggplotly()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-51-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nWe can see a slight fan-shape here: observations on the right have\nlarger variance (as expected since $var(\\bar y) = \\pi(1-\\pi)/n$ is\nmaximized when $\\pi = 0.5$).\n\n### Pearson residuals\n\nThe fan-shape in the response residuals plot isn't necessarily a concern\nhere, since we haven't made an assumption of constant residual variance,\nas we did for linear regression.\n\nHowever, we might want to divide by the standard error in order to make\nthe graph easier to interpret. Here's one way to do that:\n\nThe Pearson (chi-squared) residual for covariate pattern $k$ is: $$\n\\begin{aligned}\nX_k &= \\frac{\\bar y_k - \\hat\\pi_k}{\\sqrt{\\hat \\pi_k (1-\\hat\\pi_k)/n_k}}\n\\end{aligned}\n$$\n\nwhere $$\n\\begin{aligned}\n\\hat\\pi_k \n&\\eqdef \\hat\\pi(x_k)\\\\\n&\\eqdef \\hat P(Y=1|X=x_k)\\\\ \n&\\eqdef \\expit(x_i'\\hat \\beta)\\\\\n&\\eqdef \\expit(\\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j x_{ij})\n\\end{aligned}\n$$\n\nLet's take a look at the Pearson residuals for our CHD model from the\nWCGS data (graphed against the fitted values on the logit scale):\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggfortify)\n```\n:::\n\n\n\n\n\n\n\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(chd_glm_strat_grouped, which = 1, ncol = 1) |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-53-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"html\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(chd_glm_strat_grouped, which = 1, ncol = 1) |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-54-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nThe fan-shape is gone, and these residuals don't show any obvious signs\nof model fit issues.\n\n#### Pearson residuals plot for `beetles` data\n\nIf we create the same plot for the `beetles` model, we see some strong\nevidence of a lack of fit:\n\n::: {.content-visible when-format=\"html\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(beetles_glm_grouped, which = 1, ncol = 1) |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-55-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(beetles_glm_grouped, which = 1, ncol = 1) |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-56-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n#### Pearson residuals with individual (ungrouped) data\n\nWhat happens if we try to compute residuals without grouping the data by\ncovariate pattern?\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggfortify)\n```\n:::\n\n\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(chd_glm_strat, which = 1, ncol = 1) |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-58-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(chd_glm_strat, which = 1, ncol = 1) |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-59-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nMeaningless.\n\n#### Residuals plot by hand (*optional section*)\n\nIf you want to check your understanding of what these residual plots\nare, try building them yourself:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nwcgs_grouped = \n  wcgs_grouped |> \n  mutate(\n    fitted = chd_glm_strat_grouped |> fitted(),\n    fitted_logit = fitted |> logit(),\n    resids = chd_glm_strat_grouped |> resid(type = \"pearson\")\n  )\n\nwcgs_resid_plot1 = \n  wcgs_grouped |> \n  ggplot(\n    mapping = aes(\n      x = fitted_logit,\n      y = resids\n      \n    ) \n    \n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n  geom_smooth(se = FALSE, \n              method.args = list(\n                span=2/3,\n                degree=1,\n                family=\"symmetric\",\n                iterations=3,\n                surface=\"direct\"\n                # span = 2/3, \n                # iterations = 3\n              ),\n              method = stats::loess)\n# plot.glm and autoplot use stats::lowess, which is hard to use with \n# geom_smooth and hard to match exactly; \n# see https://support.bioconductor.org/p/2323/\n\n```\n:::\n\n\n\n\n\n\n\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwcgs_resid_plot1 |> print()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-61-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"html\"}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwcgs_resid_plot1 |> ggplotly()\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-pdf/unnamed-chunk-62-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n### Pearson chi-squared goodness of fit test\n\nThe Pearson chi-squared goodness of fit statistic is: $$\nX^2 = \\sum_{k=1}^m X_k^2 \n$$ Under the null hypothesis that the model in question is correct\n(i.e., sufficiently complex), $X^2\\ \\dot \\sim\\ \\chi^2(N-p)$.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nX = chd_glm_strat_grouped |> \n  resid(type = \"pearson\")\n\nchisq_stat = sum(X^2)\n\npval = pchisq(\n  chisq_stat, \n  lower = FALSE, \n  df = length(X) - length(coef(chd_glm_strat_grouped)))\n\n```\n:::\n\n\n\n\n\n\n\n\nFor our CHD model, the p-value for this test is 0.26523556; no significant\nevidence of a lack of fit at the 0.05 level.\n\n#### Standardized Pearson residuals\n\nEspecially for small data sets, we might want to adjust our residuals\nfor leverage (since outliers in $X$ add extra variance to the\nresiduals):\n\n$$r_{P_k} = \\frac{X_k}{\\sqrt{1-h_k}}$$\n\nwhere $h_k$ is the leverage of $X_k$. The functions `autoplot()` and\n`plot.lm()` use these for some of their graphs.\n\n### Deviance residuals\n\nFor large sample sizes, the Pearson and deviance residuals will be\napproximately the same. For small sample sizes, the deviance residuals\nfrom covariate patterns with small sample sizes can be unreliable (high\nvariance).\n\n$$d_k = \\text{sign}(y_k - n_k \\hat \\pi_k)\\left\\{\\sqrt{2[\\ell_{\\text{full}}(x_k) - \\ell(\\hat\\beta; x_k)]}\\right\\}$$\n\n#### Standardized deviance residuals\n\n$$r_{D_k} = \\frac{d_k}{\\sqrt{1-h_k}}$$\n\n### Diagnostic plots\n\nLet's take a look at the full set of `autoplot()` diagnostics now for\nour `CHD` model:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchd_glm_strat_grouped |> autoplot(which = 1:6) |> print()\n```\n\n::: {.cell-output-display}\n![Diagnostics for CHD model](logistic-regression_files/figure-pdf/fig-chd-model-diagnostics-1.pdf){#fig-chd-model-diagnostics}\n:::\n:::\n\n\n\n\n\n\n\n\n::: notes\n\nThings look pretty good here. The QQ plot is still usable; with large\nsamples; the residuals should be approximately Gaussian.\n\n:::\n\n#### Beetles\n\nLet's look at the beetles model diagnostic plots for comparison:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeetles_glm_grouped |> autoplot(which = 1:6) |> print()\n```\n\n::: {.cell-output-display}\n![Diagnostics for logistic model of `BeetleMortality` data](logistic-regression_files/figure-pdf/fig-beetles-glm-diag-1.pdf){#fig-beetles-glm-diag}\n:::\n:::\n\n\n\n\n\n\n\n\n\nHard to tell much from so little data, but there might be some issues\nhere.\n\n## Other link functions for Bernoulli outcomes\n\nIf you want risk ratios, you can sometimes get them by changing the link\nfunction:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndata(anthers, package = \"dobson\")\nanthers.sum<-aggregate(\n  anthers[c(\"n\",\"y\")], \n  by=anthers[c(\"storage\")],FUN=sum) \n\nanthers_glm_log = glm(\n  formula = cbind(y,n-y)~storage,\n  data=anthers.sum, \n  family=binomial(link=\"log\"))\n\nanthers_glm_log |> parameters() |> print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter   | Log-Risk |   SE |         95% CI |     z |      p |\n|:-----------|:--------:|:----:|:--------------:|:-----:|:------:|\n|(Intercept) |    -0.80 | 0.12 | (-1.04, -0.58) | -6.81 | < .001 |\n|storage     |     0.17 | 0.07 |   (0.02, 0.31) |  2.31 | 0.021  |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\nNow $\\exp{\\beta}$ gives us risk ratios instead of odds ratios:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanthers_glm_log |> parameters(exponentiate = TRUE) |> print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter   | Risk Ratio |   SE |       95% CI |     z |      p |\n|:-----------|:----------:|:----:|:------------:|:-----:|:------:|\n|(Intercept) |       0.45 | 0.05 | (0.35, 0.56) | -6.81 | < .001 |\n|storage     |       1.18 | 0.09 | (1.03, 1.36) |  2.31 | 0.021  |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\nLet's compare this model with a logistic model:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanthers_glm_logit = glm(\n  formula = cbind(y, n - y) ~ storage,\n  data = anthers.sum,\n  family = binomial(link = \"logit\"))\n\nanthers_glm_logit |> parameters(exponentiate = TRUE) |> print_md()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter   | Odds Ratio |   SE |       95% CI |     z |     p |\n|:-----------|:----------:|:----:|:------------:|:-----:|:-----:|\n|(Intercept) |       0.76 | 0.20 | (0.45, 1.27) | -1.05 | 0.296 |\n|storage     |       1.49 | 0.26 | (1.06, 2.10) |  2.29 | 0.022 |\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\\[to add: fitted plots on each outcome scale\\]\n\n---\n\nWhen I try to use `link =\"log\"` in practice, I often get errors about\nnot finding good starting values for the estimation procedure. \nThis is likely because the model is producing fitted probabilities greater than\n1.\n\nWhen this happens, you can try to fit Poisson regression models instead\n(we will see those soon!). \nBut then the outcome distribution isn't quite\nright, and you won't get warnings about fitted probabilities greater\nthan 1. \nIn my opinion, the Poisson model for binary outcomes is\nconfusing and not very appealing.\n\n### WCGS: link functions\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwcgs_glm_logit_link = \n  chd_grouped_data |> \n  mutate(type = dibpat |> relevel(ref = \"Type B\")) |> \n  glm(\n    \"formula\" = cbind(x, `n - x`) ~ dibpat * age, \n    \"data\" = _,\n    \"family\" = binomial(link = \"logit\")\n  )\n\nwcgs_glm_identity_link = \n  chd_grouped_data |> \n  mutate(type = dibpat |> relevel(ref = \"Type B\")) |> \n  glm(\n    \"formula\" = cbind(x, `n - x`) ~ dibpat * age, \n    \"data\" = _,\n    \"family\" = binomial(link = \"identity\")\n  )\nwcgs_glm_identity_link |> coef() |> pander()\n```\n\n::: {.cell-output-display}\n\n----------------------------------------------------------\n (Intercept)   dibpatType A     age      dibpatType A:age \n------------- -------------- ---------- ------------------\n  -0.08257       -0.1374      0.002906       0.004194     \n----------------------------------------------------------\n\n\n:::\n:::\n\n::: {#fig-diagnostics-glm1 .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nlibrary(ggfortify)\nwcgs_glm_logit_link |> autoplot(which = c(1), ncol = 1) + facet_wrap(~dibpat)\nwcgs_glm_identity_link |> autoplot(which = c(1), ncol = 1) + facet_wrap(~dibpat)\n```\n\n::: {.cell-output-display}\n![Logistic link](logistic-regression_files/figure-pdf/fig-diagnostics-glm1-1.pdf){#fig-diagnostics-glm1-1}\n:::\n\n::: {.cell-output-display}\n![Identity link](logistic-regression_files/figure-pdf/fig-diagnostics-glm1-2.pdf){#fig-diagnostics-glm1-2}\n:::\n\nResiduals vs Fitted plot for `wcgs` models\n:::\n\n::: {#fig-diagnostics-beetles .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nbeetles_lm = \n  beetles_long |> \n  lm(formula = died ~ dose)\n\nbeetles = \n  beetles |> mutate(\n    resid_logit = beetles_glm_grouped |> resid(type = \"response\"))\nbeetles_glm_grouped |> autoplot(which = c(1), ncol = 1)\nbeetles_lm |> autoplot(which = c(1), ncol = 1)\n```\n\n::: {.cell-output-display}\n![Logistic link](logistic-regression_files/figure-pdf/fig-diagnostics-beetles-1.pdf){#fig-diagnostics-beetles-1}\n:::\n\n::: {.cell-output-display}\n![Identity link](logistic-regression_files/figure-pdf/fig-diagnostics-beetles-2.pdf){#fig-diagnostics-beetles-2}\n:::\n\nResiduals vs Fitted plot for `BeetleMortality` models\n:::\n\n\n\n\n\n\n\n\n## Quasibinomial\n\nSee [Hua Zhou](https://hua-zhou.github.io/)'s [lecture notes](https://ucla-biostat-200c-2020spring.github.io/slides/04-binomial/binomial.html#:~:text=0.05%20%27.%27%200.1%20%27%20%27%201-,Quasi%2Dbinomial,-Another%20way%20to)\n\n## Further reading\n\n- @hosmer2013applied is a classic textbook on logistic regression\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}