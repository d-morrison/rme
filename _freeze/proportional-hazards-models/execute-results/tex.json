{
  "hash": "f0fc1907d454e63c15db49f6c70a9c87",
  "result": {
    "engine": "knitr",
    "markdown": "# Proportional Hazards Models\n\n---\n\n\n<!-- ::: {.content-hidden when-format=\"revealjs\"} -->\n\n---\n\n### Configuring R {.unnumbered}\n\nFunctions from these packages will be used throughout this document:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%>%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\n```\n:::\n\n\n\n\n\n\n\nHere are some R settings I use in this document:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9\n```\n:::\n\n\n\n\n\n\n\n<!-- ::: -->\n\n\n\n\\providecommand{\\cbl}[1]{\\left\\{#1\\right.}\n\\providecommand{\\cb}[1]{\\left\\{#1\\right\\}}\n\\providecommand{\\paren}[1]{\\left(#1\\right)}\n\\providecommand{\\sb}[1]{\\left[#1\\right]}\n\\def\\pr{\\text{p}}\n\\def\\am{\\arg \\max}\n\\def\\argmax{\\arg \\max}\n\\def\\p{\\text{p}}\n\\def\\P{\\text{P}}\n\\def\\ph{\\hat{\\text{p}}}\n\\def\\hp{\\hat{\\text{p}}}\n\\def\\ga{\\alpha}\n\\def\\b{\\beta}\n\\providecommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor}\n\\providecommand{\\ceiling}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\providecommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\def\\Ber{\\text{Ber}}\n\\def\\Bernoulli{\\text{Bernoulli}}\n\\def\\Pois{\\text{Pois}}\n\\def\\Poisson{\\text{Poisson}}\n\\def\\Gaus{\\text{Gaussian}}\n\\def\\Normal{\\text{N}}\n\\def\\NB{\\text{NegBin}}\n\\def\\NegBin{\\text{NegBin}}\n\\def\\vbeta{\\vec \\beta}\n\\def\\vb{\\vec \\b}\n\\def\\v0{\\vec{0}}\n\\def\\gb{\\beta}\n\\def\\gg{\\gamma}\n\\def\\gd{\\delta}\n\\def\\eps{\\varepsilon}\n\\def\\om{\\omega}\n\\def\\m{\\mu}\n\\def\\s{\\sigma}\n\\def\\l{\\lambda}\n\\def\\gs{\\sigma}\n\\def\\gm{\\mu}\n\\def\\M{\\text{M}}\n\\def\\gM{\\text{M}}\n\\def\\Mu{\\text{M}}\n\\def\\cd{\\cdot}\n\\def\\cds{\\cdots}\n\\def\\lds{\\ldots}\n\\def\\eqdef{\\stackrel{\\text{def}}{=}}\n\\def\\defeq{\\stackrel{\\text{def}}{=}}\n\\def\\hb{\\hat \\beta}\n\\def\\hl{\\hat \\lambda}\n\\def\\hy{\\hat y}\n\\def\\yh{\\hat y}\n\\def\\V{{\\text{Var}}}\n\\def\\hs{\\hat \\sigma}\n\\def\\hsig{\\hat \\sigma}\n\\def\\hS{\\hat \\Sigma}\n\\def\\hSig{\\hat \\Sigma}\n\\def\\hSigma{\\hat \\Sigma}\n\\def\\hSurv{\\hat{S}}\n\\providecommand{\\hSurvf}[1]{\\hat{S}\\paren{#1}}\n\\def\\dist{\\ \\sim \\ }\n\\def\\ddist{\\ \\dot{\\sim} \\ }\n\\def\\dsim{\\ \\dot{\\sim} \\ }\n\\def\\za{z_{1 - \\frac{\\alpha}{2}}}\n\\def\\cirad{\\za \\cdot \\hse{\\hb}}\n\\def\\ci{\\hb {\\color{red}\\pm} \\cirad}\n\\def\\th{\\theta}\n\\def\\Th{\\Theta}\n\\def\\xbar{\\bar{x}}\n\\def\\hth{\\hat\\theta}\n\\def\\hthml{\\hth_{\\text{ML}}}\n\\def\\ba{\\begin{aligned}}\n\\def\\ea{\\end{aligned}}\n\\def\\ind{тлл}\n\\def\\indpt{тлл}\n\\def\\all{\\forall}\n\\def\\iid{\\text{iid}}\n\\def\\ciid{\\text{ciid}}\n\\def\\simind{\\ \\sim_{\\ind}\\ }\n\\def\\siid{\\ \\sim_{\\iid}\\ }\n\\def\\simiid{\\siid}\n\\def\\distiid{\\siid}\n\\def\\tf{\\therefore}\n\\def\\Lik{\\mathcal{L}}\n\\def\\llik{\\ell}\n\\providecommand{\\llikf}[1]{\\llik \\paren{#1}}\n\\def\\score{\\ell'}\n\\providecommand{\\scoref}[1]{\\score \\paren{#1}}\n\\def\\hess{\\ell''}\n\\def\\hessian{\\ell''}\n\\providecommand{\\hessf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\hessianf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\starf}[1]{#1^*}\n\\def\\lik{\\ell}\n\\providecommand{\\est}[1]{\\widehat{#1}}\n\\providecommand{\\esttmp}[1]{{\\widehat{#1}}^*}\n\\def\\esttmpl{\\esttmp{\\lambda}}\n\\def\\cR{\\mathcal{R}}\n\\def\\range{\\mathcal{R}}\n\\def\\Range{\\mathcal{R}}\n\\providecommand{\\rangef}[1]{\\cR(#1)}\n\\def\\~{\\approx}\n\\def\\dapp{\\dot\\approx}\n\\providecommand{\\red}[1]{{\\color{red}#1}}\n\\providecommand{\\deriv}[1]{\\frac{\\partial}{\\partial #1}}\n\\providecommand{\\derivf}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\providecommand{\\blue}[1]{{\\color{blue}#1}}\n\\providecommand{\\green}[1]{{\\color{green}#1}}\n\\providecommand{\\hE}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hExp}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hmu}[1]{\\hat{\\mu}\\sb{#1}}\n\\def\\Expp{\\mathbb{E}}\n\\def\\Ep{\\mathbb{E}}\n\\def\\expit{\\text{expit}}\n\\providecommand{\\expitf}[1]{\\expit\\cb{#1}}\n\\providecommand{\\dexpitf}[1]{\\expit'\\cb{#1}}\n\\def\\logit{\\text{logit}}\n\\providecommand{\\logitf}[1]{\\logit\\cb{#1}}\n\\providecommand{\\E}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Ef}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Exp}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Expf}[1]{\\mathbb{E}\\sb{#1}}\n\\def\\Varr{\\text{Var}}\n\\providecommand{\\var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\varf}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Varf}[1]{\\text{Var}\\paren{#1}}\n\\def\\Covt{\\text{Cov}}\n\\providecommand{\\covh}[1]{\\widehat{\\text{Cov}}\\paren{#1}}\n\\providecommand{\\Cov}[1]{\\Covt \\paren{#1}}\n\\providecommand{\\Covf}[1]{\\Covt \\paren{#1}}\n\\def\\varht{\\widehat{\\text{Var}}}\n\\providecommand{\\varh}[1]{\\varht\\paren{#1}}\n\\providecommand{\\varhf}[1]{\\varht\\paren{#1}}\n\\providecommand{\\vc}[1]{\\boldsymbol{#1}}\n\\providecommand{\\sd}[1]{\\text{sd}\\paren{#1}}\n\\providecommand{\\SD}[1]{\\text{SD}\\paren{#1}}\n\\providecommand{\\hSD}[1]{\\widehat{\\text{SD}}\\paren{#1}}\n\\providecommand{\\se}[1]{\\text{se}\\paren{#1}}\n\\providecommand{\\hse}[1]{\\hat{\\text{se}}\\paren{#1}}\n\\providecommand{\\SE}[1]{\\text{SE}\\paren{#1}}\n\\providecommand{\\HSE}[1]{\\widehat{\\text{SE}}\\paren{#1}}\n\\renewcommand{\\log}[1]{\\text{log}\\cb{#1}}\n\\providecommand{\\logf}[1]{\\text{log}\\cb{#1}}\n\\def\\dlog{\\text{log}'}\n\\providecommand{\\dlogf}[1]{\\dlog \\cb{#1}}\n\\renewcommand{\\exp}[1]{\\text{exp}\\cb{#1}}\n\\providecommand{\\expf}[1]{\\exp{#1}}\n\\def\\dexp{\\text{exp}'}\n\\providecommand{\\dexpf}[1]{\\dexp \\cb{#1}}\n\\providecommand{\\e}[1]{\\text{e}^{#1}}\n\\providecommand{\\ef}[1]{\\text{e}^{#1}}\n\\providecommand{\\inv}[1]{\\paren{#1}^{-1}}\n\\providecommand{\\invf}[1]{\\paren{#1}^{-1}}\n\\def\\oinf{I}\n\\def\\Nat{\\mathbb{N}}\n\\providecommand{\\oinff}[1]{\\oinf\\paren{#1}}\n\\def\\einf{\\mathcal{I}}\n\\providecommand{\\einff}[1]{\\einf\\paren{#1}}\n\\def\\heinf{\\hat{\\einf}}\n\\providecommand{\\heinff}[1]{\\heinf \\paren{#1}}\n\\providecommand{\\1}[1]{\\mathbb{1}_{#1}}\n\\providecommand{\\set}[1]{\\cb{#1}}\n\\providecommand{\\pf}[1]{\\p \\paren{#1}}\n\\providecommand{\\Bias}[1]{\\text{Bias}\\paren{#1}}\n\\providecommand{\\bias}[1]{\\text{Bias}\\paren{#1}}\n\\def\\ss{\\sigma^2}\n\\providecommand{\\ssqf}[1]{\\sigma^2\\paren{#1}}\n\\providecommand{\\mselr}[1]{\\text{MSE}\\paren{#1}}\n\\providecommand{\\maelr}[1]{\\text{MAE}\\paren{#1}}\n\\providecommand{\\abs}[1]{\\left|#1\\right|}\n\\providecommand{\\sqf}[1]{\\paren{#1}^2}\n\\providecommand{\\sq}{^2}\n\\def\\err{\\eps}\n\\providecommand{\\erf}[1]{\\err\\paren{#1}}\n\\renewcommand{\\vec}[1]{\\tilde{#1}}\n\\providecommand{\\v}[1]{\\vec{#1}}\n\\providecommand{\\matr}[1]{\\mathbf{#1}}\n\\def\\mX{\\matr{X}}\n\\def\\mx{\\matr{x}}\n\\def\\vx{\\vec{x}}\n\\def\\vX{\\vec{X}}\n\\def\\vy{\\vec{y}}\n\\def\\vY{\\vec{Y}}\n\\def\\vpi{\\vec{\\pi}}\n\\providecommand{\\mat}[1]{\\mathbf{#1}}\n\\providecommand{\\dsn}[1]{#1_1, \\ldots, #1_n}\n\\def\\X1n{\\dsn{X}}\n\\def\\Xin{\\dsn{X}}\n\\def\\x1n{\\dsn{x}}\n\\def\\'{^{\\top}}\n\\def\\dpr{\\cdot}\n\\def\\Xx1n{X_1=x_1, \\ldots, X_n = x_n}\n\\providecommand{\\dsvn}[2]{#1_1=#2_1, \\ldots, #1_n = #2_n}\n\\providecommand{\\sumn}[1]{\\sum_{#1=1}^n}\n\\def\\sumin{\\sum_{i=1}^n}\n\\def\\sumi1n{\\sum_{i=1}^n}\n\\def\\prodin{\\prod_{i=1}^n}\n\\def\\prodi1n{\\prod_{i=1}^n}\n\\providecommand{\\lp}[2]{#1 \\' \\beta}\n\\def\\odds{\\omega}\n\\def\\OR{\\text{OR}}\n\\def\\logodds{\\eta}\n\\def\\oddst{\\text{odds}}\n\\def\\probst{\\text{probs}}\n\\def\\probt{\\text{probt}}\n\\def\\probit{\\text{probit}}\n\\providecommand{\\oddsf}[1]{\\oddst\\cb{#1}}\n\\providecommand{\\doddsf}[1]{{\\oddst}'\\cb{#1}}\n\\def\\oddsinv{\\text{invodds}}\n\\providecommand{\\oddsinvf}[1]{\\oddsinv\\cb{#1}}\n\\def\\invoddsf{\\oddsinvf}\n\\providecommand{\\doddsinvf}[1]{{\\oddsinv}'\\cb{#1}}\n\\def\\dinvoddsf{\\doddsinvf}\n\\def\\haz{h}\n\\def\\cuhaz{H}\n\\def\\incidence{\\bar{\\haz}}\n\\def\\phaz{\\Expf{\\haz}}\n\n\n\n\n\n\n\n\n\n```{=html}\n<style>\n.quarto-figure-center > figure {\ntext-align: center;\n}\n</style>\n```\n\n\n\n\n\n\n\n\n\n## Introduction\n\n---\n\n::: notes\nRecall: the exponential distribution has constant hazard:\n:::\n\n$$\n\\begin{aligned}\nf(t) &= \\lambda e^{-\\lambda t}\\\\\nS(t) &= e^{-\\lambda t}\\\\\nh(t) &= \\lambda\n\\end{aligned}\n$$\n\n---\n\n::: notes\nLet's make two generalizations. \nFirst, we let the hazard depend on some\ncovariates $x_1,x_2, \\dots, x_p$; \nwe will indicate this dependence by extending our notation for hazard:\n:::\n\n$$\\haz(t|\\vx) \\eqdef \\p(T=t|T\\ge t, \\vX = \\vx)$$\n\n---\n\n:::{#def-base-hazard}\n\n#### baseline hazard\n\n:::: notes\nThe **baseline hazard**, **base hazard**, or **reference hazard**, \ndenoted $h_0(t)$ or $\\lambda_0(t)$, is the [hazard function](intro-to-survival-analysis.qmd#def-hazard) for the \nsubpopulation of individuals whose covariates \nare all equal to their reference levels:\n\n::::\n\n$$\\haz_0(t) \\eqdef \\haz(t | \\vX = \\v0)$$\n\n:::\n\n---\n\nSimilarly: \n\n:::{#def-base-cuhaz}\n\n#### baseline cumulative hazard\n\n:::: notes\nThe **baseline cumulative hazard**, \n**base cumulative hazard**, \nor **reference cumulative hazard**, \ndenoted $H_0(t)$ or $\\Lambda_0(t)$, \nis the [cumulative hazard function](intro-to-survival-analysis.qmd#def-cuhaz)\nfor the subpopulation of individuals whose covariates \nare all equal to their reference levels:\n::::\n\n$$\\cuhaz_0(t) \\eqdef \\cuhaz(t | \\vX = \\v0)$$\n\n:::\n\n---\n\nAlso:\n\n:::{#def-baseline-surv}\n#### Baseline survival function\nThe **baseline survival function** \nis the survival function \nfor an individual whose covariates \nare all equal to their default values.\n \n$$S_0(t) \\eqdef S(t | \\vX = \\v0)$$\n:::\n\n---\n\n::: notes\nAs the second generalization, \nwe let the \nbase hazard, \ncumulative hazard, and \nsurvival functions\ndepend on $t$, \nbut not on the covariates (for now). \nWe can do this using either parametric \nor semi-parametric approaches.\n:::\n\n## Cox's Proportional Hazards Model\n\n---\n\n::: notes\nThe generalization is that the hazard function is:\n:::\n\n$$\n\\begin{aligned}\nh(t|x)&= h_0(t)\\theta(x)\\\\\n\\theta(x) &= \\exp{\\eta(x)}\\\\\n\\eta(x) &= \\vx\\'\\vb \\\\\n&\\eqdef \\beta_1x_1+\\cdots+\\beta_px_p\n\\end{aligned}\n$$\n\n::: notes\nThe relationship between $h(t|x)$ and $\\eta(x)$ is typically modeled using a log link, \nas in a generalized linear model; that is:\n:::\n\n$$\\log{h(t|x)} = \\log{h_0(t)} + \\eta(x)$$\n\n---\n\n::: notes\nThis model is **semi-parametric**, \nbecause the linear predictor depends on estimated\nparameters but the base hazard function is unspecified. \nThere is no constant term in $\\eta(x)$, \nbecause it is absorbed in the base hazard. \n\n:::\n\n---\n\nAlternatively, we could define $\\beta_0(t) = \\log{h_0(t)}$, and then:\n\n$$\\eta(x,t) = \\beta_0(t) + \\beta_1x_1+\\cdots+\\beta_px_p$$\n\n---\n\n::: notes\nFor two different individuals with covariate patterns $\\boldsymbol x_1$ and $\\boldsymbol x_2$, the ratio of the hazard functions (a.k.a. **hazard ratio**, a.k.a. **relative hazard**) is:\n:::\n\n$$\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{h_0(t)\\theta(\\boldsymbol x_1)}{h_0(t)\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n\\end{aligned}\n$$ \n\n::: notes\nUnder the proportional hazards model, \nthis ratio (a.k.a. proportion) does not depend on $t$. \nThis property is a structural limitation of the model; \nit is called the **proportional hazards assumption**.\n:::\n\n---\n\n::: {#def-pha}\n### proportional hazards\n\nA conditional probability distribution $p(T|X)$ \nhas **proportional hazards** if the hazard ratio $h(t|\\boldsymbol x_1)/h(t|\\boldsymbol x_2)$ does not depend on $t$. Mathematically, it can be written as:\n\n$$\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n= \\theta(\\boldsymbol x_1,\\boldsymbol x_2)\n$$\n\n:::\n\n::: notes\nAs we saw above, Cox's proportional hazards model has this property, with $\\theta(\\boldsymbol x_1,\\boldsymbol x_2) = \\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}$.\n:::\n\n---\n\n:::{.callout-note}\n\nWe are using two similar notations, $\\theta(\\boldsymbol x_1,\\boldsymbol x_2)$ and $\\theta(\\boldsymbol x)$. We can link these notations if we define $\\theta(\\boldsymbol x) \\eqdef \\theta(\\boldsymbol x, \\boldsymbol 0)$ and $\\theta(\\boldsymbol 0) = 1$.\n\n:::\n\n---\n\nThe proportional hazards model also has additional notable properties:\n\n$$\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\exp{\\eta(\\boldsymbol x_1)}}{\\exp{\\eta(\\boldsymbol x_2)}}\\\\\n&=\\exp{\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)}\\\\\n&=\\exp{\\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta}\\\\\n&=\\exp{(\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta}\\\\\n\\end{aligned}\n$$ \n\n---\n\nHence on the log scale, we have:\n\n$$\n\\begin{aligned}\n\\log{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}}\n&=\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)\\\\\n&= \\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta\\\\\n&= (\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta\n\\end{aligned}\n$$ \n\n---\n\nIf only one covariate $x_j$ is changing, then:\n\n$$\n\\begin{aligned}\n\\log{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}} \n&=  (x_{1j} - x_{2j}) \\cdot \\beta_j\\\\\n&\\propto (x_{1j} - x_{2j})\n\\end{aligned}\n$$\n\n::: notes\nThat is, under Cox's model $h(t|\\boldsymbol x) = h_0(t)\\exp{\\boldsymbol x'\\beta}$, the log of the hazard ratio is proportional to the difference in $x_j$, with the proportionality coefficient equal to $\\beta_j$.\n:::\n\n---\n\nFurther,\n\n$$\n\\begin{aligned}\n\\log{h(t|\\boldsymbol x)}\n&=\\log{h_0(t)}  + x'\\beta\n\\end{aligned}\n$$\n\n::: notes\nThat is, the covariate effects are additive on the log-hazard scale; hazard functions for different covariate patterns should be vertical shifts of each other.\n\nSee also: \n\n<https://en.wikipedia.org/wiki/Proportional_hazards_model#Why_it_is_called_%22proportional%22>\n\n:::\n\n### Additional properties of the proportional hazards model\n\nIf $h(t|x)= h_0(t)\\theta(x)$, then:\n\n:::{#thm-ph-cuhaz}\n\n#### Cumulative hazards are also proportional to $H_0(t)$\n\n$$\n\\begin{aligned}\nH(t|x)\n&\\eqdef \\int_{u=0}^t h(u)du\\\\\n&= \\int_{u=0}^t h_0(u)\\theta(x)du\\\\\n&= \\theta(x)\\int_{u=0}^t h_0(u)du\\\\\n&= \\theta(x)H_0(t)\n\\end{aligned}\n$$\n\nwhere $H_0(t) \\eqdef H(t|0) = \\int_{u=0}^t h_0(u)du$.\n\n:::\n\n---\n\n:::{#thm-log-cuhaz-parallel}\n\n#### The logarithms of cumulative hazard should be parallel\n\n$$\n\\log{H(t|\\vx)} =\\log{H_0(t)}  + \\vx'\\vb\n$$\n:::\n\n---\n\n:::{#thm-ph-surv}\n#### Survival functions are exponential multiples of $S_0(t)$\n\n$$\n\\begin{aligned}\nS(t|x)\n&= \\exp{-H(t|x)}\\\\\n&= \\exp{-\\theta(x)\\cdot H_0(t)}\\\\\n&= \\left(\\exp{- H_0(t)}\\right)^{\\theta(x)}\\\\\n&= \\sb{S_0(t)}^{\\theta(x)}\\\\\n\\end{aligned}\n$$\n\n\n:::\n\n\n### Testing the proportional hazards assumption\n\n::: notes\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for estimates of the hazard and often the cumulative hazard.\n\nIf the hazards of the three groups are proportional, that means that the ratio of the hazards is constant over $t$. We can test this using the ratios of the estimated cumulative hazards, which also would be\nproportional, as shown above.\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nlibrary(KMsurv)\nlibrary(survival)\nlibrary(dplyr)\ndata(bmt)\n\nbmt = \n  bmt |> \n  as_tibble() |> \n  mutate(\n    group = \n      group |> \n      factor(\n        labels = c(\"ALL\",\"Low Risk AML\",\"High Risk AML\")))\n\nnafit = survfit(\n  formula = Surv(t2,d3) ~ group,\n  type = \"fleming-harrington\",\n  data = bmt)\n\nbmt_curves = tibble(timevec = 1:1000)\nsf1 <- with(nafit[1], stepfun(time,c(1,surv)))\nsf2 <- with(nafit[2], stepfun(time,c(1,surv)))\nsf3 <- with(nafit[3], stepfun(time,c(1,surv)))\n\nbmt_curves = \n  bmt_curves |> \n  mutate(\n    cumhaz1 = -log(sf1(timevec)),\n    cumhaz2 = -log(sf2(timevec)),\n    cumhaz3 = -log(sf3(timevec)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nbmt_rel_hazard_plot = \n  bmt_curves |> \n  ggplot(\n    aes(\n      x = timevec,\n      y = cumhaz1/cumhaz2)\n  ) +\n  geom_line(aes(col = \"ALL/Low Risk AML\")) + \n  ylab(\"Hazard Ratio\") +\n  xlab(\"Time\") + \n  ylim(0,6) +\n  geom_line(aes(y = cumhaz3/cumhaz1, col = \"High Risk AML/ALL\")) +\n  geom_line(aes(y = cumhaz3/cumhaz2, col = \"High Risk AML/Low Risk AML\")) +\n  theme_bw() +\n  labs(colour = \"Comparison\") +\n  theme(legend.position=\"bottom\")\n\nprint(bmt_rel_hazard_plot)\n```\n\n::: {.cell-output-display}\n![Hazard Ratios by Disease Group for `bmt` data](proportional-hazards-models_files/figure-pdf/fig-HR-bmt-1.pdf){#fig-HR-bmt}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\nWe can zoom in on 30-300 days to take a closer look:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt_rel_hazard_plot + xlim(c(30,300))\n```\n\n::: {.cell-output-display}\n![Hazard Ratios by Disease Group (30-300 Days)](proportional-hazards-models_files/figure-pdf/fig-HR-bmt-inset-1.pdf){#fig-HR-bmt-inset}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\nThe cumulative hazard curves should also be proportional\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggfortify)\nplot_cuhaz_bmt =\n  bmt |>\n  survfit(formula = Surv(t2, d3) ~ group) |>\n  autoplot(fun = \"cumhaz\",\n           mark.time = TRUE) + \n  ylab(\"Cumulative hazard\")\n  \nplot_cuhaz_bmt |> print()\n\n```\n\n::: {.cell-output-display}\n![Disease-Free Cumulative Hazard by Disease Group](proportional-hazards-models_files/figure-pdf/fig-cuhaz-bmt-1.pdf){#fig-cuhaz-bmt}\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_cuhaz_bmt + \n  scale_y_log10() +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![Disease-Free Cumulative Hazard by Disease Group (log-scale)](proportional-hazards-models_files/figure-pdf/fig-cuhaz-bmt-loglog-1.pdf){#fig-cuhaz-bmt-loglog}\n:::\n:::\n\n\n\n\n\n\n\n\n### Smoothed hazard functions\n\n::: notes\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for\nestimates of the hazard. Since the hazard is the derivative of the\ncumulative hazard, we need a smooth estimate of the cumulative hazard,\nwhich is provided by smoothing the step-function cumulative hazard.\n\nThe R package `muhaz` handles this for us. What we are looking for is\nwhether the hazard function is more or less the same shape, increasing,\ndecreasing, constant, etc. Are the hazards \"proportional\"?\n:::\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(muhaz)\n\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"High Risk AML\") |> plot(lwd=2,col=3)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"ALL\") |> lines(lwd=2,col=1)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"Low Risk AML\") |> lines(lwd=2,col=2)\nlegend(\"topright\",c(\"ALL\",\"Low Risk AML\",\"High Risk AML\"),col=1:3,lwd=2)\n```\n\n::: {.cell-output-display}\n![Smoothed Hazard Rate Estimates by Disease Group](proportional-hazards-models_files/figure-pdf/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n\n::: notes\nGroup 3 was plotted first because it has the highest hazard.\n\nExcept for an initial blip in the high risk AML group,\nthe hazards look roughly proportional. \nThey are all strongly decreasing.\n:::\n\n### Fitting the Proportional Hazards Model\n\n::: notes\nHow do we fit a proportional hazards regression model? We need to\nestimate the coefficients of the covariates, and we need to estimate the\nbase hazard $h_0(t)$. For the covariates, supposing for simplicity that\nthere are no tied event times, let the event times for the whole data\nset be $t_1, t_2,\\ldots,t_D$. Let the risk set at time $t_i$ be $R(t_i)$\nand \n:::\n\n$$\n\\begin{aligned}\n\\eta(\\boldsymbol{x}) &= \\beta_1x_{1}+\\cdots+\\beta_p x_{p}\\\\\n\\theta(\\boldsymbol{x}) &= e^{\\eta(\\boldsymbol{x})}\\\\\nh(t|X=x)&= h_0(t)e^{\\eta(\\boldsymbol{x})}=\\theta(\\boldsymbol{x}) h_0(t)\n\\end{aligned}\n$$\n\n---\n\nConditional on a single failure at time $t$, the probability that the\nevent is due to subject $f\\in R(t)$ is approximately\n\n$$\n\\begin{aligned}\n\\Pr(f \\text{ fails}|\\text{1 failure at } t) \n&= \\frac{h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}{\\sum_{k \\in R(t)}h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}\\\\\n&=\\frac{\\theta(\\boldsymbol{x}_f)}{\\sum_{k \\in R(t)} \\theta(\\boldsymbol{x}_k)}\n\\end{aligned}\n$$ \n\nThe logic behind this has several steps. We first fix (ex post) the\nfailure times and note that in this discrete context, the probability\n$p_j$ that a subject $j$ in the risk set fails at time $t$ is just the\nhazard of that subject at that time.\n\nIf all of the $p_j$ are small, the chance that exactly one subject fails\nis\n\n$$\n\\sum_{k\\in R(t)}p_k\\left[\\prod_{m\\in R(t), m\\ne k} (1-p_m)\\right]\\approx\\sum_{k\\in R(t)}p_k\n$$\n\nIf subject $i$ is the one who experiences the event of interest at time\n$t_i$, then the **partial likelihood** is\n\n$$\n\\mathcal L^*(\\beta|T)=\n\\prod_i \\frac{\\theta(x_i)}{\\sum_{k \\in R(t_i)} \\theta(\\boldsymbol{x}_k)}\n$$\n\nand we can numerically maximize this with respect to the coefficients\n$\\boldsymbol{\\beta}$ that specify\n$\\eta(\\boldsymbol{x}) = \\boldsymbol{x}'\\boldsymbol{\\beta}$. When there\nare tied event times adjustments need to be made, but the likelihood is\nstill similar. Note that we don't need to know the base hazard to solve\nfor the coefficients.\n\nOnce we have coefficient estimates\n$\\hat{\\boldsymbol{\\beta}} =(\\hat \\beta_1,\\ldots,\\hat\\beta_p)$, this also\ndefines $\\hat\\eta(x)$ and $\\hat\\theta(x)$ and then the estimated base\ncumulative hazard function is $$\\hat H(t)=\n\\sum_{t_i < t} \\frac{d_i}{\\sum_{k\\in R(t_i)} \\theta(x_k)}$$ which\nreduces to the Nelson-Aalen estimate when there are no covariates. There\nare numerous other estimates that have been proposed as well.\n\n## Cox Model for the `bmt` data\n\n### Fit the model\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbmt.cox <- coxph(Surv(t2, d3) ~ group, data = bmt)\nsummary(bmt.cox)\n#> Call:\n#> coxph(formula = Surv(t2, d3) ~ group, data = bmt)\n#> \n#>   n= 137, number of events= 83 \n#> \n#>                      coef exp(coef) se(coef)     z Pr(>|z|)  \n#> groupLow Risk AML  -0.574     0.563    0.287 -2.00    0.046 *\n#> groupHigh Risk AML  0.383     1.467    0.267  1.43    0.152  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                    exp(coef) exp(-coef) lower .95 upper .95\n#> groupLow Risk AML      0.563      1.776     0.321     0.989\n#> groupHigh Risk AML     1.467      0.682     0.869     2.478\n#> \n#> Concordance= 0.625  (se = 0.03 )\n#> Likelihood ratio test= 13.4  on 2 df,   p=0.001\n#> Wald test            = 13  on 2 df,   p=0.001\n#> Score (logrank) test = 13.8  on 2 df,   p=0.001\n```\n:::\n\n\n\n\n\n\n\n\nThe table provides hypothesis tests comparing groups 2 and 3 to group 1.\nGroup 3 has the highest hazard, so the most significant comparison is\nnot directly shown.\n\nThe coefficient 0.3834 is on the log-hazard-ratio scale, as in\nlog-risk-ratio. The next column gives the hazard ratio 1.4673, and a\nhypothesis (Wald) test.\n\nThe (not shown) group 3 vs. group 2 log hazard ratio is 0.3834 + 0.5742\n= 0.9576. The hazard ratio is then exp(0.9576) or 2.605.\n\nInference on all coefficients and combinations can be constructed using\n`coef(bmt.cox)` and `vcov(bmt.cox)` as with logistic and poisson\nregression.\n\n**Concordance** is agreement of first failure between pairs of subjects\nand higher predicted risk between those subjects, omitting\nnon-informative pairs.\n\nThe Rsquare value is Cox and Snell's pseudo R-squared and is not very\nuseful.\n\n### Tests for nested models\n\n`summary()` prints three tests for whether the model with the group\ncovariate is better than the one without\n\n-   `Likelihood ratio test` (chi-squared)\n-   `Wald test` (also chi-squared), obtained by adding the squares of the z-scores\n-   `Score` = log-rank test, as with comparison of survival functions.\n\nThe likelihood ratio test is probably best in smaller samples, followed\nby the Wald test.\n\n### Survival Curves from the Cox Model\n\nWe can take a look at the resulting group-specific curves:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n#| fig-cap: \"Survival Functions for Three Groups by KM and Cox Model\"\n\nkm_fit = survfit(Surv(t2, d3) ~ group, data = as.data.frame(bmt))\n\ncox_fit = survfit(\n  bmt.cox, \n  newdata = \n    data.frame(\n      group = unique(bmt$group), \n      row.names = unique(bmt$group)))\n\nlibrary(survminer)\n\nlist(KM = km_fit, Cox = cox_fit) |> \n  survminer::ggsurvplot(\n    # facet.by = \"group\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    combine = TRUE, \n    fun = 'pct', \n    size = .5,\n    ggtheme = theme_bw(), \n    conf.int = FALSE, \n    censor = FALSE) |> \n  suppressWarnings() # ggsurvplot() throws some warnings that aren't too worrying\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-10-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n\nWhen we use `survfit()` with a Cox model, we have to specify the covariate levels we are interested in; the argument `newdata` should include a `data.frame` with the same named columns as the predictors in the Cox model and one or more levels of each.\n\n---\n\nFrom `?survfit.coxph`:\n\n> If the `newdata` argument is missing, a curve is produced for a \n> single \"pseudo\" subject with covariate values equal to the means component of the fit. \n> The resulting curve(s) almost never make sense, \n> but the default remains due to an unwarranted attachment to the option shown by some users and by other packages.\n> Two particularly egregious examples are factor variables and interactions. \n> Suppose one were studying interspecies transmission of a virus, and the data set has a factor variable with levels (\"pig\", \"chicken\") and about equal numbers of observations for each. \n> The \"mean\" covariate level will be 0.5 -- is this a flying pig?\n\n### Examining `survfit`\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvfit(Surv(t2, d3) ~ group, data = bmt)\n#> Call: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n#> \n#>                      n events median 0.95LCL 0.95UCL\n#> group=ALL           38     24    418     194      NA\n#> group=Low Risk AML  54     25   2204     704      NA\n#> group=High Risk AML 45     34    183     115     456\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvfit(Surv(t2, d3) ~ group, data = bmt) |> summary()\n#> Call: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n#> \n#>                 group=ALL \n#>  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#>     1     38       1    0.974  0.0260        0.924        1.000\n#>    55     37       1    0.947  0.0362        0.879        1.000\n#>    74     36       1    0.921  0.0437        0.839        1.000\n#>    86     35       1    0.895  0.0498        0.802        0.998\n#>   104     34       1    0.868  0.0548        0.767        0.983\n#>   107     33       1    0.842  0.0592        0.734        0.966\n#>   109     32       1    0.816  0.0629        0.701        0.949\n#>   110     31       1    0.789  0.0661        0.670        0.930\n#>   122     30       2    0.737  0.0714        0.609        0.891\n#>   129     28       1    0.711  0.0736        0.580        0.870\n#>   172     27       1    0.684  0.0754        0.551        0.849\n#>   192     26       1    0.658  0.0770        0.523        0.827\n#>   194     25       1    0.632  0.0783        0.495        0.805\n#>   230     23       1    0.604  0.0795        0.467        0.782\n#>   276     22       1    0.577  0.0805        0.439        0.758\n#>   332     21       1    0.549  0.0812        0.411        0.734\n#>   383     20       1    0.522  0.0817        0.384        0.709\n#>   418     19       1    0.494  0.0819        0.357        0.684\n#>   466     18       1    0.467  0.0818        0.331        0.658\n#>   487     17       1    0.439  0.0815        0.305        0.632\n#>   526     16       1    0.412  0.0809        0.280        0.605\n#>   609     14       1    0.382  0.0803        0.254        0.577\n#>   662     13       1    0.353  0.0793        0.227        0.548\n#> \n#>                 group=Low Risk AML \n#>  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#>    10     54       1    0.981  0.0183        0.946        1.000\n#>    35     53       1    0.963  0.0257        0.914        1.000\n#>    48     52       1    0.944  0.0312        0.885        1.000\n#>    53     51       1    0.926  0.0356        0.859        0.998\n#>    79     50       1    0.907  0.0394        0.833        0.988\n#>    80     49       1    0.889  0.0428        0.809        0.977\n#>   105     48       1    0.870  0.0457        0.785        0.965\n#>   211     47       1    0.852  0.0483        0.762        0.952\n#>   219     46       1    0.833  0.0507        0.740        0.939\n#>   248     45       1    0.815  0.0529        0.718        0.925\n#>   272     44       1    0.796  0.0548        0.696        0.911\n#>   288     43       1    0.778  0.0566        0.674        0.897\n#>   381     42       1    0.759  0.0582        0.653        0.882\n#>   390     41       1    0.741  0.0596        0.633        0.867\n#>   414     40       1    0.722  0.0610        0.612        0.852\n#>   421     39       1    0.704  0.0621        0.592        0.837\n#>   481     38       1    0.685  0.0632        0.572        0.821\n#>   486     37       1    0.667  0.0642        0.552        0.805\n#>   606     36       1    0.648  0.0650        0.533        0.789\n#>   641     35       1    0.630  0.0657        0.513        0.773\n#>   704     34       1    0.611  0.0663        0.494        0.756\n#>   748     33       1    0.593  0.0669        0.475        0.739\n#>  1063     26       1    0.570  0.0681        0.451        0.720\n#>  1074     25       1    0.547  0.0691        0.427        0.701\n#>  2204      6       1    0.456  0.1012        0.295        0.704\n#> \n#>                 group=High Risk AML \n#>  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#>     2     45       1    0.978  0.0220        0.936        1.000\n#>    16     44       1    0.956  0.0307        0.897        1.000\n#>    32     43       1    0.933  0.0372        0.863        1.000\n#>    47     42       2    0.889  0.0468        0.802        0.986\n#>    48     40       1    0.867  0.0507        0.773        0.972\n#>    63     39       1    0.844  0.0540        0.745        0.957\n#>    64     38       1    0.822  0.0570        0.718        0.942\n#>    74     37       1    0.800  0.0596        0.691        0.926\n#>    76     36       1    0.778  0.0620        0.665        0.909\n#>    80     35       1    0.756  0.0641        0.640        0.892\n#>    84     34       1    0.733  0.0659        0.615        0.875\n#>    93     33       1    0.711  0.0676        0.590        0.857\n#>   100     32       1    0.689  0.0690        0.566        0.838\n#>   105     31       1    0.667  0.0703        0.542        0.820\n#>   113     30       1    0.644  0.0714        0.519        0.801\n#>   115     29       1    0.622  0.0723        0.496        0.781\n#>   120     28       1    0.600  0.0730        0.473        0.762\n#>   157     27       1    0.578  0.0736        0.450        0.742\n#>   162     26       1    0.556  0.0741        0.428        0.721\n#>   164     25       1    0.533  0.0744        0.406        0.701\n#>   168     24       1    0.511  0.0745        0.384        0.680\n#>   183     23       1    0.489  0.0745        0.363        0.659\n#>   242     22       1    0.467  0.0744        0.341        0.638\n#>   268     21       1    0.444  0.0741        0.321        0.616\n#>   273     20       1    0.422  0.0736        0.300        0.594\n#>   318     19       1    0.400  0.0730        0.280        0.572\n#>   363     18       1    0.378  0.0723        0.260        0.550\n#>   390     17       1    0.356  0.0714        0.240        0.527\n#>   422     16       1    0.333  0.0703        0.221        0.504\n#>   456     15       1    0.311  0.0690        0.201        0.481\n#>   467     14       1    0.289  0.0676        0.183        0.457\n#>   625     13       1    0.267  0.0659        0.164        0.433\n#>   677     12       1    0.244  0.0641        0.146        0.409\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvfit(bmt.cox)\n#> Call: survfit(formula = bmt.cox)\n#> \n#>        n events median 0.95LCL 0.95UCL\n#> [1,] 137     83    422     268      NA\nsurvfit(bmt.cox, newdata = tibble(group = unique(bmt$group)))\n#> Call: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n#> \n#>     n events median 0.95LCL 0.95UCL\n#> 1 137     83    422     268      NA\n#> 2 137     83     NA     625      NA\n#> 3 137     83    268     162     467\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt.cox |> \n  survfit(newdata = tibble(group = unique(bmt$group))) |> \n  summary()\n#> Call: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n#> \n#>  time n.risk n.event survival1 survival2 survival3\n#>     1    137       1     0.993     0.996     0.989\n#>     2    136       1     0.985     0.992     0.978\n#>    10    135       1     0.978     0.987     0.968\n#>    16    134       1     0.970     0.983     0.957\n#>    32    133       1     0.963     0.979     0.946\n#>    35    132       1     0.955     0.975     0.935\n#>    47    131       2     0.941     0.966     0.914\n#>    48    129       2     0.926     0.957     0.893\n#>    53    127       1     0.918     0.953     0.882\n#>    55    126       1     0.911     0.949     0.872\n#>    63    125       1     0.903     0.944     0.861\n#>    64    124       1     0.896     0.940     0.851\n#>    74    123       2     0.881     0.931     0.830\n#>    76    121       1     0.873     0.926     0.819\n#>    79    120       1     0.865     0.922     0.809\n#>    80    119       2     0.850     0.913     0.788\n#>    84    117       1     0.843     0.908     0.778\n#>    86    116       1     0.835     0.903     0.768\n#>    93    115       1     0.827     0.899     0.757\n#>   100    114       1     0.820     0.894     0.747\n#>   104    113       1     0.812     0.889     0.737\n#>   105    112       2     0.797     0.880     0.717\n#>   107    110       1     0.789     0.875     0.707\n#>   109    109       1     0.782     0.870     0.697\n#>   110    108       1     0.774     0.866     0.687\n#>   113    107       1     0.766     0.861     0.677\n#>   115    106       1     0.759     0.856     0.667\n#>   120    105       1     0.751     0.851     0.657\n#>   122    104       2     0.735     0.841     0.637\n#>   129    102       1     0.727     0.836     0.627\n#>   157    101       1     0.720     0.831     0.617\n#>   162    100       1     0.712     0.826     0.607\n#>   164     99       1     0.704     0.821     0.598\n#>   168     98       1     0.696     0.815     0.588\n#>   172     97       1     0.688     0.810     0.578\n#>   183     96       1     0.680     0.805     0.568\n#>   192     95       1     0.672     0.800     0.558\n#>   194     94       1     0.664     0.794     0.549\n#>   211     93       1     0.656     0.789     0.539\n#>   219     92       1     0.648     0.783     0.530\n#>   230     90       1     0.640     0.778     0.520\n#>   242     89       1     0.632     0.773     0.511\n#>   248     88       1     0.624     0.767     0.501\n#>   268     87       1     0.616     0.761     0.492\n#>   272     86       1     0.608     0.756     0.482\n#>   273     85       1     0.600     0.750     0.473\n#>   276     84       1     0.592     0.745     0.464\n#>   288     83       1     0.584     0.739     0.454\n#>   318     82       1     0.576     0.733     0.445\n#>   332     81       1     0.568     0.727     0.436\n#>   363     80       1     0.560     0.722     0.427\n#>   381     79       1     0.552     0.716     0.418\n#>   383     78       1     0.544     0.710     0.409\n#>   390     77       2     0.528     0.698     0.392\n#>   414     75       1     0.520     0.692     0.383\n#>   418     74       1     0.512     0.686     0.374\n#>   421     73       1     0.504     0.680     0.366\n#>   422     72       1     0.496     0.674     0.357\n#>   456     71       1     0.488     0.667     0.349\n#>   466     70       1     0.480     0.661     0.340\n#>   467     69       1     0.472     0.655     0.332\n#>   481     68       1     0.464     0.649     0.324\n#>   486     67       1     0.455     0.642     0.315\n#>   487     66       1     0.447     0.636     0.307\n#>   526     65       1     0.439     0.629     0.299\n#>   606     63       1     0.431     0.623     0.291\n#>   609     62       1     0.423     0.616     0.283\n#>   625     61       1     0.415     0.609     0.275\n#>   641     60       1     0.407     0.603     0.267\n#>   662     59       1     0.399     0.596     0.260\n#>   677     58       1     0.391     0.589     0.252\n#>   704     57       1     0.383     0.582     0.244\n#>   748     56       1     0.374     0.575     0.237\n#>  1063     47       1     0.365     0.567     0.228\n#>  1074     46       1     0.356     0.559     0.220\n#>  2204      9       1     0.313     0.520     0.182\n```\n:::\n\n\n\n\n\n\n\n\n## Adjustment for Ties (optional)\n\n---\n\nAt each time $t_i$ at which more than one of the subjects has an event,\nlet $d_i$ be the number of events at that time, $D_i$ the set of\nsubjects with events at that time, and let $s_i$ be a covariate vector\nfor an artificial subject obtained by adding up the covariate values for\nthe subjects with an event at time $t_i$. Let\n$$\\bar\\eta_i = \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}$$ and\n$\\bar\\theta_i = \\exp{\\bar\\eta_i}$.\n\nLet $s_i$ be a covariate vector for an artificial subject obtained by\nadding up the covariate values for the subjects with an event at time\n$t_i$. Note that \n\n$$\n\\begin{aligned}\n\\bar\\eta_i &=\\sum_{j \\in D_i}\\beta_1x_{j1}+\\cdots+\\beta_px_{jp}\\\\\n&= \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}\\\\\n\\bar\\theta_i &= \\exp{\\bar\\eta_i}\\\\\n&= \\prod_{j \\in D_i}\\theta_i\n\\end{aligned}\n$$\n\n#### Breslow's method for ties\n\nBreslow's method estimates the partial likelihood as\n\n$$\n\\begin{aligned}\nL(\\beta|T) &=\n\\prod_i \\frac{\\bar\\theta_i}{[\\sum_{k \\in R(t_i)} \\theta_k]^{d_i}}\\\\\n&= \\prod_i \\prod_{j \\in D_i}\\frac{\\theta_j}{\\sum_{k \\in R(t_i)} \\theta_k}\n\\end{aligned}\n$$\n\nThis method is equivalent to treating each event as distinct and using the non-ties formula. \nIt works best when the number of ties is small. \nIt is the default in many statistical packages, including PROC PHREG in SAS.\n\n#### Efron's method for ties\n\nThe other common method is Efron's, which is the default in R.\n\n$$L(\\beta|T)=\n\\prod_i \\frac{\\bar\\theta_i}{\\prod_{j=1}^{d_i}[\\sum_{k \\in R(t_i)} \\theta_k-\\frac{j-1}{d_i}\\sum_{k \\in D_i} \\theta_k]}$$\nThis is closer to the exact discrete partial likelihood when there are\nmany ties.\n\nThe third option in R (and an option also in SAS as `discrete`) is the\n\"exact\" method, which is the same one used for matched logistic\nregression.\n\n#### Example: Breslow's method\n\nSuppose as an example we have a time $t$ where there are 20 individuals\nat risk and three failures. Let the three individuals have risk\nparameters $\\theta_1, \\theta_2, \\theta_3$ and let the sum of the risk\nparameters of the remaining 17 individuals be $\\theta_R$. Then the\nfactor in the partial likelihood at time $t$ using Breslow's method is\n\n::: smaller\n$$\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n$$\n:::\n\nIf on the other hand, they had died in the order 1,2, 3, then the\ncontribution to the partial likelihood would be:\n\n::: smaller\n$$\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_3}\\right)\n$$\n:::\n\nas the risk set got smaller with each failure. The exact method roughly\naverages the results for the six possible orderings of the failures.\n\n#### Example: Efron's method\n\nBut we don't know the order they failed in, so instead of reducing the\ndenominator by one risk coefficient each time, we reduce it by the same\nfraction. This is Efron's method.\n\n::: smaller\n$$\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+2(\\theta_1+\\theta_2+\\theta_3)/3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+(\\theta_1+\\theta_2+\\theta_3)/3}\\right)$$\n:::\n\n\n## Building Cox Proportional Hazards models\n\n### `hodg` Lymphoma Data Set from `KMsurv`\n\n#### Participants\n\n43 bone marrow transplant patients at Ohio State University (Avalos\n1993)\n\n#### Variables\n\n-   `dtype`: Disease type (Hodgkin's or non-Hodgkins lymphoma)\n-   `gtype`: Bone marrow graft type:\n   -   allogeneic: from HLA-matched sibling\n   -   autologous: from self (prior to chemo)\n-   `time`: time to study exit\n-   `delta`: study exit reason (death/relapse vs censored)\n-   `wtime`: waiting time to transplant (in months)\n-   `score`: Karnofsky score:\n   -   80--100: Able to carry on normal activity and to work; no special\n    care needed.\n   -   50--70: Unable to work; able to live at home and care for most\n    personal needs; varying amount of assistance needed.\n   -   10--60: Unable to care for self; requires equivalent of\n    institutional or hospital care; disease may be progressing rapidly.\n\n#### Data\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndata(hodg, package = \"KMsurv\")\nhodg2 = hodg |> \n  as_tibble() |> \n  mutate(\n    # We add factor labels to the categorical variables:\n    gtype = gtype |> \n      case_match(\n        1 ~ \"Allogenic\",\n        2 ~ \"Autologous\"),\n    dtype = dtype |> \n      case_match(\n        1 ~ \"Non-Hodgkins\",\n        2 ~ \"Hodgkins\") |> \n      factor() |> \n      relevel(ref = \"Non-Hodgkins\"), \n    delta = delta |> \n      case_match(\n        1 ~ \"dead\",\n        0 ~ \"alive\"),\n    surv = Surv(\n      time = time, \n      event = delta == \"dead\")\n  )\nhodg2 |> print()\n#> # A tibble: 43 x 7\n#>    gtype     dtype         time delta score wtime   surv\n#>    <chr>     <fct>        <int> <chr> <int> <int> <Surv>\n#>  1 Allogenic Non-Hodgkins    28 dead     90    24    28 \n#>  2 Allogenic Non-Hodgkins    32 dead     30     7    32 \n#>  3 Allogenic Non-Hodgkins    49 dead     40     8    49 \n#>  4 Allogenic Non-Hodgkins    84 dead     60    10    84 \n#>  5 Allogenic Non-Hodgkins   357 dead     70    42   357 \n#>  6 Allogenic Non-Hodgkins   933 alive    90     9   933+\n#>  7 Allogenic Non-Hodgkins  1078 alive   100    16  1078+\n#>  8 Allogenic Non-Hodgkins  1183 alive    90    16  1183+\n#>  9 Allogenic Non-Hodgkins  1560 alive    80    20  1560+\n#> 10 Allogenic Non-Hodgkins  2114 alive    80    27  2114+\n#> # i 33 more rows\n```\n:::\n\n\n\n\n\n\n\n### Proportional hazards model\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nhodg.cox1 = coxph(\n  formula = surv ~ gtype * dtype + score + wtime, \n  data = hodg2)\n\nsummary(hodg.cox1)\n#> Call:\n#> coxph(formula = surv ~ gtype * dtype + score + wtime, data = hodg2)\n#> \n#>   n= 43, number of events= 26 \n#> \n#>                                  coef exp(coef) se(coef)     z Pr(>|z|)    \n#> gtypeAutologous                0.6394    1.8953   0.5937  1.08   0.2815    \n#> dtypeHodgkins                  2.7603   15.8050   0.9474  2.91   0.0036 ** \n#> score                         -0.0495    0.9517   0.0124 -3.98  6.8e-05 ***\n#> wtime                         -0.0166    0.9836   0.0102 -1.62   0.1046    \n#> gtypeAutologous:dtypeHodgkins -2.3709    0.0934   1.0355 -2.29   0.0220 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                               exp(coef) exp(-coef) lower .95 upper .95\n#> gtypeAutologous                  1.8953     0.5276    0.5920     6.068\n#> dtypeHodgkins                   15.8050     0.0633    2.4682   101.207\n#> score                            0.9517     1.0507    0.9288     0.975\n#> wtime                            0.9836     1.0167    0.9641     1.003\n#> gtypeAutologous:dtypeHodgkins    0.0934    10.7074    0.0123     0.711\n#> \n#> Concordance= 0.776  (se = 0.059 )\n#> Likelihood ratio test= 32.1  on 5 df,   p=6e-06\n#> Wald test            = 27.2  on 5 df,   p=5e-05\n#> Score (logrank) test = 37.7  on 5 df,   p=4e-07\n```\n:::\n\n\n\n\n\n\n\n::: content-hidden\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### fancier alternatives:\n# library(parameters)\n# hodg.cox1 |>  \n#   parameters() |> \n#   print_md()\n# library(gtsummary)\n# hodg.cox1 |> \n#   tbl_regression() |> \n#   as_hux_table()\n```\n:::\n\n\n\n\n\n\n:::\n\n## Diagnostic graphs for proportional hazards assumption\n\n### Analysis plan\n\n-   **survival function** for the four combinations of disease type and\n    graft type.\n-   **observed (nonparametric) vs. expected (semiparametric) survival**\n    functions.\n-   **complementary log-log survival** for the four groups.\n\n### Kaplan-Meier survival functions\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2)\n\nkm_model |> \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(\n    legend.position=\"bottom\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = legend_text_size)\n  ) +\n  guides(col=guide_legend(ncol=2)) +\n  ylab('Survival probability, S(t)') +\n  xlab(\"Time since transplant (days)\")\n```\n\n::: {.cell-output-display}\n![Kaplan-Meier Survival Curves for HOD/NHL and Allo/Auto Grafts](proportional-hazards-models_files/figure-pdf/unnamed-chunk-18-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n### Observed and expected survival curves\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we need to create a tibble of covariate patterns;\n# we will set score and wtime to mean values for disease and graft types:\nmeans = hodg2 |> \n  summarize(\n    .by = c(dtype, gtype), \n    score = mean(score), \n    wtime = mean(wtime)) |> \n  arrange(dtype, gtype) |> \n  mutate(strata = paste(dtype, gtype, sep = \",\")) |> \n  as.data.frame() \n\n# survfit.coxph() will use the rownames of its `newdata`\n# argument to label its output:\nrownames(means) = means$strata\n\ncox_model = \n  hodg.cox1 |> \n  survfit(\n    data = hodg2, # ggsurvplot() will need this\n    newdata = means)\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# I couldn't find a good function to reformat `cox_model` for ggplot, \n# so I made my own:\nstack_surv_ph = function(cox_model)\n{\n  cox_model$surv |> \n    as_tibble() |> \n    mutate(time = cox_model$time) |> \n    pivot_longer(\n      cols = -time,\n      names_to = \"strata\",\n      values_to = \"surv\") |> \n    mutate(\n      cumhaz = -log(surv),\n      model = \"Cox PH\")\n}\n\nkm_and_cph =\n  km_model |> \n  fortify(surv.connect = TRUE) |> \n  mutate(\n    strata = trimws(strata),\n    model = \"Kaplan-Meier\",\n    cumhaz = -log(surv)) |>\n  bind_rows(stack_surv_ph(cox_model))\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_and_cph |> \n  ggplot(aes(x = time, y = surv, col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  ylab(\"S(t) = P(T>=t)\") +\n  xlab(\"Survival time (t, days)\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Observed and expected survival curves for `bmt` data](proportional-hazards-models_files/figure-pdf/unnamed-chunk-21-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n::: content-hidden\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# these are some old-style R plots inherited from Prof. Rocke's notes; \n# they should be updated to ggplot2-style plots eventually:\n\npar(mfrow = c(2,2), mai = rep(0.5, 4)) # set up a multipanel panel\nplot(km_model[1],xlim=c(0,600),col=1:4,lwd=2, conf.int = FALSE,\n     main = \"NHL Allogeneic\", lty = 1)\nlines(cox_model[1],lwd= 2,lty=2, col = 2, conf.int = FALSE)\nlegend(\"bottomleft\",c(\"K-M\", \"Cox PH\"),col=1:2,lwd=1:2)\nplot(km_model[2],xlim=c(0,600),col=1:4,lwd=2, conf.int = FALSE,\n     main = \"NHL Autologous\", lty = 1)\nlines(cox_model[2],lwd= 2,lty=2, col = 2, conf.int = FALSE)\nplot(km_model[3],xlim=c(0,600),col=1:4,lwd=2, conf.int = FALSE,\n     main = \"HL Allogeneic\", lty = 1)\nlines(cox_model[3],lwd= 2,lty=2, col = 2, conf.int = FALSE)\n\nplot(\n  km_model[4],\n  xlim=c(0,600),\n  col=1:4,\n  lwd=2, \n  conf.int = FALSE,\n  main = \"HL Autologous\", \n  lty = 1)\n\nlines(\n  cox_model[4],\n  lwd= 2,\n  lty=2, \n  col = 2, \n  conf.int = FALSE)\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-22-1.pdf)\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,1)) # reset the paneling\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp1 = \n  km_model |>\n  survminer::ggsurvplot(\n    legend = \"bottom\", \n    legend.title = \"\",\n    combine = TRUE, \n    fun = 'pct', \n    size = .5,\n    ggtheme = theme_bw(), \n    conf.int = FALSE, \n    censor = FALSE) |> \n  suppressWarnings() # ggsurvplot() throws some warnings that aren't too worrying\n\ntemp2 = \n  cox_model |>\n  survminer::ggsurvplot(\n    legend = \"bottom\", \n    legend.title = \"\",\n    combine = TRUE, \n    fun = 'pct', \n    size = .5,\n    ggtheme = theme_bw(), \n    conf.int = FALSE, \n    censor = FALSE) |> \n  suppressWarnings()\n\ntemp = \n  list(KM = km_model,\n       Cox = cox_model) |>\n  survminer::ggsurvplot(\n    # facet.by = gtype,\n    legend = \"bottom\", \n    legend.title = \"\",\n    combine = TRUE, \n    fun = 'pct', \n    size = .5,\n    ggtheme = theme_bw(), \n    conf.int = FALSE, \n    censor = FALSE) |> \n  suppressWarnings() # ggsurvplot() throws some warnings that aren't too worrying\n```\n:::\n\n\n\n\n\n\n:::\n\n### Cumulative hazard (log-scale) curves\n\nAlso known as \"complementary log-log (clog-log) survival curves\".\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nna_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2,\n  type = \"fleming\")\n\nna_model |> \n  survminer::ggsurvplot(\n  legend = \"bottom\", \n  legend.title = \"\",\n  ylab = \"log(Cumulative Hazard)\",\n  xlab = \"Time since transplant (days, log-scale)\",\n  fun = 'cloglog', \n  size = .5,\n  ggtheme = theme_bw(),\n  conf.int = FALSE, \n  censor = TRUE) |>  \n  magrittr::extract2(\"plot\") +\n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n```\n\n::: {.cell-output-display}\n![Complementary log-log survival curves - Nelson-Aalen estimates](proportional-hazards-models_files/figure-pdf/fig-cloglog-na-1.pdf){#fig-cloglog-na}\n:::\n:::\n\n\n\n\n\n\n\nLet's compare these empirical (i.e., non-parametric) curves with the\nfitted curves from our `coxph()` model:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncox_model |> \n  survminer::ggsurvplot(\n    facet_by = \"\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    ylab = \"log(Cumulative Hazard)\",\n    xlab = \"Time since transplant (days, log-scale)\",\n    fun = 'cloglog', \n    size = .5,\n    ggtheme = theme_bw(),\n    censor = FALSE, # doesn't make sense for cox model\n    conf.int = FALSE) |>  \n  magrittr::extract2(\"plot\") +\n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n```\n\n::: {.cell-output-display}\n![Complementary log-log survival curves - PH estimates](proportional-hazards-models_files/figure-pdf/fig-cloglog-ph-1.pdf){#fig-cloglog-ph}\n:::\n:::\n\n\n\n\n\n\n\nNow let's overlay these cumulative hazard curves:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nna_and_cph = \n  na_model |> \n  fortify(fun = \"cumhaz\") |> \n  # `fortify.survfit()` doesn't name cumhaz correctly:\n  rename(cumhaz = surv) |>  \n  mutate(\n    surv = exp(-cumhaz),\n    strata = trimws(strata)) |> \n  mutate(model = \"Nelson-Aalen\") |> \n  bind_rows(stack_surv_ph(cox_model))\n\nna_and_cph |> \n  ggplot(\n    aes(\n      x = time, \n      y = cumhaz, \n      col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard H(t) (log-scale)\") +\n  scale_x_continuous(\n    trans = \"log10\",\n    name = \"Survival time (t, days, log-scale)\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Observed and expected cumulative hazard curves for `bmt` data (cloglog format)](proportional-hazards-models_files/figure-pdf/fig-bmt-cumhaz-1.pdf){#fig-bmt-cumhaz}\n:::\n:::\n\n\n\n\n\n\n\n## Predictions and Residuals\n\n### Review: Predictions in Linear Regression\n\n-   In linear regression, we have a linear predictor for each data point\n    $i$\n\n$$\n\\begin{aligned}\n\\eta_i &= \\beta_0+\\beta_1x_{1i}+\\cdots+\\beta_px_{pi}\\\\\n\\hat y_i &=\\hat\\eta_i = \\hat\\beta_0+\\hat\\beta_1x_{1i}+\\cdots+\\hat\\beta_px_{pi}\\\\\ny_i &\\sim N(\\eta_i,\\sigma^2)\n\\end{aligned}\n$$\n\n-   $\\hat y_i$ estimates the conditional mean of $y_i$ given the\n    covariate values $\\vx_i$. This together with the prediction\n    error says that we are predicting the distribution of values of $y$.\n\n### Review: Residuals in Linear Regression\n\n-   The usual residual is $r_i=y_i-\\hat y_i$, the difference between the\n    actual value of $y$ and a prediction of its mean.\n-   The residuals are also the quantities the sum of whose squares is\n    being minimized by the least squares/MLE estimation.\n\n### Predictions and Residuals in survival models\n\n-   In survival analysis, the equivalent of $y_i$ is the event time\n    $t_i$, which is unknown for the censored observations.\n-   The expected event time can be tricky to calculate:\n\n$$\n\\hat{\\text{E}}[T|X=x] = \\int_{t=0}^{\\infty} \\hat S(t)dt\n$$\n\n### Wide prediction intervals\n\nThe nature of time-to-event data results in very wide prediction\nintervals:\n\n-   Suppose a cancer patient is predicted to have a mean lifetime of 5\n    years after diagnosis and suppose the distribution is exponential.\n-   If we want a 95% interval for survival, the lower end is at the\n    0.025 percentage point of the exponential which is\n    `qexp(.025, rate = 1/5)` = 0.1266 years, or 1/40\n    of the mean lifetime.\n-   The upper end is at the 0.975 point which is\n    `qexp(.975, rate = 1/5)` = 18.4444 years, or 3.7\n    times the mean lifetime.\n-   Saying that the survival time is somewhere between 6 weeks and 18\n    years does not seem very useful, but it may be the best we can do.\n-   For survival analysis, something is like a residual if it is small\n    when the model is accurate or if the accumulation of them is in some\n    way minimized by the estimation algorithm, but there is no exact\n    equivalence to linear regression residuals.\n-   And if there is, they are mostly quite large!\n\n### Types of Residuals in Time-to-Event Models\n\n-   It is often hard to make a decision from graph appearances, though\n    the process can reveal much.\n-   Some diagnostic tests are based on residuals as with other\n    regression methods:\n    -   **Schoenfeld residuals** (via `cox.zph`) for proportionality\n    -   **Cox-Snell residuals** for goodness of fit (@sec-cox-snell)\n    -   **martingale residuals** for non-linearity\n    -   **dfbeta** for influence.\n\n::: content-hidden\n### Martingale and deviance residuals\n\nFor martingale and deviance residuals, the returned object is a vector\nwith one element for each subject (without collapse). Even censored\nobservations have a martingale residual and a deviance residual. Each\nsubject's value for the martingale residual and the deviance residual is\na single value.\n\n### Score residuals\n\nEach observation also has a score residual, though this is a vector, not\na scalar.\n\nFor score residuals the returned object is a matrix with one row per\nsubject and one column per variable. The row order will match the input\ndata for the original fit.\n\nThe score residuals are each individual's contribution to the score\nvector: $\\ell'(\\beta;y_i)$\n\n-   For Schoenfeld residuals, the returned object is a matrix with one\n    row for each event and one column per variable.\n\n-   The rows are ordered by time within strata, and an attribute\n    **strata** is attached that contains the number of observations in\n    each stratum.\n\n-   The scaled Schoenfeld residuals are used in the `cox.zph()`\n    function.\n\n-   Only subjects with an observed event time have a Schoenfeld\n    residual, which like the score residual is a vector.\n:::\n\n### Schoenfeld residuals\n\n-   There is a Schoenfeld residual for each subject $i$ with an event\n    (not censored) and for each predictor $x_{k}$.\n\n-   At the event time $t$ for that subject, there is a risk set $R$, and\n    each subject $j$ in the risk set has a risk coefficient $\\theta_j$\n    and also a value $x_{jk}$ of the predictor.\n\n-   The Schoenfeld residual is the difference between $x_{ik}$ and the\n    risk-weighted average of all the $x_{jk}$ over the risk set.\n\n$$\nr^S_{ik} = \nx_{ik}-\\frac{\\sum_{k\\in R}x_{jk}\\theta_k}{\\sum_{k\\in R}\\theta_k}\n$$\n\nThis residual measures how typical the individual subject is with\nrespect to the covariate at the time of the event. Since subjects should\nfail more or less uniformly according to risk, the Schoenfeld residuals\nshould be approximately level over time, not increasing or decreasing.\n\nWe can test this with the correlation with time on some scale, which\ncould be the time itself, the log time, or the rank in the set of\nfailure times.\n\nThe default is to use the KM curve as a transform, which is similar to\nthe rank but deals better with censoring.\n\nThe `cox.zph()` function implements a score test proposed in  @grambsch1994proportional.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg.zph = cox.zph(hodg.cox1)\nprint(hodg.zph)\n#>               chisq df     p\n#> gtype        0.5400  1 0.462\n#> dtype        1.8012  1 0.180\n#> score        3.8805  1 0.049\n#> wtime        0.0173  1 0.895\n#> gtype:dtype  4.0474  1 0.044\n#> GLOBAL      13.7573  5 0.017\n```\n:::\n\n\n\n\n\n\n\n#### `gtype`\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoxzph(hodg.zph, var = \"gtype\")\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-28-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n#### `dtype`\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoxzph(hodg.zph, var = \"dtype\")\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-29-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n#### `score`\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoxzph(hodg.zph, var = \"score\")\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-30-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n#### `wtime`\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoxzph(hodg.zph, var = \"wtime\")\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-31-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n#### `gtype:dtype`\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoxzph(hodg.zph, var = \"gtype:dtype\")\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-32-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\n#### Conclusions\n\n-   From the correlation test, the Karnofsky score and the interaction\n    with graft type disease type induce modest but statistically\n    significant non-proportionality.\n-   The sample size here is relatively small (26 events in 43 subjects).\n    If the sample size is large, very small amounts of\n    non-proportionality can induce a significant result.\n-   As time goes on, autologous grafts are over-represented at their own\n    event times, but those from HOD patients become less represented.\n-   Both the statistical tests and the plots are useful.\n\n## Goodness of Fit using the Cox-Snell Residuals {#sec-cox-snell}\n\n(references: @klein2003survival, ┬з11.2, and @dobson4e, ┬з10.6)\n\n---\n\n::: notes\nSuppose that an individual has a survival time $T$ which has survival\nfunction $S(t)$, meaning that $\\Pr(T> t) = S(t)$. Then $S(T)$ has a\nuniform distribution on $(0,1)$:\n:::\n\n$$\n\\begin{aligned}\n\\Pr(S(T_i) \\le u)\n&= \\Pr(T_i > S_i^{-1}(u))\\\\\n&= S_i(S_i^{-1}(u))\\\\\n&= u\n\\end{aligned}\n$$\n\n---\n\n::: notes\nAlso, if $U$ has a uniform distribution on $(0,1)$, then what is the\ndistribution of $-\\ln(U)$?\n:::\n\n---\n\n$$\n\\begin{aligned}\n\\Pr(-\\ln(U) < x) &= \\Pr(U>\\exp{-x})\\\\\n&= 1-e^{-x} \n\\end{aligned}\n$$\n\n::: notes\nwhich is the CDF of an exponential distribution with parameter\n$\\lambda=1$.\n:::\n\n---\n\n:::{#def-CS-resid}\n\n#### Cox-Snell generalized residuals\n\n::: notes\nThe **Cox-Snell generalized residuals** are defined as:\n:::\n\n$$\nr^{CS}_i \\eqdef \\hat H(t_i|\\vx_i)\n$$\n\n:::\n\n::: notes\nIf the estimate $\\hat S_i$ is accurate, $r^{CS}_i$\nshould have an exponential distribution with constant hazard $\\lambda=1$, \nwhich means that these values\nshould look like a censored sample from this exponential distribution.\n:::\n\n---\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg2 = hodg2 |> \n  mutate(cs = predict(hodg.cox1, type = \"expected\"))\n\nsurv.csr = survfit(\n  data = hodg2,\n  formula = Surv(time = cs, event = delta == \"dead\") ~ 1,\n  type = \"fleming-harrington\")\n\nautoplot(surv.csr, fun = \"cumhaz\") + \n  geom_abline(aes(intercept = 0, slope = 1), col = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Cumulative Hazard of Cox-Snell Residuals](proportional-hazards-models_files/figure-pdf/unnamed-chunk-33-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe line with slope 1 and intercept 0 fits the curve relatively well, so\nwe don't see lack of fit using this procedure.\n\n## Martingale Residuals\n\nThe **martingale residuals** are a slight modification of the Cox-Snell\nresiduals. If the censoring indicator is $\\delta_i$, then\n$$r^M_i=\\delta_i-r^{CS}_i$$ These residuals can be interpreted as an\nestimate of the excess number of events seen in the data but not\npredicted by the model. We will use these to examine the functional\nforms of continuous covariates.\n\n::: content-hidden\n### Martingale\n\nOriginally, a martingale referred to a betting strategy where you bet\n\\$1 on the first play, then you double the bet if you lose and continue\nuntil you win. This seems like a sure thing, because at the end of each\nseries when you finally win, you are up \\$1. For example,\n$-1-2-4-8+16=1$. But this assumes that you have infinite resources.\nReally, you have a large probability of winning \\$1, and a small\nprobability of losing everything you have, kind of the opposite of a\nlottery.\n\nmartingale\n\n:   In probability, a **martingale** is a sequence of random variables\n    such that the expected value of the next event at any time is the\n    present observed value, and that no better predictor can be derived\n    even with all past values of the series available. At least to a\n    close approximation, the stock market is a martingale. Under the\n    assumptions of the proportional hazards model, the martingale\n    residuals ordered in time form a martingale.\n:::\n\n### Using Martingale Residuals\n\nMartingale residuals can be used to examine the functional form of a\nnumeric variable.\n\n-   We fit the model without that variable and compute the martingale\n    residuals.\n-   We then plot these martingale residuals against the values of the\n    variable.\n-   We can see curvature, or a possible suggestion that the variable can\n    be discretized.\n\nLet's use this to examine the `score` and `wtime` variables in the\n`wtime` data set.\n\n#### Karnofsky score {.unnumbered}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg2 = hodg2 |> \n  mutate(\n    mres = \n      hodg.cox1 |> \n      update(. ~ . - score) |> \n      residuals(type=\"martingale\"))\n\nhodg2 |> \n  ggplot(aes(x = score, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Karnofsky Score\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n```\n\n::: {.cell-output-display}\n![Martingale Residuals vs. Karnofsky Score](proportional-hazards-models_files/figure-pdf/unnamed-chunk-34-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe line is almost straight. It could be some modest transformation of\nthe Karnofsky score would help, but it might not make much difference.\n\n#### Waiting time {.unnumbered}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg2$mres = \n  hodg.cox1 |> \n  update(. ~ . - wtime) |> \n  residuals(type=\"martingale\")\n\nhodg2 |> \n  ggplot(aes(x = wtime, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Waiting Time\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n```\n\n::: {.cell-output-display}\n![Martingale Residuals vs. Waiting Time](proportional-hazards-models_files/figure-pdf/unnamed-chunk-35-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe line could suggest a step function. To see where the drop is, we can\nlook at the largest waiting times and the associated martingale\nresidual.\n\nThe martingale residuals are all negative for `wtime` \\>83 and positive\nfor the next smallest value. A reasonable cut-point is 80 days.\n\n#### Updating the model {.unnumbered}\n\nLet's reformulate the model with dichotomized `wtime`.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg2 = \n  hodg2 |> \n  mutate(\n    wt2 = cut(\n      wtime,c(0, 80, 200),\n      labels=c(\"short\",\"long\")))\n\nhodg.cox2 =\n  coxph(\n    formula = \n      Surv(time, event = delta == \"dead\") ~ \n      gtype*dtype + score + wt2,\n    data = hodg2)\n```\n:::\n\n::: {.cell tbl-cap='Model summary table with waiting time on continuous scale'}\n\n```{.r .cell-code}\nhodg.cox1 |> drop1(test=\"Chisq\")\n#> # A tibble: 4 x 4\n#>      Df   AIC   LRT `Pr(>Chi)`\n#>   <dbl> <dbl> <dbl>      <dbl>\n#> 1    NA  152. NA    NA        \n#> 2     1  168. 17.2   0.0000330\n#> 3     1  154.  3.28  0.0702   \n#> 4     1  156.  5.44  0.0197\n```\n:::\n\n::: {.cell tbl-cap='Model summary table with dichotomized waiting time'}\n\n```{.r .cell-code}\nhodg.cox2 |> drop1(test=\"Chisq\")\n#> # A tibble: 4 x 4\n#>      Df   AIC   LRT  `Pr(>Chi)`\n#>   <dbl> <dbl> <dbl>       <dbl>\n#> 1    NA  149. NA    NA         \n#> 2     1  169. 21.6   0.00000335\n#> 3     1  154.  6.61  0.0102    \n#> 4     1  152.  4.97  0.0258\n```\n:::\n\n\n\n\n\n\n\nThe new model has better (lower) AIC.\n\n## Checking for Outliers and Influential Observations\n\nWe will check for outliers using the deviance residuals. The martingale\nresiduals show excess events or the opposite, but highly skewed, with\nthe maximum possible value being 1, but the smallest value can be very\nlarge negative. Martingale residuals can detect unexpectedly long-lived\npatients, but patients who die unexpectedly early show up only in the\ndeviance residual. Influence will be examined using dfbeta in a similar\nway to linear regression, logistic regression, or Poisson regression.\n\n### Deviance Residuals\n\n$$\n\\begin{aligned}\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(\\delta_i-r_i^M)  \\right]}\\\\\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(r_i^{CS})  \\right]}\n\\end{aligned}\n$$\n\nRoughly centered on 0 with approximate standard deviation 1.\n\n### \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg.mart = residuals(hodg.cox2,type=\"martingale\")\nhodg.dev = residuals(hodg.cox2,type=\"deviance\")\nhodg.dfb = residuals(hodg.cox2,type=\"dfbeta\")\nhodg.preds = predict(hodg.cox2)                   #linear predictor\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hodg.preds,\n     hodg.mart,\n     xlab=\"Linear Predictor\",\n     ylab=\"Martingale Residual\")\n```\n\n::: {.cell-output-display}\n![Martingale Residuals vs. Linear Predictor](proportional-hazards-models_files/figure-pdf/unnamed-chunk-40-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe smallest three martingale residuals in order are observations 1, 29,\nand 18.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hodg.preds,hodg.dev,xlab=\"Linear Predictor\",ylab=\"Deviance Residual\")\n```\n\n::: {.cell-output-display}\n![Deviance Residuals vs. Linear Predictor](proportional-hazards-models_files/figure-pdf/unnamed-chunk-41-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe two largest deviance residuals are observations 1 and 29. Worth\nexamining.\n\n### dfbeta\n\n-   dfbeta is the approximate change in the coefficient vector if that\n    observation were dropped\n\n-   dfbetas is the approximate change in the coefficients, scaled by the\n    standard error for the coefficients.\n\n#### Graft type\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hodg.dfb[,1],xlab=\"Observation Order\",ylab=\"dfbeta for Graft Type\")\n```\n\n::: {.cell-output-display}\n![dfbeta Values by Observation Order for Graft Type](proportional-hazards-models_files/figure-pdf/unnamed-chunk-42-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe smallest dfbeta for graft type is observation 1.\n\n#### Disease type\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hodg.dfb[,2],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Disease Type\")\n```\n\n::: {.cell-output-display}\n![dfbeta Values by Observation Order for Disease Type](proportional-hazards-models_files/figure-pdf/unnamed-chunk-43-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe smallest two dfbeta values for disease type are observations 1 and\n16.\n\n#### Karnofsky score\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hodg.dfb[,3],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Karnofsky Score\")\n```\n\n::: {.cell-output-display}\n![dfbeta Values by Observation Order for Karnofsky Score](proportional-hazards-models_files/figure-pdf/unnamed-chunk-44-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe two highest dfbeta values for score are observations 1 and 18. The\nnext three are observations 17, 29, and 19. The smallest value is\nobservation 2.\n\n#### Waiting time (dichotomized)\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(\n  hodg.dfb[,4],\n  xlab=\"Observation Order\",\n  ylab=\"dfbeta for `Waiting Time < 80`\")\n```\n\n::: {.cell-output-display}\n![dfbeta Values by Observation Order for Waiting Time (dichotomized)](proportional-hazards-models_files/figure-pdf/unnamed-chunk-45-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe two large values of dfbeta for dichotomized waiting time are\nobservations 15 and 16. This may have to do with the discretization of\nwaiting time.\n\n#### Interaction: graft type and disease type\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hodg.dfb[,5],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for dtype:gtype\")\n```\n\n::: {.cell-output-display}\n![dfbeta Values by Observation Order for dtype:gtype](proportional-hazards-models_files/figure-pdf/unnamed-chunk-46-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe two largest values are observations 1 and 16. The smallest value is observation 35.\n\n\n| Diagnostic                 | Observations to Examine |\n|----------------------------|-------------------------|\n| Martingale Residuals       | 1, 29, 18               |\n| Deviance Residuals         | 1, 29                   |\n| Graft Type Influence       | 1                       |\n| Disease Type Influence     | 1, 16                   |\n| Karnofsky Score Influence  | 1, 18 (17, 29, 19)      |\n| Waiting Time Influence     | 15, 16                  |\n| Graft by Disease Influence | 1, 16, 35               |\n\n: Observations to Examine by Residuals and Influence {#tbl-obs-to-examine}\n\nThe most important observations to examine seem to be 1, 15, 16, 18, and\n29.\n\n### \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(hodg,summary(time[delta==1]))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     2.0    41.2    62.5    97.6    83.2   524.0\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(hodg,summary(wtime))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     5.0    16.0    24.0    37.7    55.5   171.0\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(hodg,summary(score))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    20.0    60.0    80.0    76.3    90.0   100.0\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg.cox2\n#> Call:\n#> coxph(formula = Surv(time, event = delta == \"dead\") ~ gtype * \n#>     dtype + score + wt2, data = hodg2)\n#> \n#>                                coef exp(coef) se(coef)  z     p\n#> gtypeAutologous                0.67      1.94     0.59  1 0.263\n#> dtypeHodgkins                  2.33     10.25     0.73  3 0.002\n#> score                         -0.06      0.95     0.01 -4 8e-06\n#> wt2long                       -2.06      0.13     1.05 -2 0.050\n#> gtypeAutologous:dtypeHodgkins -2.07      0.13     0.93 -2 0.026\n#> \n#> Likelihood ratio test=35  on 5 df, p=1e-06\n#> n= 43, number of events= 26\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhodg2[c(1,15,16,18,29),] |> \n  select(gtype, dtype, time, delta, score, wtime) |> \n  mutate(\n    comment = \n      c(\n        \"early death, good score, low risk\",\n        \"high risk grp, long wait, poor score\",\n        \"high risk grp, short wait, poor score\",\n        \"early death, good score, med risk grp\",\n        \"early death, good score, med risk grp\"\n      ))\n#> # A tibble: 5 x 7\n#>   gtype      dtype         time delta score wtime comment                       \n#>   <chr>      <fct>        <int> <chr> <int> <int> <chr>                         \n#> 1 Allogenic  Non-Hodgkins    28 dead     90    24 early death, good score, low ~\n#> 2 Allogenic  Hodgkins        77 dead     60   102 high risk grp, long wait, poo~\n#> 3 Allogenic  Hodgkins        79 dead     70    71 high risk grp, short wait, po~\n#> 4 Autologous Non-Hodgkins    53 dead     90    17 early death, good score, med ~\n#> 5 Autologous Hodgkins        30 dead     90    73 early death, good score, med ~\n```\n:::\n\n\n\n\n\n\n\n### Action Items\n\n-   Unusual points may need checking, particularly if the data are not\n    completely cleaned. In this case, observations 15 and 16 may show\n    some trouble with the dichotomization of waiting time, but it still\n    may be useful.\n-   The two largest residuals seem to be due to unexpectedly early\n    deaths, but unfortunately this can occur.\n-   If hazards don't look proportional, then we may need to use strata,\n    between which the base hazards are permitted to be different. For\n    this problem, the natural strata are the two diseases, because they\n    could need to be managed differently anyway.\n-   A main point that we want to be sure of is the relative risk\n    difference by disease type and graft type.\n\n\n\n\n\n\n\n::: {.cell tbl-cap='Linear Risk Predictors for Lymphoma'}\n\n```{.r .cell-code}\nhodg.cox2 |> \n  predict(\n    reference = \"zero\",\n    newdata = means |> \n      mutate(\n        wt2 = \"short\", \n        score = 0), \n    type = \"lp\") |> \n  data.frame('linear predictor' = _) |> \n  pander()\n```\n\n::: {.cell-output-display}\n\n--------------------------------------------\n         &nbsp;            linear.predictor \n------------------------- ------------------\n Non-Hodgkins,Allogenic           0         \n\n Non-Hodgkins,Autologous        0.6651      \n\n   Hodgkins,Allogenic           2.327       \n\n   Hodgkins,Autologous          0.9256      \n--------------------------------------------\n\n\n:::\n:::\n\n\n\n\n\n\n\nFor Non-Hodgkin's, the allogenic graft is better. For Hodgkin's, the\nautologous graft is much better.\n\n## Stratified survival models\n\n### Revisiting the leukemia dataset (`anderson`)\n\nWe will analyze remission survival times on 42 leukemia patients, half\non new treatment, half on standard treatment.\n\nThis is the same data as the `drug6mp` data from `KMsurv`, but with two\nother variables and without the pairing. This version comes from @kleinbaum2012survival (e.g., p281):\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanderson = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets/\",\n    \"surv2datasets/anderson.dta\") |> \n  haven::read_dta() |> \n  mutate(\n    status = status |> \n      case_match(\n        1 ~ \"relapse\",\n        0 ~ \"censored\"\n      ),\n    \n    sex = sex |> \n      case_match(\n        0 ~ \"female\",\n        1 ~ \"male\"\n      ) |> \n      factor() |> \n      relevel(ref = \"female\"),\n    \n    rx = rx |> \n      case_match(\n        0 ~ \"new\",\n        1 ~ \"standard\"\n      ) |> \n      factor() |> relevel(ref = \"standard\"),\n    \n    surv = Surv(\n      time = survt, \n      event = (status == \"relapse\"))\n  )\n\nprint(anderson)\n```\n:::\n\n\n\n\n\n\n\n### Cox semi-parametric proportional hazards model\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanderson.cox1 = coxph(\n  formula = surv ~ rx + sex + logwbc,\n  data = anderson)\n\nsummary(anderson.cox1)\n#> Call:\n#> coxph(formula = surv ~ rx + sex + logwbc, data = anderson)\n#> \n#>   n= 42, number of events= 30 \n#> \n#>           coef exp(coef) se(coef)     z Pr(>|z|)    \n#> rxnew   -1.504     0.222    0.462 -3.26   0.0011 ** \n#> sexmale  0.315     1.370    0.455  0.69   0.4887    \n#> logwbc   1.682     5.376    0.337  5.00  5.8e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>         exp(coef) exp(-coef) lower .95 upper .95\n#> rxnew       0.222      4.498     0.090     0.549\n#> sexmale     1.370      0.730     0.562     3.338\n#> logwbc      5.376      0.186     2.779    10.398\n#> \n#> Concordance= 0.851  (se = 0.041 )\n#> Likelihood ratio test= 47.2  on 3 df,   p=3e-10\n#> Wald test            = 33.5  on 3 df,   p=2e-07\n#> Score (logrank) test = 48  on 3 df,   p=2e-10\n```\n:::\n\n\n\n\n\n\n\n#### Test the proportional hazards assumption\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncox.zph(anderson.cox1)\n#>        chisq df    p\n#> rx     0.036  1 0.85\n#> sex    5.420  1 0.02\n#> logwbc 0.142  1 0.71\n#> GLOBAL 5.879  3 0.12\n```\n:::\n\n\n\n\n\n\n\n#### Graph the K-M survival curves\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanderson_km_model = survfit(\n  formula = surv ~ sex,\n  data = anderson)\n\nanderson_km_model |> \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\n```\n\n::: {.cell-output-display}\n![](proportional-hazards-models_files/figure-pdf/unnamed-chunk-57-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThe survival curves cross, which indicates a problem in the\nproportionality assumption by sex.\n\n### Graph the Nelson-Aalen cumulative hazard\n\nWe can also look at the log-hazard (\"cloglog survival\") plots:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanderson_na_model = survfit(\n  formula = surv ~ sex,\n  data = anderson,\n  type = \"fleming\")\n\nanderson_na_model |> \n  autoplot(\n    fun = \"cumhaz\",\n    conf.int = FALSE) +\n  theme_classic() +\n  theme(legend.position=\"bottom\") +\n  ylab(\"log(Cumulative Hazard)\") +\n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard (H(t), log scale)\") +\n  scale_x_continuous(\n    breaks = c(1,2,5,10,20,50),\n    trans = \"log\"\n  )\n```\n\n::: {.cell-output-display}\n![Cumulative hazard (cloglog scale) for `anderson` data](proportional-hazards-models_files/figure-pdf/unnamed-chunk-58-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nThis can be fixed by using strata or possibly by other model\nalterations.\n\n### The Stratified Cox Model\n\n-   In a stratified Cox model, each stratum, defined by one or more\n    factors, has its own base survival function $h_0(t)$.\n-   But the coefficients for each variable not used in the strata\n    definitions are assumed to be the same across strata.\n-   To check if this assumption is reasonable one can include\n    interactions with strata and see if they are significant (this may\n    generate a warning and NA lines but these can be ignored).\n-   Since the `sex` variable shows possible non-proportionality, we try\n    stratifying on `sex`.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanderson.coxph.strat = \n  coxph(\n    formula = \n      surv ~ rx + logwbc + strata(sex),\n    data = anderson)\n\nsummary(anderson.coxph.strat)\n#> Call:\n#> coxph(formula = surv ~ rx + logwbc + strata(sex), data = anderson)\n#> \n#>   n= 42, number of events= 30 \n#> \n#>          coef exp(coef) se(coef)     z Pr(>|z|)    \n#> rxnew  -0.998     0.369    0.474 -2.11    0.035 *  \n#> logwbc  1.454     4.279    0.344  4.22  2.4e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>        exp(coef) exp(-coef) lower .95 upper .95\n#> rxnew      0.369      2.713     0.146     0.932\n#> logwbc     4.279      0.234     2.180     8.398\n#> \n#> Concordance= 0.812  (se = 0.059 )\n#> Likelihood ratio test= 32.1  on 2 df,   p=1e-07\n#> Wald test            = 22.8  on 2 df,   p=1e-05\n#> Score (logrank) test = 30.8  on 2 df,   p=2e-07\n```\n:::\n\n\n\n\n\n\n\nLet's compare this to a model fit only on the subset of males:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanderson.coxph.male = \n  coxph(\n    formula = surv ~ rx + logwbc,\n    subset = sex == \"male\",\n    data = anderson)\n\nsummary(anderson.coxph.male)\n#> Call:\n#> coxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n#>     \"male\")\n#> \n#>   n= 20, number of events= 14 \n#> \n#>          coef exp(coef) se(coef)     z Pr(>|z|)   \n#> rxnew  -1.978     0.138    0.739 -2.68   0.0075 **\n#> logwbc  1.743     5.713    0.536  3.25   0.0011 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>        exp(coef) exp(-coef) lower .95 upper .95\n#> rxnew      0.138      7.227    0.0325     0.589\n#> logwbc     5.713      0.175    1.9991    16.328\n#> \n#> Concordance= 0.905  (se = 0.043 )\n#> Likelihood ratio test= 29.2  on 2 df,   p=5e-07\n#> Wald test            = 15.3  on 2 df,   p=5e-04\n#> Score (logrank) test = 26.4  on 2 df,   p=2e-06\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanderson.coxph.female = \n  coxph(\n    formula = \n      surv ~ rx + logwbc,\n    subset = sex == \"female\",\n    data = anderson)\n\nsummary(anderson.coxph.female)\n#> Call:\n#> coxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n#>     \"female\")\n#> \n#>   n= 22, number of events= 16 \n#> \n#>          coef exp(coef) se(coef)     z Pr(>|z|)  \n#> rxnew  -0.311     0.733    0.564 -0.55    0.581  \n#> logwbc  1.206     3.341    0.503  2.40    0.017 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>        exp(coef) exp(-coef) lower .95 upper .95\n#> rxnew      0.733      1.365     0.243      2.21\n#> logwbc     3.341      0.299     1.245      8.96\n#> \n#> Concordance= 0.692  (se = 0.085 )\n#> Likelihood ratio test= 6.65  on 2 df,   p=0.04\n#> Wald test            = 6.36  on 2 df,   p=0.04\n#> Score (logrank) test = 6.74  on 2 df,   p=0.03\n```\n:::\n\n\n\n\n\n\n\nThe coefficients of treatment look different. Are they statistically\ndifferent?\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanderson.coxph.strat.intxn = \n  coxph(\n    formula = surv ~ strata(sex) * (rx + logwbc),\n    data = anderson)\n\nanderson.coxph.strat.intxn |> summary()\n#> Call:\n#> coxph(formula = surv ~ strata(sex) * (rx + logwbc), data = anderson)\n#> \n#>   n= 42, number of events= 30 \n#> \n#>                          coef exp(coef) se(coef)     z Pr(>|z|)  \n#> rxnew                  -0.311     0.733    0.564 -0.55    0.581  \n#> logwbc                  1.206     3.341    0.503  2.40    0.017 *\n#> strata(sex)male:rxnew  -1.667     0.189    0.930 -1.79    0.073 .\n#> strata(sex)male:logwbc  0.537     1.710    0.735  0.73    0.465  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                        exp(coef) exp(-coef) lower .95 upper .95\n#> rxnew                      0.733      1.365    0.2427      2.21\n#> logwbc                     3.341      0.299    1.2452      8.96\n#> strata(sex)male:rxnew      0.189      5.294    0.0305      1.17\n#> strata(sex)male:logwbc     1.710      0.585    0.4048      7.23\n#> \n#> Concordance= 0.797  (se = 0.058 )\n#> Likelihood ratio test= 35.8  on 4 df,   p=3e-07\n#> Wald test            = 21.7  on 4 df,   p=2e-04\n#> Score (logrank) test = 33.1  on 4 df,   p=1e-06\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(\n  anderson.coxph.strat.intxn,\n  anderson.coxph.strat)\n#> # A tibble: 2 x 4\n#>   loglik Chisq    Df `Pr(>|Chi|)`\n#>    <dbl> <dbl> <int>        <dbl>\n#> 1  -53.9 NA       NA       NA    \n#> 2  -55.7  3.77     2        0.152\n```\n:::\n\n\n\n\n\n\n\nWe don't have enough evidence to tell the difference between these two\nmodels.\n\n### Conclusions\n\n-   We chose to use a stratified model because of the apparent\n    non-proportionality of the hazard for the sex variable.\n-   When we fit interactions with the strata variable, we did not get an\n    improved model (via the likelihood ratio test).\n-   So we use the stratifed model with coefficients that are the same\n    across strata.\n\n### Another Modeling Approach\n\n-   We used an additive model without interactions and saw that we might\n    need to stratify by sex.\n-   Instead, we could try to improve the model's functional form - maybe\n    the interaction of treatment and sex is real, and after fitting that\n    we might not need separate hazard functions.\n-   Either approach may work.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nanderson.coxph.intxn = \n  coxph(\n    formula = surv ~ (rx + logwbc) * sex,\n    data = anderson)\n\nanderson.coxph.intxn |> summary()\n#> Call:\n#> coxph(formula = surv ~ (rx + logwbc) * sex, data = anderson)\n#> \n#>   n= 42, number of events= 30 \n#> \n#>                   coef exp(coef) se(coef)     z Pr(>|z|)  \n#> rxnew          -0.3748    0.6874   0.5545 -0.68    0.499  \n#> logwbc          1.0637    2.8971   0.4726  2.25    0.024 *\n#> sexmale        -2.8052    0.0605   2.0323 -1.38    0.167  \n#> rxnew:sexmale  -2.1782    0.1132   0.9109 -2.39    0.017 *\n#> logwbc:sexmale  1.2303    3.4223   0.6301  1.95    0.051 .\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                exp(coef) exp(-coef) lower .95 upper .95\n#> rxnew             0.6874      1.455   0.23185     2.038\n#> logwbc            2.8971      0.345   1.14730     7.315\n#> sexmale           0.0605     16.531   0.00113     3.248\n#> rxnew:sexmale     0.1132      8.830   0.01899     0.675\n#> logwbc:sexmale    3.4223      0.292   0.99539    11.766\n#> \n#> Concordance= 0.861  (se = 0.036 )\n#> Likelihood ratio test= 57  on 5 df,   p=5e-11\n#> Wald test            = 35.6  on 5 df,   p=1e-06\n#> Score (logrank) test = 57.1  on 5 df,   p=5e-11\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncox.zph(anderson.coxph.intxn)\n#>            chisq df    p\n#> rx         0.136  1 0.71\n#> logwbc     1.652  1 0.20\n#> sex        1.266  1 0.26\n#> rx:sex     0.149  1 0.70\n#> logwbc:sex 0.102  1 0.75\n#> GLOBAL     3.747  5 0.59\n```\n:::\n\n\n\n\n\n\n\n## Time-varying covariates\n\n(adapted from @klein2003survival, ┬з9.2)\n\n### Motivating example: back to the leukemia dataset\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# load the data:\ndata(bmt, package = 'KMsurv')\nbmt |> as_tibble() |> print(n = 5)\n#> # A tibble: 137 x 22\n#>   group    t1    t2    d1    d2    d3    ta    da    tc    dc    tp    dp    z1\n#>   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n#> 1     1  2081  2081     0     0     0    67     1   121     1    13     1    26\n#> 2     1  1602  1602     0     0     0  1602     0   139     1    18     1    21\n#> 3     1  1496  1496     0     0     0  1496     0   307     1    12     1    26\n#> 4     1  1462  1462     0     0     0    70     1    95     1    13     1    17\n#> 5     1  1433  1433     0     0     0  1433     0   236     1    12     1    32\n#> # i 132 more rows\n#> # i 9 more variables: z2 <int>, z3 <int>, z4 <int>, z5 <int>, z6 <int>,\n#> #   z7 <int>, z8 <int>, z9 <int>, z10 <int>\n```\n:::\n\n\n\n\n\n\n\nThis dataset comes from the @copelan1991treatment study of allogenic\nbone marrow transplant therapy for acute myeloid leukemia (AML) and\nacute lymphoblastic leukemia (ALL).\n\n##### Outcomes (endpoints)\n\n-   The main endpoint is disease-free survival (`t2` and `d3`) for the\n    three risk groups, \"ALL\", \"AML Low Risk\", and \"AML High Risk\".\n\n##### Possible intermediate events\n\n-   graft vs. host disease (**GVHD**), an immunological rejection\n    response to the transplant (bad)\n-   acute (**AGVHD**)\n-   chronic (**CGVHD**)\n-   platelet recovery, a return of platelet count to normal levels\n    (good)\n\nOne or the other, both in either order, or neither may occur.\n\n##### Covariates\n\n-   We are interested in possibly using the covariates `z1`-`z10` to\n    adjust for other factors.\n\n-   In addition, the time-varying covariates for acute GVHD, chronic\n    GVHD, and platelet recovery may be useful.\n\n#### Preprocessing\n\nWe reformat the data before analysis:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reformat the data:\nbmt1 = \n  bmt |> \n  as_tibble() |> \n  mutate(\n    id = 1:n(), # will be used to connect multiple records for the same individual\n    \n    group =  group |> \n      case_match(\n        1 ~ \"ALL\",\n        2 ~ \"Low Risk AML\",\n        3 ~ \"High Risk AML\") |> \n      factor(levels = c(\"ALL\", \"Low Risk AML\", \"High Risk AML\")),\n    \n    `patient age` = z1,\n    \n    `donor age` = z2,\n    \n    `patient sex` = z3 |> \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `donor sex` = z4 |> \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `Patient CMV Status` = z5 |> \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Donor CMV Status` = z6 |> \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Waiting Time to Transplant` = z7,\n    \n    FAB = z8 |> \n      case_match(\n        1 ~ \"Grade 4 Or 5 (AML only)\",\n        0 ~ \"Other\") |> \n      factor() |> \n      relevel(ref = \"Other\"),\n    \n    hospital = z9 |>  # `z9` is hospital\n      case_match(\n        1 ~ \"Ohio State University\",\n        2 ~ \"Alferd\",\n        3 ~ \"St. Vincent\",\n        4 ~ \"Hahnemann\") |> \n      factor() |> \n      relevel(ref = \"Ohio State University\"),\n    \n    MTX = (z10 == 1) # a prophylatic treatment for GVHD\n    \n  ) |> \n  select(-(z1:z10)) # don't need these anymore\n\nbmt1 |> \n  select(group, id:MTX) |> \n  print(n = 10)\n#> # A tibble: 137 x 12\n#>    group    id `patient age` `donor age` `patient sex` `donor sex`\n#>    <fct> <int>         <int>       <int> <chr>         <chr>      \n#>  1 ALL       1            26          33 Male          Female     \n#>  2 ALL       2            21          37 Male          Male       \n#>  3 ALL       3            26          35 Male          Male       \n#>  4 ALL       4            17          21 Female        Male       \n#>  5 ALL       5            32          36 Male          Male       \n#>  6 ALL       6            22          31 Male          Male       \n#>  7 ALL       7            20          17 Male          Female     \n#>  8 ALL       8            22          24 Male          Female     \n#>  9 ALL       9            18          21 Female        Male       \n#> 10 ALL      10            24          40 Male          Male       \n#> # i 127 more rows\n#> # i 6 more variables: `Patient CMV Status` <chr>, `Donor CMV Status` <chr>,\n#> #   `Waiting Time to Transplant` <int>, FAB <fct>, hospital <fct>, MTX <lgl>\n```\n:::\n\n\n\n\n\n\n\n::: content-hidden\nWe can do this with stepwise regression or hand examination of the\nresults of adding or removing variables.\n:::\n\n### Time-Dependent Covariates\n\n-   A **time-dependent covariate** (\"**TDC**\") is a covariate whose\n    value changes during the course of the study.\n-   For variables like age that change in a linear manner with time, we\n    can just use the value at the start.\n-   But it may be plausible that when and if GVHD occurs, the risk of\n    relapse or death increases, and when and if platelet recovery\n    occurs, the risk decreases.\n\n### Analysis in R\n\n-   We form a variable `precovery` which is = 0 before platelet recovery\n    and is = 1 after platelet recovery, if it occurs.\n-   For each subject where platelet recovery occurs, we set up multiple\n    records (lines in the data frame); for example one from t = 0 to the\n    time of platelet recovery, and one from that time to relapse,\n    recovery, or death.\n-   We do the same for acute GVHD and chronic GVHD.\n-   For each record, the covariates are constant.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbmt2 = bmt1 |> \n  #set up new long-format data set:\n  tmerge(bmt1, id = id, tstop = t2) |> \n  \n  # the following three steps can be in any order, \n  # and will still produce the same result:\n  #add aghvd as tdc:\n  tmerge(bmt1, id = id, agvhd = tdc(ta)) |> \n  #add cghvd as tdc:\n  tmerge(bmt1, id = id, cgvhd = tdc(tc)) |> \n  #add platelet recovery as tdc:\n  tmerge(bmt1, id = id, precovery = tdc(tp))   \n\nbmt2 = bmt2 |> \n  as_tibble() |> \n  mutate(status = as.numeric((tstop == t2) & d3))\n# status only = 1 if at end of t2 and not censored\n\n```\n:::\n\n\n\n\n\n\n\nLet's see how we've rearranged the first row of the data:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbmt1 |> \n  dplyr::filter(id == 1) |> \n  dplyr::select(id, t1, d1, t2, d2, d3, ta, da, tc, dc, tp, dp)\n#> # A tibble: 1 x 12\n#>      id    t1    d1    t2    d2    d3    ta    da    tc    dc    tp    dp\n#>   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n#> 1     1  2081     0  2081     0     0    67     1   121     1    13     1\n```\n:::\n\n\n\n\n\n\n\nThe event times for this individual are:\n\n-   `t` = 0 time of transplant\n-   `tp` = 13 platelet recovery\n-   `ta` = 67 acute GVHD onset\n-   `tc` = 121 chronic GVHD onset\n-   `t2` = 2081 end of study, patient not relapsed or dead\n\nAfter converting the data to long-format, we have:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt2 |> \n  select(\n    id,\n    tstart,\n    tstop,\n    agvhd,\n    cgvhd,\n    precovery,\n    status\n  ) |> \n  dplyr::filter(id == 1)\n#> # A tibble: 4 x 7\n#>      id tstart tstop agvhd cgvhd precovery status\n#>   <int>  <dbl> <int> <int> <int>     <int>  <dbl>\n#> 1     1      0    13     0     0         0      0\n#> 2     1     13    67     0     0         1      0\n#> 3     1     67   121     1     0         1      0\n#> 4     1    121  2081     1     1         1      0\n```\n:::\n\n\n\n\n\n\n\nNote that `status` could have been `1` on the last row, indicating that\nrelapse or death occurred; since it is false, the participant must have\nexited the study without experiencing relapse or death (i.e., they were\ncensored).\n\n### Event sequences\n\nLet:\n\n-   A = acute GVHD\n-   C = chronic GVHD\n-   P = platelet recovery\n\nEach of the eight possible combinations of A or not-A, with C or not-C,\nwith P or not-P occurs in this data set.\n\n-   A always occurs before C, and P always occurs before C, if both\n    occur.\n-   Thus there are ten event sequences in the data set: None, A, C, P,\n    AC, AP, PA, PC, APC, and PAC.\n-   In general, there could be as many as $1+3+(3)(2)+6=16$ sequences,\n    but our domain knowledge tells us that some are missing: CA, CP,\n    CAP, CPA, PCA, PC, PAC\n-   Different subjects could have 1, 2, 3, or 4 intervals, depending on\n    which of acute GVHD, chronic GVHD, and/or platelet recovery\n    occurred.\n-   The final interval for any subject has `status` = 1 if the subject\n    relapsed or died at that time; otherwise `status` = 0.\n-   Any earlier intervals have `status` = 0.\n-   Even though there might be multiple lines per ID in the dataset,\n    there is never more than one event, so no alterations need be made\n    in the estimation procedures or in the interpretation of the output.\n-   The function `tmerge` in the `survival` package eases the process of\n    constructing the new long-format dataset.\n\n### Model with Time-Fixed Covariates\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt1 = \n  bmt1 |> \n  mutate(surv = Surv(t2,d3))\n\nbmt_coxph_TF = coxph(\n  formula = surv ~ group + `patient age`*`donor age` + FAB,\n  data = bmt1)\nsummary(bmt_coxph_TF)\n#> Call:\n#> coxph(formula = surv ~ group + `patient age` * `donor age` + \n#>     FAB, data = bmt1)\n#> \n#>   n= 137, number of events= 83 \n#> \n#>                                 coef exp(coef)  se(coef)     z Pr(>|z|)    \n#> groupLow Risk AML          -1.090648  0.335999  0.354279 -3.08  0.00208 ** \n#> groupHigh Risk AML         -0.403905  0.667707  0.362777 -1.11  0.26555    \n#> `patient age`              -0.081639  0.921605  0.036107 -2.26  0.02376 *  \n#> `donor age`                -0.084587  0.918892  0.030097 -2.81  0.00495 ** \n#> FABGrade 4 Or 5 (AML only)  0.837416  2.310388  0.278464  3.01  0.00264 ** \n#> `patient age`:`donor age`   0.003159  1.003164  0.000951  3.32  0.00089 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                            exp(coef) exp(-coef) lower .95 upper .95\n#> groupLow Risk AML              0.336      2.976     0.168     0.673\n#> groupHigh Risk AML             0.668      1.498     0.328     1.360\n#> `patient age`                  0.922      1.085     0.859     0.989\n#> `donor age`                    0.919      1.088     0.866     0.975\n#> FABGrade 4 Or 5 (AML only)     2.310      0.433     1.339     3.988\n#> `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#> \n#> Concordance= 0.665  (se = 0.033 )\n#> Likelihood ratio test= 32.8  on 6 df,   p=1e-05\n#> Wald test            = 33  on 6 df,   p=1e-05\n#> Score (logrank) test = 35.8  on 6 df,   p=3e-06\ndrop1(bmt_coxph_TF, test = \"Chisq\")\n#> # A tibble: 4 x 4\n#>      Df   AIC   LRT `Pr(>Chi)`\n#>   <dbl> <dbl> <dbl>      <dbl>\n#> 1    NA  726. NA      NA      \n#> 2     2  734. 12.5     0.00192\n#> 3     1  733.  9.22    0.00240\n#> 4     1  733.  9.51    0.00204\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt1$mres = \n  bmt_coxph_TF |> \n  update(. ~ . - `donor age`) |> \n  residuals(type=\"martingale\")\n\nbmt1 |> \n  ggplot(aes(x = `donor age`, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Donor age\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n```\n\n::: {.cell-output-display}\n![Martingale residuals for `Donor age`](proportional-hazards-models_files/figure-pdf/unnamed-chunk-72-1.pdf)\n:::\n:::\n\n\n\n\n\n\n\nA more complex functional form for `donor age` seems warranted; left as\nan exercise for the reader.\n\nNow we will add the time-varying covariates:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# add counting process formulation of Surv():\nbmt2 = \n  bmt2 |> \n  mutate(\n    surv = \n      Surv(\n        time = tstart,\n        time2 = tstop,\n        event = status,\n        type = \"counting\"))\n\n```\n:::\n\n\n\n\n\n\n\nLet's see how the data looks for patient 15:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt1 |> dplyr::filter(id == 15) |> dplyr::select(tp, dp, tc,dc, ta, da, FAB, surv, t1, d1, t2, d2, d3)\n#> # A tibble: 1 x 13\n#>      tp    dp    tc    dc    ta    da FAB     surv    t1    d1    t2    d2    d3\n#>   <int> <int> <int> <int> <int> <int> <fct> <Surv> <int> <int> <int> <int> <int>\n#> 1    21     1   220     1   418     0 Other    418   418     1   418     0     1\nbmt2 |> dplyr::filter(id == 15) |> dplyr::select(id, agvhd, cgvhd, precovery, surv)\n#> # A tibble: 3 x 5\n#>      id agvhd cgvhd precovery       surv\n#>   <int> <int> <int>     <int>     <Surv>\n#> 1    15     0     0         0 (  0, 21+]\n#> 2    15     0     0         1 ( 21,220+]\n#> 3    15     0     1         1 (220,418]\n```\n:::\n\n\n\n\n\n\n\n\n### Model with Time-Dependent Covariates\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt_coxph_TV = coxph(\n  formula = \n    surv ~ \n    group + `patient age`*`donor age` + FAB + agvhd + cgvhd + precovery,\n  data = bmt2)\n\nsummary(bmt_coxph_TV)\n#> Call:\n#> coxph(formula = surv ~ group + `patient age` * `donor age` + \n#>     FAB + agvhd + cgvhd + precovery, data = bmt2)\n#> \n#>   n= 341, number of events= 83 \n#> \n#>                                 coef exp(coef)  se(coef)     z Pr(>|z|)   \n#> groupLow Risk AML          -1.038514  0.353980  0.358220 -2.90   0.0037 **\n#> groupHigh Risk AML         -0.380481  0.683533  0.374867 -1.01   0.3101   \n#> `patient age`              -0.073351  0.929275  0.035956 -2.04   0.0413 * \n#> `donor age`                -0.076406  0.926440  0.030196 -2.53   0.0114 * \n#> FABGrade 4 Or 5 (AML only)  0.805700  2.238263  0.284273  2.83   0.0046 **\n#> agvhd                       0.150565  1.162491  0.306848  0.49   0.6237   \n#> cgvhd                      -0.116136  0.890354  0.289046 -0.40   0.6878   \n#> precovery                  -0.941123  0.390190  0.347861 -2.71   0.0068 **\n#> `patient age`:`donor age`   0.002895  1.002899  0.000944  3.07   0.0022 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                            exp(coef) exp(-coef) lower .95 upper .95\n#> groupLow Risk AML              0.354      2.825     0.175     0.714\n#> groupHigh Risk AML             0.684      1.463     0.328     1.425\n#> `patient age`                  0.929      1.076     0.866     0.997\n#> `donor age`                    0.926      1.079     0.873     0.983\n#> FABGrade 4 Or 5 (AML only)     2.238      0.447     1.282     3.907\n#> agvhd                          1.162      0.860     0.637     2.121\n#> cgvhd                          0.890      1.123     0.505     1.569\n#> precovery                      0.390      2.563     0.197     0.772\n#> `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#> \n#> Concordance= 0.702  (se = 0.028 )\n#> Likelihood ratio test= 40.3  on 9 df,   p=7e-06\n#> Wald test            = 42.4  on 9 df,   p=3e-06\n#> Score (logrank) test = 47.2  on 9 df,   p=4e-07\n```\n:::\n\n\n\n\n\n\n\nPlatelet recovery is highly significant.\n\nNeither acute GVHD (`agvhd`) nor chronic GVHD (`cgvhd`) has a\nstatistically significant effect here, nor are they significant in\nmodels with the other one removed.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nupdate(bmt_coxph_TV, .~.-agvhd) |> summary()\n#> Call:\n#> coxph(formula = surv ~ group + `patient age` + `donor age` + \n#>     FAB + cgvhd + precovery + `patient age`:`donor age`, data = bmt2)\n#> \n#>   n= 341, number of events= 83 \n#> \n#>                                 coef exp(coef)  se(coef)     z Pr(>|z|)   \n#> groupLow Risk AML          -1.049870  0.349983  0.356727 -2.94   0.0032 **\n#> groupHigh Risk AML         -0.417049  0.658988  0.365348 -1.14   0.2537   \n#> `patient age`              -0.070749  0.931696  0.035477 -1.99   0.0461 * \n#> `donor age`                -0.075693  0.927101  0.030075 -2.52   0.0118 * \n#> FABGrade 4 Or 5 (AML only)  0.807035  2.241253  0.283437  2.85   0.0044 **\n#> cgvhd                      -0.095393  0.909015  0.285979 -0.33   0.7387   \n#> precovery                  -0.983653  0.373942  0.338170 -2.91   0.0036 **\n#> `patient age`:`donor age`   0.002859  1.002863  0.000936  3.05   0.0023 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                            exp(coef) exp(-coef) lower .95 upper .95\n#> groupLow Risk AML              0.350      2.857     0.174     0.704\n#> groupHigh Risk AML             0.659      1.517     0.322     1.349\n#> `patient age`                  0.932      1.073     0.869     0.999\n#> `donor age`                    0.927      1.079     0.874     0.983\n#> FABGrade 4 Or 5 (AML only)     2.241      0.446     1.286     3.906\n#> cgvhd                          0.909      1.100     0.519     1.592\n#> precovery                      0.374      2.674     0.193     0.726\n#> `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#> \n#> Concordance= 0.701  (se = 0.027 )\n#> Likelihood ratio test= 40  on 8 df,   p=3e-06\n#> Wald test            = 42.4  on 8 df,   p=1e-06\n#> Score (logrank) test = 47.2  on 8 df,   p=1e-07\nupdate(bmt_coxph_TV, .~.-cgvhd) |> summary()\n#> Call:\n#> coxph(formula = surv ~ group + `patient age` + `donor age` + \n#>     FAB + agvhd + precovery + `patient age`:`donor age`, data = bmt2)\n#> \n#>   n= 341, number of events= 83 \n#> \n#>                                 coef exp(coef)  se(coef)     z Pr(>|z|)   \n#> groupLow Risk AML          -1.019638  0.360725  0.355311 -2.87   0.0041 **\n#> groupHigh Risk AML         -0.381356  0.682935  0.374568 -1.02   0.3086   \n#> `patient age`              -0.073189  0.929426  0.035890 -2.04   0.0414 * \n#> `donor age`                -0.076753  0.926118  0.030121 -2.55   0.0108 * \n#> FABGrade 4 Or 5 (AML only)  0.811716  2.251769  0.284012  2.86   0.0043 **\n#> agvhd                       0.131621  1.140676  0.302623  0.43   0.6636   \n#> precovery                  -0.946697  0.388021  0.347265 -2.73   0.0064 **\n#> `patient age`:`donor age`   0.002904  1.002908  0.000943  3.08   0.0021 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                            exp(coef) exp(-coef) lower .95 upper .95\n#> groupLow Risk AML              0.361      2.772     0.180     0.724\n#> groupHigh Risk AML             0.683      1.464     0.328     1.423\n#> `patient age`                  0.929      1.076     0.866     0.997\n#> `donor age`                    0.926      1.080     0.873     0.982\n#> FABGrade 4 Or 5 (AML only)     2.252      0.444     1.291     3.929\n#> agvhd                          1.141      0.877     0.630     2.064\n#> precovery                      0.388      2.577     0.196     0.766\n#> `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#> \n#> Concordance= 0.701  (se = 0.027 )\n#> Likelihood ratio test= 40.1  on 8 df,   p=3e-06\n#> Wald test            = 42.1  on 8 df,   p=1e-06\n#> Score (logrank) test = 47.1  on 8 df,   p=1e-07\n```\n:::\n\n\n\n\n\n\n\nLet's drop them both:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmt_coxph_TV2 = update(bmt_coxph_TV, . ~ . - agvhd -cgvhd)\nbmt_coxph_TV2 |> summary()\n#> Call:\n#> coxph(formula = surv ~ group + `patient age` + `donor age` + \n#>     FAB + precovery + `patient age`:`donor age`, data = bmt2)\n#> \n#>   n= 341, number of events= 83 \n#> \n#>                                 coef exp(coef)  se(coef)     z Pr(>|z|)   \n#> groupLow Risk AML          -1.032520  0.356108  0.353202 -2.92   0.0035 **\n#> groupHigh Risk AML         -0.413888  0.661075  0.365209 -1.13   0.2571   \n#> `patient age`              -0.070965  0.931495  0.035453 -2.00   0.0453 * \n#> `donor age`                -0.076052  0.926768  0.030007 -2.53   0.0113 * \n#> FABGrade 4 Or 5 (AML only)  0.811926  2.252242  0.283231  2.87   0.0041 **\n#> precovery                  -0.983505  0.373998  0.337997 -2.91   0.0036 **\n#> `patient age`:`donor age`   0.002872  1.002876  0.000936  3.07   0.0021 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                            exp(coef) exp(-coef) lower .95 upper .95\n#> groupLow Risk AML              0.356      2.808     0.178     0.712\n#> groupHigh Risk AML             0.661      1.513     0.323     1.352\n#> `patient age`                  0.931      1.074     0.869     0.999\n#> `donor age`                    0.927      1.079     0.874     0.983\n#> FABGrade 4 Or 5 (AML only)     2.252      0.444     1.293     3.924\n#> precovery                      0.374      2.674     0.193     0.725\n#> `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#> \n#> Concordance= 0.7  (se = 0.027 )\n#> Likelihood ratio test= 39.9  on 7 df,   p=1e-06\n#> Wald test            = 42.2  on 7 df,   p=5e-07\n#> Score (logrank) test = 47.1  on 7 df,   p=5e-08\n```\n:::\n\n\n\n\n\n\n\n## Recurrent Events\n\n(Adapted from @kleinbaum2012survival, Ch 8)\n\n-   Sometimes an appropriate analysis requires consideration of\n    recurrent events.\n-   A patient with arthritis may have more than one flareup. The same is\n    true of many recurring-remitting diseases.\n-   In this case, we have more than one line in the data frame, but each\n    line may have an event.\n-   We have to use a \"robust\" variance estimator to account for\n    correlation of time-to-events within a patient.\n\n### Bladder Cancer Data Set\n\nThe bladder cancer dataset from @kleinbaum2012survival contains recurrent\nevent outcome information for eighty-six cancer patients followed for\nthe recurrence of bladder cancer tumor after transurethral surgical\nexcision (Byar and Green 1980). The exposure of interest is the effect\nof the drug treatment of thiotepa. Control variables are the initial\nnumber and initial size of tumors. The data layout is suitable for a\ncounting processes approach.\n\nThis drug is still a possible choice for some patients. Another\ntherapeutic choice is Bacillus Calmette-Guerin (BCG), a live bacterium\nrelated to cow tuberculosis.\n\n#### Data dictionary\n\n| Variable   | Definition                                         |\n|------------|----------------------------------------------------|\n| `id`       | Patient unique ID                                  |\n| `status`   | for each time interval: 1 = recurred, 0 = censored |\n| `interval` | 1 = first recurrence, etc.                         |\n| `intime`   | \\``tstop - tstart` (all times in months)           |\n| `tstart`   | start of interval                                  |\n| `tstop`    | end of interval                                    |\n| `tx`       | treatment code, 1 = thiotepa                       |\n| `num`      | number of initial tumors                           |\n| `size`     | size of initial tumors (cm)                        |\n\n: Variables in the `bladder` dataset\n\n-   There are 85 patients and 190 lines in the dataset, meaning that\n    many patients have more than one line.\n-   Patient 1 with 0 observation time was removed.\n-   Of the 85 patients, 47 had at least one recurrence and 38 had none.\n-   18 patients had exactly one recurrence.\n-   There were up to 4 recurrences in a patient.\n-   Of the 190 intervals, 112 terminated with a recurrence and 78 were\n    censored.\n\n#### Different intervals for the same patient are correlated.\n\n-   Is the effective sample size 47 or 112? This might narrow confidence\n    intervals by as much as a factor of $\\sqrt{112/47}=1.54$\n\n-   What happens if I have 5 treatment and 5 control values and want to\n    do a t-test and I then duplicate the 10 values as if the sample size\n    was 20? This falsely narrows confidence intervals by a factor of\n    $\\sqrt{2}=1.41$.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbladder = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets\",\n    \"/surv2datasets/bladder.dta\") |> \n  read_dta() |> \n  as_tibble()\n\nbladder = bladder[-1,]  #remove subject with 0 observation time\nprint(bladder)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbladder = \n  bladder |> \n  mutate(\n    surv = \n      Surv(\n        time = start,\n        time2 = stop,\n        event = event,\n        type = \"counting\"))\n\nbladder.cox1 = coxph(\n  formula = surv~tx+num+size,\n  data = bladder)\n\n#results with biased variance-covariance matrix:\nsummary(bladder.cox1)\n#> Call:\n#> coxph(formula = surv ~ tx + num + size, data = bladder)\n#> \n#>   n= 190, number of events= 112 \n#> \n#>         coef exp(coef) se(coef)     z Pr(>|z|)    \n#> tx   -0.4116    0.6626   0.1999 -2.06  0.03947 *  \n#> num   0.1637    1.1778   0.0478  3.43  0.00061 ***\n#> size -0.0411    0.9598   0.0703 -0.58  0.55897    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>      exp(coef) exp(-coef) lower .95 upper .95\n#> tx       0.663      1.509     0.448      0.98\n#> num      1.178      0.849     1.073      1.29\n#> size     0.960      1.042     0.836      1.10\n#> \n#> Concordance= 0.624  (se = 0.032 )\n#> Likelihood ratio test= 14.7  on 3 df,   p=0.002\n#> Wald test            = 15.9  on 3 df,   p=0.001\n#> Score (logrank) test = 16.2  on 3 df,   p=0.001\n```\n:::\n\n\n\n\n\n\n\n::: callout-note\nThe likelihood ratio and score tests assume independence of observations\nwithin a cluster. The Wald and robust score tests do not.\n:::\n\n#### adding `cluster = id`\n\nIf we add `cluster= id` to the call to `coxph`, the coefficient\nestimates don't change, but we get an additional column in the\n`summary()` output: `robust se`:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbladder.cox2 = coxph(\n  formula = surv ~ tx + num + size,\n  cluster = id,\n  data = bladder)\n\n#unbiased though this reduces power:\nsummary(bladder.cox2)\n#> Call:\n#> coxph(formula = surv ~ tx + num + size, data = bladder, cluster = id)\n#> \n#>   n= 190, number of events= 112 \n#> \n#>         coef exp(coef) se(coef) robust se     z Pr(>|z|)   \n#> tx   -0.4116    0.6626   0.1999    0.2488 -1.65   0.0980 . \n#> num   0.1637    1.1778   0.0478    0.0584  2.80   0.0051 **\n#> size -0.0411    0.9598   0.0703    0.0742 -0.55   0.5799   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>      exp(coef) exp(-coef) lower .95 upper .95\n#> tx       0.663      1.509     0.407      1.08\n#> num      1.178      0.849     1.050      1.32\n#> size     0.960      1.042     0.830      1.11\n#> \n#> Concordance= 0.624  (se = 0.031 )\n#> Likelihood ratio test= 14.7  on 3 df,   p=0.002\n#> Wald test            = 11.2  on 3 df,   p=0.01\n#> Score (logrank) test = 16.2  on 3 df,   p=0.001,   Robust = 10.8  p=0.01\n#> \n#>   (Note: the likelihood ratio and score tests assume independence of\n#>      observations within a cluster, the Wald and robust score tests do not).\n```\n:::\n\n\n\n\n\n\n\n`robust se` is larger than `se`, and accounts for the repeated\nobservations from the same individuals:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(bladder.cox2$naive.var, 4)\n#>         [,1]    [,2]   [,3]\n#> [1,]  0.0400 -0.0014 0.0000\n#> [2,] -0.0014  0.0023 0.0007\n#> [3,]  0.0000  0.0007 0.0049\nround(bladder.cox2$var, 4)\n#>         [,1]    [,2]    [,3]\n#> [1,]  0.0619 -0.0026 -0.0004\n#> [2,] -0.0026  0.0034  0.0013\n#> [3,] -0.0004  0.0013  0.0055\n```\n:::\n\n\n\n\n\n\n\nThese are the ratios of correct confidence intervals to naive ones:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(bladder.cox2, diag(var)/diag(naive.var)) |> sqrt()\n#> [1] 1.244 1.223 1.056\n```\n:::\n\n\n\n\n\n\n\nWe might try dropping the non-significant `size` variable:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#remove non-significant size variable:\nbladder.cox3 = bladder.cox2 |> update(. ~ . - size)\nsummary(bladder.cox3)\n#> Call:\n#> coxph(formula = surv ~ tx + num, data = bladder, cluster = id)\n#> \n#>   n= 190, number of events= 112 \n#> \n#>        coef exp(coef) se(coef) robust se     z Pr(>|z|)   \n#> tx  -0.4117    0.6625   0.2003    0.2515 -1.64   0.1017   \n#> num  0.1700    1.1853   0.0465    0.0564  3.02   0.0026 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>     exp(coef) exp(-coef) lower .95 upper .95\n#> tx      0.663      1.509     0.405      1.08\n#> num     1.185      0.844     1.061      1.32\n#> \n#> Concordance= 0.623  (se = 0.031 )\n#> Likelihood ratio test= 14.3  on 2 df,   p=8e-04\n#> Wald test            = 10.2  on 2 df,   p=0.006\n#> Score (logrank) test = 15.8  on 2 df,   p=4e-04,   Robust = 10.6  p=0.005\n#> \n#>   (Note: the likelihood ratio and score tests assume independence of\n#>      observations within a cluster, the Wald and robust score tests do not).\n```\n:::\n\n\n\n\n\n\n\n::: hidden\nWays to check PH assumption:\n\n-   cloglog\n-   schoenfeld residuals\n-   interaction with time\n:::\n\n## Age as the time scale\n\nSee @canchola2003cox.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}