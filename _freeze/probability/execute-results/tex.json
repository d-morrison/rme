{
  "hash": "989f159be7392baf208b0b7dc2107aa8",
  "result": {
    "engine": "knitr",
    "markdown": "# Probability\n\n\n<!-- ::: {.content-hidden when-format=\"revealjs\"} -->\n\n---\n\n### Configuring R {.unnumbered}\n\nFunctions from these packages will be used throughout this document:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%>%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere are some R settings I use in this document:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- ::: -->\n\n\n\n\\providecommand{\\cbl}[1]{\\left\\{#1\\right.}\n\\providecommand{\\cb}[1]{\\left\\{#1\\right\\}}\n\\providecommand{\\paren}[1]{\\left(#1\\right)}\n\\providecommand{\\sb}[1]{\\left[#1\\right]}\n\\def\\pr{\\text{p}}\n\\def\\am{\\arg \\max}\n\\def\\argmax{\\arg \\max}\n\\def\\p{\\text{p}}\n\\def\\P{\\text{P}}\n\\def\\ph{\\hat{\\text{p}}}\n\\def\\hp{\\hat{\\text{p}}}\n\\def\\ga{\\alpha}\n\\def\\b{\\beta}\n\\providecommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor}\n\\providecommand{\\ceiling}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\providecommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\def\\Ber{\\text{Ber}}\n\\def\\Bernoulli{\\text{Bernoulli}}\n\\def\\Pois{\\text{Pois}}\n\\def\\Poisson{\\text{Poisson}}\n\\def\\Gaus{\\text{Gaussian}}\n\\def\\Normal{\\text{N}}\n\\def\\NB{\\text{NegBin}}\n\\def\\NegBin{\\text{NegBin}}\n\\def\\vbeta{\\vec \\beta}\n\\def\\vb{\\vec \\b}\n\\def\\v0{\\vec{0}}\n\\def\\gb{\\beta}\n\\def\\gg{\\gamma}\n\\def\\gd{\\delta}\n\\def\\eps{\\varepsilon}\n\\def\\om{\\omega}\n\\def\\m{\\mu}\n\\def\\s{\\sigma}\n\\def\\l{\\lambda}\n\\def\\gs{\\sigma}\n\\def\\gm{\\mu}\n\\def\\M{\\text{M}}\n\\def\\gM{\\text{M}}\n\\def\\Mu{\\text{M}}\n\\def\\cd{\\cdot}\n\\def\\cds{\\cdots}\n\\def\\lds{\\ldots}\n\\def\\eqdef{\\stackrel{\\text{def}}{=}}\n\\def\\defeq{\\stackrel{\\text{def}}{=}}\n\\def\\hb{\\hat \\beta}\n\\def\\hl{\\hat \\lambda}\n\\def\\hy{\\hat y}\n\\def\\yh{\\hat y}\n\\def\\V{{\\text{Var}}}\n\\def\\hs{\\hat \\sigma}\n\\def\\hsig{\\hat \\sigma}\n\\def\\hS{\\hat \\Sigma}\n\\def\\hSig{\\hat \\Sigma}\n\\def\\hSigma{\\hat \\Sigma}\n\\def\\hSurv{\\hat{S}}\n\\providecommand{\\hSurvf}[1]{\\hat{S}\\paren{#1}}\n\\def\\dist{\\ \\sim \\ }\n\\def\\ddist{\\ \\dot{\\sim} \\ }\n\\def\\dsim{\\ \\dot{\\sim} \\ }\n\\def\\za{z_{1 - \\frac{\\alpha}{2}}}\n\\def\\cirad{\\za \\cdot \\hse{\\hb}}\n\\def\\ci{\\hb {\\color{red}\\pm} \\cirad}\n\\def\\th{\\theta}\n\\def\\Th{\\Theta}\n\\def\\xbar{\\bar{x}}\n\\def\\hth{\\hat\\theta}\n\\def\\hthml{\\hth_{\\text{ML}}}\n\\def\\ba{\\begin{aligned}}\n\\def\\ea{\\end{aligned}}\n\\def\\ind{тлл}\n\\def\\indpt{тлл}\n\\def\\all{\\forall}\n\\def\\iid{\\text{iid}}\n\\def\\ciid{\\text{ciid}}\n\\def\\simind{\\ \\sim_{\\ind}\\ }\n\\def\\siid{\\ \\sim_{\\iid}\\ }\n\\def\\simiid{\\siid}\n\\def\\distiid{\\siid}\n\\def\\tf{\\therefore}\n\\def\\Lik{\\mathcal{L}}\n\\def\\llik{\\ell}\n\\providecommand{\\llikf}[1]{\\llik \\paren{#1}}\n\\def\\score{\\ell'}\n\\providecommand{\\scoref}[1]{\\score \\paren{#1}}\n\\def\\hess{\\ell''}\n\\def\\hessian{\\ell''}\n\\providecommand{\\hessf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\hessianf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\starf}[1]{#1^*}\n\\def\\lik{\\ell}\n\\providecommand{\\est}[1]{\\widehat{#1}}\n\\providecommand{\\esttmp}[1]{{\\widehat{#1}}^*}\n\\def\\esttmpl{\\esttmp{\\lambda}}\n\\def\\cR{\\mathcal{R}}\n\\def\\range{\\mathcal{R}}\n\\def\\Range{\\mathcal{R}}\n\\providecommand{\\rangef}[1]{\\cR(#1)}\n\\def\\~{\\approx}\n\\def\\dapp{\\dot\\approx}\n\\providecommand{\\red}[1]{{\\color{red}#1}}\n\\providecommand{\\deriv}[1]{\\frac{\\partial}{\\partial #1}}\n\\providecommand{\\derivf}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\providecommand{\\blue}[1]{{\\color{blue}#1}}\n\\providecommand{\\green}[1]{{\\color{green}#1}}\n\\providecommand{\\hE}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hExp}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hmu}[1]{\\hat{\\mu}\\sb{#1}}\n\\def\\Expp{\\mathbb{E}}\n\\def\\Ep{\\mathbb{E}}\n\\def\\expit{\\text{expit}}\n\\providecommand{\\expitf}[1]{\\expit\\cb{#1}}\n\\providecommand{\\dexpitf}[1]{\\expit'\\cb{#1}}\n\\def\\logit{\\text{logit}}\n\\providecommand{\\logitf}[1]{\\logit\\cb{#1}}\n\\providecommand{\\E}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Ef}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Exp}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Expf}[1]{\\mathbb{E}\\sb{#1}}\n\\def\\Varr{\\text{Var}}\n\\providecommand{\\var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\varf}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Varf}[1]{\\text{Var}\\paren{#1}}\n\\def\\Covt{\\text{Cov}}\n\\providecommand{\\covh}[1]{\\widehat{\\text{Cov}}\\paren{#1}}\n\\providecommand{\\Cov}[1]{\\Covt \\paren{#1}}\n\\providecommand{\\Covf}[1]{\\Covt \\paren{#1}}\n\\def\\varht{\\widehat{\\text{Var}}}\n\\providecommand{\\varh}[1]{\\varht\\paren{#1}}\n\\providecommand{\\varhf}[1]{\\varht\\paren{#1}}\n\\providecommand{\\vc}[1]{\\boldsymbol{#1}}\n\\providecommand{\\sd}[1]{\\text{sd}\\paren{#1}}\n\\providecommand{\\SD}[1]{\\text{SD}\\paren{#1}}\n\\providecommand{\\hSD}[1]{\\widehat{\\text{SD}}\\paren{#1}}\n\\providecommand{\\se}[1]{\\text{se}\\paren{#1}}\n\\providecommand{\\hse}[1]{\\hat{\\text{se}}\\paren{#1}}\n\\providecommand{\\SE}[1]{\\text{SE}\\paren{#1}}\n\\providecommand{\\HSE}[1]{\\widehat{\\text{SE}}\\paren{#1}}\n\\renewcommand{\\log}[1]{\\text{log}\\cb{#1}}\n\\providecommand{\\logf}[1]{\\text{log}\\cb{#1}}\n\\def\\dlog{\\text{log}'}\n\\providecommand{\\dlogf}[1]{\\dlog \\cb{#1}}\n\\renewcommand{\\exp}[1]{\\text{exp}\\cb{#1}}\n\\providecommand{\\expf}[1]{\\exp{#1}}\n\\def\\dexp{\\text{exp}'}\n\\providecommand{\\dexpf}[1]{\\dexp \\cb{#1}}\n\\providecommand{\\e}[1]{\\text{e}^{#1}}\n\\providecommand{\\ef}[1]{\\text{e}^{#1}}\n\\providecommand{\\inv}[1]{\\paren{#1}^{-1}}\n\\providecommand{\\invf}[1]{\\paren{#1}^{-1}}\n\\def\\oinf{I}\n\\def\\Nat{\\mathbb{N}}\n\\providecommand{\\oinff}[1]{\\oinf\\paren{#1}}\n\\def\\einf{\\mathcal{I}}\n\\providecommand{\\einff}[1]{\\einf\\paren{#1}}\n\\def\\heinf{\\hat{\\einf}}\n\\providecommand{\\heinff}[1]{\\heinf \\paren{#1}}\n\\providecommand{\\1}[1]{\\mathbb{1}_{#1}}\n\\providecommand{\\set}[1]{\\cb{#1}}\n\\providecommand{\\pf}[1]{\\p \\paren{#1}}\n\\providecommand{\\Bias}[1]{\\text{Bias}\\paren{#1}}\n\\providecommand{\\bias}[1]{\\text{Bias}\\paren{#1}}\n\\def\\ss{\\sigma^2}\n\\providecommand{\\ssqf}[1]{\\sigma^2\\paren{#1}}\n\\providecommand{\\mselr}[1]{\\text{MSE}\\paren{#1}}\n\\providecommand{\\maelr}[1]{\\text{MAE}\\paren{#1}}\n\\providecommand{\\abs}[1]{\\left|#1\\right|}\n\\providecommand{\\sqf}[1]{\\paren{#1}^2}\n\\providecommand{\\sq}{^2}\n\\def\\err{\\eps}\n\\providecommand{\\erf}[1]{\\err\\paren{#1}}\n\\renewcommand{\\vec}[1]{\\tilde{#1}}\n\\providecommand{\\v}[1]{\\vec{#1}}\n\\providecommand{\\matr}[1]{\\mathbf{#1}}\n\\def\\mX{\\matr{X}}\n\\def\\mx{\\matr{x}}\n\\def\\vx{\\vec{x}}\n\\def\\vX{\\vec{X}}\n\\def\\vy{\\vec{y}}\n\\def\\vY{\\vec{Y}}\n\\def\\vpi{\\vec{\\pi}}\n\\providecommand{\\mat}[1]{\\mathbf{#1}}\n\\providecommand{\\dsn}[1]{#1_1, \\ldots, #1_n}\n\\def\\X1n{\\dsn{X}}\n\\def\\Xin{\\dsn{X}}\n\\def\\x1n{\\dsn{x}}\n\\def\\'{^{\\top}}\n\\def\\dpr{\\cdot}\n\\def\\Xx1n{X_1=x_1, \\ldots, X_n = x_n}\n\\providecommand{\\dsvn}[2]{#1_1=#2_1, \\ldots, #1_n = #2_n}\n\\providecommand{\\sumn}[1]{\\sum_{#1=1}^n}\n\\def\\sumin{\\sum_{i=1}^n}\n\\def\\sumi1n{\\sum_{i=1}^n}\n\\def\\prodin{\\prod_{i=1}^n}\n\\def\\prodi1n{\\prod_{i=1}^n}\n\\providecommand{\\lp}[2]{#1 \\' \\beta}\n\\def\\odds{\\omega}\n\\def\\OR{\\text{OR}}\n\\def\\logodds{\\eta}\n\\def\\oddst{\\text{odds}}\n\\def\\probst{\\text{probs}}\n\\def\\probt{\\text{probt}}\n\\def\\probit{\\text{probit}}\n\\providecommand{\\oddsf}[1]{\\oddst\\cb{#1}}\n\\providecommand{\\doddsf}[1]{{\\oddst}'\\cb{#1}}\n\\def\\oddsinv{\\text{invodds}}\n\\providecommand{\\oddsinvf}[1]{\\oddsinv\\cb{#1}}\n\\def\\invoddsf{\\oddsinvf}\n\\providecommand{\\doddsinvf}[1]{{\\oddsinv}'\\cb{#1}}\n\\def\\dinvoddsf{\\doddsinvf}\n\\def\\haz{h}\n\\def\\cuhaz{H}\n\\def\\incidence{\\bar{\\haz}}\n\\def\\phaz{\\Expf{\\haz}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```{=html}\n<style>\n.quarto-figure-center > figure {\ntext-align: center;\n}\n</style>\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n\nMost of the content in this chapter should be review from UC Davis Epi 202.\n\n## Statistical events\n\n---\n\n:::{#thm-prob-subset}\nIf $A$ and $B$ are statistical events and $A\\subseteq B$, then $p(A, B) = p(A)$.\n:::\n\n---\n\n::: proof\nLeft to the reader.\n:::\n\n## Random variables\n\n### Binary variables {#sec-binary-vars}\n\n\n\n:::{#def-binary}\n\n##### binary variable\n\nA **binary variable** is a random variable which has only two possible values in its range.\n\n:::\n\n\n::: {#exr-binary-examples}\n##### Examples of binary variables\n\nWhat are some examples of binary variables in the health sciences?\n:::\n\n---\n\n:::: {.solution}\n\nExamples of binary outcomes include:\n\n* exposure (exposed vs unexposed)\n* disease (diseased vs healthy)\n* recovery (recovered vs unrecovered)\n* relapse (relapse vs remission)\n* return to hospital (returned vs not)\n* vital status (dead vs alive)\n\n\n\n\n::::\n\n\n\n---\n\n### Count variables {#sec-count-vars}\n\n\n\n:::{#def-count}\n#### Count variable\n\nA **count variable** is a random variable whose possible values are some subset of the non-negative integers; that is, a random variable $X$ such that:\n\n$$\\rangef{X} \\in \\Nat$$\n\n:::\n\n---\n\n::: {#exr-count-examples}\nWhat are some examples of count variables?\n:::\n\n---\n\n::: solution\n* Number of fish in a pond\n* Number of cyclones per season\n* [Seconds of tooth-brushing per session (if rounded)](https://pubmed.ncbi.nlm.nih.gov/35587489/)\n* Infections per person-year\n* Visits to ER per person-month\n* Car accidents per 1000 miles driven\n\n\n:::\n\n---\n\n#### Exposure magnitude\n\n:::{#def-exposure}\n\n##### Exposure magnitude\n\nFor many count outcomes, \nthere is some sense of \n**exposure magnitude**,\n**population size**,  or \n**duration of observation**.\n\n:::\n\n---\n\n:::{#exr-exposure-magnitude}\nWhat are some examples of exposure magnitudes?\n:::\n\n---\n\n::: {.solution .smaller}\n\noutcome                      | exposure units\n-----------------------------| -------------\ndisease incidence            | number of individuals exposed; time at risk\ncar accidents                | miles driven\nworksite accidents           | person-hours worked\npopulation size              | size of habitat\n\n: Examples of exposure units {#tbl-exposure-units}\n\n:::\n\n:::: notes\nExposure units are similar to \nthe number of trials in a binomial distribution,\nbut **in non-binomial count outcomes, there can be more than one event per unit of exposure**.\n\nWe can use $t$ to represent continuous-valued exposures/observation durations, \nand $n$ to represent discrete-valued exposures.\n\n::::\n\n---\n\n::: {#def-event-rate}\n#### Event rate\n\n:::: notes\nFor a count outcome $Y$ with exposure magnitude $t$,\nthe **event rate** (denoted $\\lambda$) \nis defined as the mean of $Y$ divided by the the exposure magnitude. \nThat is:\n::::\n\n$$\\mu \\eqdef \\Expp[Y|T=t]$$\n\n$$\\lambda \\defeq \\frac{\\mu}{t}$$ {#eq-def-event-rate}\n:::\n\n\n::: notes\nEvent rate is somewhat analogous to odds in binary outcome models;\nit typically serves as an intermediate transformation between the mean of the outcome and the linear component of the model. \nHowever, in contrast with the odds function, the transformation $\\lambda = \\mu/t$ is *not* considered part of the Poisson model's link function, and it treats the exposure magnitude covariate differently from the other covariates.\n:::\n\n---\n\n:::{#thm-mean-vs-event-rate}\n#### Transformation function from event rate to mean\n\nFor a count variable with mean $\\mu$, event rate $\\lambda$, and exposure magnitude $t$:\n\n$$\\tf \\mu  = \\lambda \\cdot t$${#eq-lambda-to-mu}\n\n:::\n\n---\n\n::: solution\nStart from definition of event rate and use algebra to solve for $\\mu$.\n:::\n\n---\n\n@eq-lambda-to-mu is analogous to the inverse-odds function for binary variables.\n\n---\n\n::: {#thm-non-exposed}\nWhen the exposure magnitude is 0, there is no opportunity for events to occur:\n\n$$\\Expp[Y|T=0] = 0$$\n:::\n\n--- \n\n::: proof\n$$\\Expp[Y|T=0] = \\lambda \\cdot 0 = 0$$\n:::\n\n---\n\n#### Probability distributions for count outcomes\n\n- [Poisson distribution](#sec-poisson-dist)\n\n- [Negative binomial distribution](#sec-nb-dist)\n\n\n\n---\n\n## Key probability distributions\n\n### The Bernoulli distribution {#sec-bern-dist}\n\n\n:::::{#def-bernoulli}\n#### Bernoulli distribution\nThe **Bernoulli distribution** family for a random variable $X$ is defined as:\n\n$$\n\\ba\n\\Pr(X=x) &= \\1{x\\in \\set{0,1}}\\pi^x(1-\\pi)^{1-x}\\\\\n&= \\cbl{{\\pi, x=1}\\atop{1-\\pi, x=0}}\n\\ea\n$$\n\n\n:::::\n\n\n\n---\n\n### The Poisson distribution {#sec-poisson-dist}\n\n\n::: {#fig-poissons layout-ncol=2}\n\n![Sim├йon Denis Poisson](images/poisson.jpg){width=40%, height=40%}\n\n![[Les Poissons](https://youtu.be/UoJxBEQRLd0?t=12)](images/poissons.jpeg){width=40%, height=40%}\n\n\"Les Poissons\"\n\n:::\n\n---\n\n:::::{#def-poisson}\n#### Poisson distribution\n\n$$\\rangef{Y} = \\set{0, 1, 2, ...} = \\Nat$$\n\n$$\\P(Y = y) = \\frac{\\mu^{y} e^{-\\mu}}{y!}, y \\in \\Nat$$ {#eq-pois-pmf}\n\n::: notes\n(see @fig-pois-pmf)\n:::\n\n$$\\P(Y \\le y) = e^{-\\mu} \\sum_{j=0}^{\\floor{y}}\\frac{\\mu^j}{j!}$$ {#eq-pois-cdf}\n\n::: notes\n(see @fig-pois-cdfs)\n:::\n\n:::::\n\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\npois_dists = tibble(\n  mu = c(0.5, 1, 2, 5, 10, 20)) |> \n  reframe(\n    .by = mu,\n    x = 0:30\n  ) |> \n  mutate(\n    `P(X = x)` = dpois(x, lambda = mu),\n    `P(X <= x)` = ppois(x, lambda = mu),\n    mu = factor(mu)\n  )\n\nlibrary(ggplot2)\nlibrary(latex2exp)\n\nplot0 = pois_dists |> \n  ggplot(\n    aes(\n      x = x,\n      y = `P(X = x)`,\n      fill = mu,\n      col = mu)) +\n  theme(legend.position = \"bottom\") +\n  labs(\n    fill = latex2exp::TeX(\"$\\\\mu$\"),\n    col = latex2exp::TeX(\"$\\\\mu$\"),\n    y = latex2exp::TeX(\"$\\\\Pr_{\\\\mu}(X = x)$\"))\n\nplot1 = plot0 + \n  geom_col(position = \"identity\", alpha  = .5) +\n  facet_wrap(~mu)\n  # geom_point(alpha = 0.75) +\n  # geom_line(alpha = 0.75)\nprint(plot1)\n\n```\n\n::: {.cell-output-display}\n![Poisson PMFs, by mean parameter $\\mu$](probability_files/figure-pdf/fig-pois-pmf-1.pdf){#fig-pois-pmf}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nplot2 = \n  plot0 + \n  geom_step(alpha = 0.75) +\n  aes(y = `P(X <= x)`) + \n  labs(y = latex2exp::TeX(\"$\\\\Pr_{\\\\mu}(X \\\\leq x)$\"))\n\nprint(plot2)\n\n```\n\n::: {.cell-output-display}\n![Poisson CDFs](probability_files/figure-pdf/fig-pois-cdfs-1.pdf){#fig-pois-cdfs}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n\n:::{#exr-pois-dist-funs}\n#### Poisson distribution functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet $X \\sim \\Pois(\\mu = 3.75)$.\n\nCompute:\n\n- $\\P(X = 4 | \\mu = 3.75)$\n- $\\P(X \\le 7 | \\mu = 3.75)$\n- $\\P(X > 5 | \\mu = 3.75)$\n\n:::\n\n---\n\n::: solution\n\n- $\\P(X=4) = 0.1938$\n- $\\P(X\\le 7) = 0.9624$\n- $\\P(X > 5) = 0.1771$\n\n:::\n\n---\n\n:::{#thm-poisson-properties}\n#### Properties of the Poisson distribution\n\nIf $X \\sim \\Pois(\\mu)$, then:\n\n* $\\Expp[X] = \\mu$\n* $\\Varr(X) = \\mu$\n* $\\P(X=x) = \\frac{\\mu}{x} \\P(X = x-1)$\n* For $x < \\mu$, $\\P(X=x) > \\P(X = x-1)$\n* For $x = \\mu$, $\\P(X=x) = \\P(X = x-1)$\n* For $x > \\mu$, $\\P(X=x) < \\P(X = x-1)$\n* $\\arg \\max_{x} \\P(X=x) = \\floor{\\mu}$\n\n:::\n\n:::{#exr-poisson-properties}\nProve @thm-poisson-properties.\n:::\n\n---\n\n::: {.solution .smaller}\n\n$$\n\\begin{aligned}\n\\text{E}[X] \n&= \\sum_{x=0}^\\infty x \\cdot P(X=x)\\\\\n&= 0 \\cdot P(X=0) + \\sum_{x=1}^\\infty x \\cdot P(X=x)\\\\\n&= 0 + \\sum_{x=1}^\\infty x \\cdot P(X=x)\\\\\n&= \\sum_{x=1}^\\infty x \\cdot P(X=x)\\\\\n&= \\sum_{x=1}^\\infty x \\cdot \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\n&= \\sum_{x=1}^\\infty x \\cdot \\frac{\\lambda^x e^{-\\lambda}}{x \\cdot (x-1)!} & [\\text{definition of factorial (\"!\") function}]\\\\\n&= \\sum_{x=1}^\\infty \\frac{\\lambda^x e^{-\\lambda}}{ (x-1)!}\\\\\n&= \\sum_{x=1}^\\infty \\frac{(\\lambda \\cdot \\lambda^{x-1}) e^{-\\lambda}}{ (x-1)!}\\\\\n&= \\lambda \\cdot \\sum_{x=1}^\\infty \\frac{( \\lambda^{x-1}) e^{-\\lambda}}{ (x-1)!}\\\\\n&= \\lambda \\cdot \\sum_{y=0}^\\infty \\frac{( \\lambda^{y}) e^{-\\lambda}}{ (y)!} &[\\text{substituting } y \\eqdef x-1]\\\\\n&= \\lambda \\cdot 1 &[\\text{because PDFs sum to 1}]\\\\\n&= \\lambda\\\\\n\\end{aligned}\n$$\n\nSee also <https://statproofbook.github.io/P/poiss-mean>.\n\nFor the variance, see <https://statproofbook.github.io/P/poiss-var>.\n:::\n\n\n---\n\n#### Accounting for exposure\n\nIf the exposures/observation durations, denoted $T=t$ or $N=n$, vary between observations, we model:\n\n$$\\mu = \\lambda\\cdot t$$\n\n$\\lambda$ is interpreted as the \"expected event rate per unit of exposure\"; that is,\n\n$$\\lambda = \\frac{\\Expp[Y|T=t]}{t}$$\n\n:::{.callout-important}\n\nThe exposure magnitude, $T$, is *similar* to a covariate in linear or logistic regression. \nHowever, there is an important difference: in count regression, **there is no intercept corresponding to $\\Expp[Y|T=0]$**. \nIn other words, this model assumes that if there is no exposure, there can't be any events.\n\n:::\n\n:::{#thm-exposure-log-scale}\n\nIf $\\mu = \\lambda\\cdot t$, then:\n\n$$\\log \\mu = \\log{\\lambda} + \\log{t}$$\n:::\n\n\n:::{#def-offset}\n#### Offset\n\nWhen the linear component of a model involves a term without an unknown coefficient, \nthat term is called an **offset**.\n\n:::\n\n---\n\n:::{#thm-sum-pois}\n\nIf $X$ and $Y$ are independent Poisson random variables with means \n$\\mu_X$ and $\\mu_Y$, their sum, $Z=X+Y$, is also a Poisson random variable, with mean \n$\\mu_Z = \\mu_X + \\mu_Y$.\n\n:::\n\n---\n\n::: proof\nSee <https://web.stanford.edu/class/archive/cs/cs109/cs109.1206/lectureNotes/LN12_independent_rvs.pdf>, Example 3.\n:::\n\n\n\n---\n\n### The Negative-Binomial distribution {#sec-nb-dist}\n\n\n:::{#def-nb}\n#### Negative binomial distribution\n\n$$\n\\P(Y=y) \n= \\frac{\\mu^y}{y!} \n\\cdot \\frac{\\Gamma(\\rho + y)}{\\Gamma(\\rho) \\cdot (\\rho + \\mu)^y} \n\\cdot \\left(1+\\frac{\\mu}{\\rho}\\right)^{-\\rho}\n$$\n\nwhere $\\rho$ is an overdispersion parameter \nand $\\Gamma(x) = (x-1)!$ for integers $x$.\n\n:::\n\n::: notes\nYou don't need to memorize or understand this expression.\n\nAs $\\rho \\rightarrow \\infty$, \nthe second term converges to 1 \nand the third term converges to $\\exp{-\\mu}$, \nwhich brings us back to the Poisson distribution.\n\n:::\n\n---\n\n:::{#thm-nb}\nIf $Y \\sim \\NegBin(\\mu, \\rho)$, then:\n\n- $\\Expp[Y] = \\mu$\n- $\\Var{Y} = \\mu + \\frac{\\mu^2}{\\rho} > \\mu$\n\n:::\n\n\n\n### Weibull Distribution {#sec-weibull}\n\n$$\n\\begin{aligned}\np(t)&= \\alpha\\lambda x^{\\alpha-1}\\text{e}^{-\\lambda x^\\alpha}\\\\\nh(t)&=\\alpha\\lambda x^{\\alpha-1}\\\\\nS(t)&=\\text{e}^{-\\lambda x^\\alpha}\\\\\nE(T)&= \\Gamma(1+1/\\alpha)\\cdot \\lambda^{-1/\\alpha}\n\\end{aligned}\n$$\n\nWhen $\\alpha=1$ this is the exponential. When $\\alpha>1$ the hazard is\nincreasing and when $\\alpha < 1$ the hazard is decreasing. This provides\nmore flexibility than the exponential.\n\nWe will see more of this distribution later.\n\n## Characteristics of probability distributions\n\n### Probability density function {#sec-prob-dens}\n\n\n:::{#def-pdf}\n#### probability density\n\nIf $X$ is a continuous random variable, then the **probability density** of\n$X$ at value $x$,\ndenoted $f(x)$, $f_X(x)$, $\\p(x)$, $\\p_X(x)$, or $\\p(X=x)$,\nis defined as the limit of the probability (mass) that $X$ is in an\ninterval around $x$,\ndivided by the width of that interval,\nas that width reduces to 0.\n\n$$\n\\ba\nf(x) &\\eqdef  \\lim_{\\delta \\rightarrow 0}\n   \\frac{\\P(X \\in [x, x + \\delta])}{\\delta}\n\\ea\n$$\n\n::: notes\nSee also <https://en.wikipedia.org/wiki/Probability_density_function#Formal_definition>\n:::\n\n:::\n\n\n\n---\n\n:::{#thm-density-vs-CDF}\n### Density function is derivative of CDF\n\nThe density function $f(t)$ or $\\p(T=t)$ for a random variable $T$ at value $t$ is equal to the derivative of the cumulative probability function $F(t) \\eqdef P(T\\le t)$; that is:\n\n$$f(t) \\eqdef \\deriv{t} F(t)$$\n\n:::\n\n---\n\n:::{#thm-density-sums-to-one}\n#### Density functions integrate to 1\n\nFor any density function $f(x)$,\n\n$$\\int_{x \\in \\rangef{X}} f(x) dx = 1$$\n:::\n\n---\n\n### Hazard function {#sec-prob-haz}\n\n\n\n:::{#def-hazard}\n\n##### Hazard function, hazard rate, hazard rate function\n\n::: notes\nThe **hazard function**, **hazard rate**,  **hazard rate function**, \nfor a random variable $T$ at value $t$,\ntypically denoted as $h(t)$ or $\\lambda(t)$,\nis the conditional [density](probability.qmd#def-pdf) of $T$ at $t$, \ngiven $T \\ge t$.\nThat is:\n:::\n\n$$\\haz(t) \\eqdef \\p(T=t|T\\ge t)$$\n\n::: notes\nIf $T$ represents the time at which an event occurs, \nthen $\\haz(t)$ is the probability that the event occurs at time $t$, \ngiven that it has not occurred prior to time $t$.\n:::\n\n:::\n\n\n\n\n---\n\n### Expectation {#sec-expectation}\n\n:::{#def-expectation}\n### Expectation, expected value, population mean \\index{expectation} \\index{expected value}\n\nThe **expectation**, **expected value**, or **population mean** of a *continuous* random variable $X$, denoted $\\E{X}$, $\\mu(X)$, or $\\mu_X$, is the weighted mean of $X$'s possible values, weighted by the probability density function of those values:\n\n$$\\E{X} = \\int_{x\\in \\rangef{X}} x \\cdot \\p(X=x)dx$$\n\nThe **expectation**, **expected value**, or **population mean** of a *discrete* random variable $X$, denoted $\\E{X}$, $\\mu(X)$, or $\\mu_X$, is the mean of $X$'s possible values, weighted by the probability mass function of those values:\n\n$$\\E{X} = \\sum_{x \\in \\rangef{X}} x \\cdot \\P(X=x)$$\n\n(c.f. <https://en.wikipedia.org/wiki/Expected_value>)\n\n:::\n\n---\n\n:::{#thm-bernoulli-mean}\n#### Expectation of the Bernoulli distribution\n\nThe expectation of a Bernoulli random variable with parameter $\\pi$ is:\n\n$$\\E{X} = \\pi$$\n:::\n\n---\n\n:::{.proof}\n\n$$\n\\ba\n\\E{X}\n&= \\sum_{x\\in \\rangef{X}} x \\cd \\P(X=x)\n\\\\&= \\sum_{x\\in \\set{0,1}} x \\cd \\P(X=x)\n\\\\&= \\paren{0 \\cd \\P(X=0)} + \\paren{1 \\cd \\P(X=1)}\n\\\\&= \\paren{0 \\cd (1-\\pi)} + \\paren{1 \\cd \\pi}\n\\\\&= 0 + \\pi\n\\\\&= \\pi\n\\ea\n$$\n\n:::\n\n---\n\n### Variance and related characteristics\n\n:::{#def-variance}\n#### Variance\n\nThe variance of a random variable $X$ is the expectation of the squared difference between $X$ and $\\E{X}$; that is:\n\n$$\n\\Var{X} \\eqdef \\E{(X-\\E{X})^2}\n$$\n\n:::\n\n---\n\n:::{#thm-variance}\n#### Simplified expression for variance\n\n$$\\Var{X}=\\E{X^2} - \\sqf{\\E{X}}$$\n\n---\n\n::::{.proof}\nBy linearity of expectation, we have:\n\n$$\n\\begin{aligned}\n\\Var{X} \n&\\eqdef \\E{(X-\\E{X})^2}\\\\\n&=\\E{X^2 - 2X\\E{X} + \\sqf{\\E{X}}}\\\\\n&=\\E{X^2} - \\E{2X\\E{X}} + \\E{\\sqf{\\E{X}}}\\\\\n&=\\E{X^2} - 2\\E{X}\\E{X} + \\sqf{\\E{X}}\\\\\n&=\\E{X^2} - \\sqf{\\E{X}}\\\\\n\\end{aligned}\n$$\n::::\n\n:::\n\n---\n\n::: {#def-precision}\n#### Precision\n\nThe **precision** of a random variable $X$, often denoted $\\tau(X)$, $\\tau_X$, or shorthanded as $\\tau$, is\nthe inverse of that random variable's variance; that is:\n\n$$\\tau(X) \\eqdef \\inv{\\Var{X}}$$\n:::\n\n::: {#def-sd}\n\n#### Standard deviation\n\nThe standard deviation of a random variable $X$ is the square-root of the variance of $X$:\n\n$$\\SD{X} \\eqdef \\sqrt{\\Var{X}}$$\n\n:::\n\n---\n\n:::{#def-cov}\n#### Covariance\n\nFor any two one-dimensional random variables, $X,Y$:\n\n$$\\Cov{X,Y} \\eqdef \\Expf{(X - \\E X)(Y - \\E Y)}$$\n\n:::\n\n---\n\n:::{#thm-alt-cov}\n$$\\Cov{X,Y}= \\E{XY} - \\E{X} \\E{Y}$$\n:::\n\n---\n\n:::{.proof}\nLeft to the reader.\n:::\n\n---\n\n:::{#lem-cov-xx}\n\n##### The covariance of a variable with itself is its variance\n\nFor any random variable $X$:\n\n$$\\Cov{X,X} = \\Var{X}$$\n\n:::\n\n:::{.proof}\n$$\n\\ba\n\\Cov{X,X} &= E[XX] - E[X]E[X] \n\\\\ &= E[X^2]-(E[X])^2\n\\\\ &= \\Var{X}\n\\ea\n$$\n:::\n\n---\n\n:::{#def-cov-vec-x}\n\n#### Variance/covariance of a $p \\times 1$ random vector\n\nFor a $p \\times 1$ dimensional random vector $X$,\n\n$$\n\\begin{aligned}\n\\text{Var}(X) \n&\\eqdef \\text{Cov}(X)\\\\\n&\\eqdef E[ \\left( X - E\\lbrack X\\rbrack \\right)^{\\top}\\left( X - E\\lbrack X\\rbrack \\right) ]\\\\\n\\ea\n$$\n\n:::\n\n---\n\n:::{#thm-vcov-vec}\n\n#### Alternate expression for variance of a random vector\n\n$$\n\\ba\n\\Var{X} \n&= E[ X^{\\top}X ] - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack\n\\end{aligned}\n$$\n:::\n\n---\n\n:::{.proof}\n$$\n\\ba\n\\Var{X} \n&= E[ \\left( X^{\\top} - E\\lbrack X\\rbrack^{\\top} \\right)\\left( X - E\\lbrack X\\rbrack \\right) ]\\\\\n&= E[ X^{\\top}X - E\\lbrack X\\rbrack^{\\top}X - X^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack ]\\\\\n&= E[ X^{\\top}X ] - E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack\\\\\n&= E[ X^{\\top}X ] - 2{E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack\\\\\n&= E[ X^{\\top}X ] - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack\n\\end{aligned}\n$$\n:::\n\n---\n\n:::{#thm-var-lincom}\n\n#### Variance of a linear combination\n\nFor any set of random variables $\\Xin$ and corresponding constants $a_1, ... ,a_n$:\n\n$$\\Var{\\sumin a_i X_i} = \\sumin \\sumn{j} a_i a_j \\Cov{X_i,X_j}$$\n:::\n\n---\n\n:::{.proof}\n\nLeft to the reader...\n\n:::\n\n---\n\n:::{#lem-var-lincom2}\n\nFor any two random variables $X$ and $Y$ and scalars $a$ and $b$:\n\n$$\\Var{aX + bY} = a^2 \\Var{X} + b^2 \\Var{Y} + 2(a \\cd b) \\Cov{X,Y}$$\n\n:::\n\n---\n\n:::{.proof}\n\nApply @thm-var-lincom with $n=2$, $X_1 = X$, and $X_2 = Y$.\n\nOr, see <https://statproofbook.github.io/P/var-lincomb.html>\n\n:::\n\n---\n\n:::{#def-homosked}\n### homoskedastic, heteroskedastic\n\nA random variable $Y$ is **homoskedastic** (with respect to covariates $X$) if the variance of $Y$ does not vary with $X$: \n\n$$\\Varr(Y|X=x) = \\ss, \\forall x$$\n\nOtherwise it is **heteroskedastic**.\n\n:::\n\n---\n\n:::{#def-indpt}\n\n### Statistical independence\n\nA set of random variables $\\X1n$ are **statistically independent** \nif their joint probability is equal to the product of their marginal probabilities:\n\n$$\\Pr(\\Xx1n) = \\prodi1n{\\Pr(X_i=x_i)}$$\n\n:::\n\n::: notes\n\n::::{.callout-tip}\nThe symbol for independence, $\\ind$, is essentially just $\\prod$ upside-down.\nSo the symbol can remind you of its definition (@def-indpt).\n::::\n\n:::\n\n---\n\n:::{#def-cind}\n\n### Conditional independence\n\nA set of random variables $\\dsn{Y}$ are **conditionally statistically independent** \ngiven a set of covariates $\\X1n$\nif the joint probability of the $Y_i$s given the $X_i$s is equal to \nthe product of their marginal probabilities:\n\n$$\\Pr(\\dsvn{Y}{y}|\\dsvn{X}{x}) = \\prodi1n{\\Pr(Y_i=y_i|X_i=x_i)}$$\n\n:::\n\n---\n\n:::{#def-ident}\n\n#### Identically distributed\n\nA set of random variables $\\X1n$ are **identically distributed**\nif they have the same range $\\rangef{X}$ and if\ntheir marginal distributions $\\P(X_1=x_1), ..., \\P(X_n=x_n)$ are all \nequal to some shared distribution $\\P(X=x)$:\n\n$$\n\\forall i\\in \\set{1:n}, \\forall x \\in \\rangef{X}: \\P(X_i=x) = \\P(X=x)\n$$\n\n:::\n\n---\n\n:::{#def-cident}\n\n#### Conditionally identically distributed\n\nA set of random variables $\\dsn{Y}$ are **conditionally identically distributed** \ngiven a set of covariates $\\X1n$ \nif $\\dsn{Y}$ have the same range $\\rangef{X}$ and if\nthe distributions $\\P(Y_i=y_i|X_i =x_i)$ are all \nequal to the same distribution $\\P(Y=y|X=x)$:\n\n$$\n\\P(Y_i=y|X_i=x) = \\P(Y=y|X=x)\n$$\n \n:::\n\n---\n\n:::{#def-iid}\n#### Independent and identically distributed\n\nA set of random variables $\\dsn{X}$ are **independent and identically distributed**\n(shorthand: \"$X_i\\ \\iid$\") if they are statistically independent and identically distributed.\n\n:::\n\n---\n\n:::{#def-iid}\n#### Conditionally independent and identically distributed\n\nA set of random variables $\\dsn{Y}$ are **conditionally independent and identically distributed** (shorthand: \"$Y_i | X_i\\ \\ciid$\" or just \"$Y_i |X_i\\ \\iid$\") given a set of covariates $\\dsn{X}$\nif $\\dsn{Y}$ are conditionally independent given $\\dsn{X}$ and $\\dsn{Y}$ are identically distributed given \n$\\dsn{X}$.\n\n:::\n\n\n## The Central Limit Theorem\n\nThe sum of many independent or nearly-independent random variables with\nsmall variances (relative to the number of RVs being summed) \nproduces bell-shaped distributions.\n\nFor example, consider the sum of five dice (@fig-clt-5d6).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ndist = \n  expand.grid(1:6, 1:6, 1:6, 1:6, 1:6) |> \n  rowwise() |>\n  mutate(total = sum(c_across(everything()))) |> \n  ungroup() |> \n  count(total) |> \n  mutate(`p(X=x)` = n/sum(n))\n\nlibrary(ggplot2)\n\ndist |> \n  ggplot() +\n  aes(x = total, y = `p(X=x)`) +\n  geom_col() +\n  xlab(\"sum of dice (x)\") +\n  ylab(\"Probability of outcome, Pr(X=x)\") +\n  expand_limits(y = 0)\n\n  \n  \n```\n\n::: {.cell-output-display}\n![Distribution of the sum of five dice](probability_files/figure-pdf/fig-clt-5d6-1.pdf){#fig-clt-5d6}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn comparison, the outcome of just one die is not bell-shaped (@fig-clt-1d6).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ndist = \n  expand.grid(1:6) |> \n  rowwise() |>\n  mutate(total = sum(c_across(everything()))) |> \n  ungroup() |> \n  count(total) |> \n  mutate(`p(X=x)` = n/sum(n))\n\nlibrary(ggplot2)\n\ndist |> \n  ggplot() +\n  aes(x = total, y = `p(X=x)`) +\n  geom_col() +\n  xlab(\"sum of dice (x)\") +\n  ylab(\"Probability of outcome, Pr(X=x)\") +\n  expand_limits(y = 0)\n\n  \n  \n```\n\n::: {.cell-output-display}\n![Distribution of the outcome of one die](probability_files/figure-pdf/fig-clt-1d6-1.pdf){#fig-clt-1d6}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat distribution does a single die have?\n\nAnswer: discrete uniform on 1:6.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}