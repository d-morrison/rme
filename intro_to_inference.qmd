---
title: "intro_to_inference"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{intro_to_inference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(2023)
```

## What is statistical inference?

Quantifying uncertainty based on data and assumptions.

Uncertainty about:
- the distribution of outcomes
- where an outcome might be (prediction)
- the differences in distributions between subgroups

Modeling random^[or chaotic] outcomes

Making predictions

## What is probability?

-   Frequency of occurrence in infinite repetitions?
-   The probability that the world will end tomorrow
-   The probability that a Democrat will win the 2024 presidential election
-   The probability that the next coin flip will come up heads

## Subjective probability

Probability is how certain you are that an event will occur (in a single observation).

## All models are wrong, some are useful

Probability isn't real (except maybe quantum) but it's a useful idea.

## Different kinds of data

- counts
- proportions
- binary outcomes
- ranked scales
- continuous measurements
- lengths
- weights
- densities
- concentrations
- time durations

- nominal
- ordinal
- interval
- rational/ratio (zeros meaningful)

- discrete counts
- continuous measurements

## Probability models

- counts:
- Poisson
- geometric
- negative binomial
- hypergeometric
- proportions:
- uniform
- beta
- categorical outcomes:
- bernoulli
- binomial
- multinomial
- time durations:
- exponential
- gamma
- Weibull
- test statistics, linreg parameter estimates
- chi-square
- t
- noise, glm parameter estimates
- gaussian
- 

# Inference (review)

## Estimates

Functions of data with good properties:

- converge to the true parameter (P=1) with enough data.

## Confidence intervals

A pair of summary statistics, $L(X)$, $U(X)$, with the property: $$Pr\{\theta \in [L(X), R(X)]\} = \gamma$$ for all $\theta$.

If we observe data $X=x$ and compute $L(x)=l, R(x)=r$ what do we know about $\theta$?

$$p(\theta | L=l, R=r) = ?$$

## p-values

$$
p(x)=P(|X|>|x|\mid \theta=\theta_0)?
$$

$$
p(\theta=\theta_0 | X=x) = ?
$$



```{r}

tab1 = tribble(
  ~`Outcome`, ~Examples, ~Analysis,
  "binary", 
  c("diseased/not", 
    "died/survived"), 
  "logistic regression",
  
  "count",
  c(
    "# infections per month",
    "# follow-up visits per patient"),
  "Poisson regression",
  
  "time to event",
  c(
    "time from exposure until disease onset",
    "time from onset until recurrence",
    "time from disease onset until disease progression",
    "time from onset until death"),
  "survival analysis",
  
  "ordered levels*",
  c(
    "Pain level (0:10 scale)",
    "Approval ratings (0:5 stars)"
  ),
  "ordinal regression"
  
  
)

```


```{r}

# tab1 |> kable("html") |> kable_styling(font_size = 24)
tab1 |> pander()

```

::: aside
\* time permitting
:::



## Goal

Model the statistical (not causal (yet)) relationship between an outcome ($Y$) and one or more covariates ($X$ = ($X_1, ..., X_p$)).

For example, we might want to know how heavy a cow might be if it eats grass all summer.

Note that I didn't ask yet how heavy it might be if we intervene to make it eat grass all summer.

Suppose we go out and find some cows, some are eating grass, the others are eating hay.

We can observe the weights of the cows eating grass, and we can observe the weights of the cows eating hay, but we don't know what would happen if we made the grass cows eat hay instead.

Would it be the same as what happened to the cows we found eating hay?

Are they the same breed of cows?

How did some end up eating hay and others eating grass? Maybe the bigger ones chased the smaller ones off of the grass.

We can answer these kinds of causal questions, but we generally need to answer descriptive questions first.

Descriptive questions don't need to even involve covariates that we **could** intervene on; they just need to be covariates that we can observe^[or else things will be more complicated, but we can sometimes still do it].

See Epi 207.

## we can't model each subgroup individually, 
so we will need to make some assumptions about how the subgroups relate to each other, 
so we can combine information from different subgroups.

What assumptions might we make?

How about, the means fall along a straight line?

This assumption allows us to talk about "the change in mean outcome per one unit change in predictor".

## Back to math

[maybe use height and calories?]

We want to know the distribution of $Y$ that we observe when the covariate $X$ has some specific value, e.g., $X = x$: $$p(Y=y|X=x)$$.

Ideally, we would collect a bunch of observations for each value of $X$ that we might be interested. So if that's $X = \text{hay}$ and $X = \text{grass}$, we would collect some data on the hay cows, and some data on the grass cows, and we would analyze each pile of data separately, using the methods you learned last quarter (t-tests and confidence intervals, hopefully),

But what happens when we want to know about $X$ values that we don't have a lot of data on? or no data on?

What can we do?

Covariates = subgroups.

Depending on the source of the data (experimental vs observational), we might have just one observation for many^[or all] of the subgroups we are interested in might have just one, or even no observations.


Example data set: CD4 counts.

Zoom in.

```{r}

```

What do we know about


# Simulation

sensitivity, spec of a test. alpha level of a hypothesis test or CI
