---
title: "binom likelihood app"
date: last-modified
date-format: "[Last modified:] YYYY-MM-DD: h:mm:ss"
format: 
  html: default
  # pdf:
  #   number-sections: true
  #   code-fold: true
  #   code-summary: "Show the code"
  #   code-link: true
editor_options: 
  chunk_output_type: console
server: shiny
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
#tried to wrap code chunks with the option below, but didn't work
#knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)

options(scipen = 1, digits = 5)
library(tidyverse)
library(plotly)
library(kableExtra)
library(pander)
```

Each subsection is worth two points.

# Maximum likelihood inference for binary outcomes

## Simulate binary data

Simulate one hundred mutually independent binary outcomes with probability $\pi = 0.7$.

```{r}
rm(list = ls())

n = 100
pi = 0.7

set.seed(1)
x = rbinom(n = n, size = 1, prob = pi)

```

::: {.callout-tip}
Use `set.seed(1)` before you generate random numbers to make your simulation have the same results each time you restart it.
:::

## Write down the mathematical likelihood of your data as a function of $\pi$.

\ 
$$ 
\mathcal{L}_{\mathbb{X}}(\pi) = \pi^{\sum_{i=1}^n x_i} (1-\pi)^{n-\sum_{i=1}^n x_i} 
$$

## Implement the likelihood function and graph it.

:::{.sol}

```{r}
#| label: fig-graph-lik
#| fig-cap: "likelihood"
likelihood = function(pi, x)
{
  n = length(x)
  pi^sum(x) * (1-pi)^(n-sum(x))
}

ggplot() + 
  geom_function(
    fun = likelihood,
    args = list(x = x),
    n = 100000) + 
  xlim(0,1) +
  ylab("L(pi)") +
  xlab("pi") +
  theme_bw()
```

```{r}
#| label: fig-graph-llik
#| fig-cap: "log(likelihood)"
ll = function(pi, x)
{
  n = length(x)
  log(pi)*sum(x) + log(1-pi)*(n-sum(x))
}
ggplot() + 
  geom_function(
    fun = ll,
    args = list(x = x),
    n = 100000) + 
  xlim(0,1) +
  ylab("log(L(pi))") +
  xlab("pi") +
  theme_bw()
```

```{r}
#| context: server
lik = function(pi, x)
{
  n = length(x)
  pi^sum(x) * (1-pi)^(n-sum(x))
}
llik = function(pi, x)
{
  n = length(x)
  log(pi)*sum(x) + log(1-pi)*(n-sum(x))
}


```

```{r}
sliderInput(
  inputId = "logn",
  label = "log_10(n)", 
  value = 3, 
  min = 0,
  step = .1,
  max = 6)
textOutput("n")
checkboxInput(
  "logl",
  "log-likelihood",
  value = TRUE
)
plotOutput("plot1")
```

```{r}
#| context: server

library(ggplot2)
n = reactive(round(10^(input$logn)))
pi = 0.7
x2 = reactive(
  {
    set.seed(1)
    rbinom(
      n = n(), 
      size = 1, 
      prob = pi)
  })

output$n = shiny::renderText(n())

likelihood_plot = reactive(
  {
    temp = 
      ggplot() + 
      xlim(0,1) +
      ylab("L(pi)") +
      xlab("pi") +
      theme_bw()
    if(input$logl)
    {
      temp = temp + 
        geom_function(
        fun = llik,
        args = list(x = x2()),
        n = 100000) +
        scale_y_continuous(label = scales::comma)
      
    } else
    {
      temp = temp + 
        geom_function(
        fun = lik,
        args = list(x = x2()),
        n = 100000)
    }
    temp
  }
)
output$plot1 =  shiny::renderPlot(likelihood_plot())

```

:::

## Find the maximum likelihood estimate of $\pi$ using calculus.

\ 

First we find the log-likelihood:
$$
\begin{aligned}
\mathcal{L}_{\mathbb{X}}(\pi) &= \pi^{\sum_{i=1}^n x_i} (1-\pi)^{n-\sum_{i=1}^n x_i}\\
\ell(\pi) &= \log\mathcal{L}_{\mathbb{X}}(\pi)\\
&= \log(\pi){\sum_{i=1}^n x_i} +  \log(1-\pi)\left(n-\sum_{i=1}^n x_i\right)\\
\end{aligned}
$$


Then we take the first derivative of the log-likelihood:
$$
\begin{aligned}
\ell'(\pi) = \frac{\partial}{\partial\pi}\ell(\pi) &= \frac{1}{\pi}\left(\frac{\partial}{\partial\pi}\pi\right){\sum_{i=1}^n x_i} +   \frac{1}{(1-\pi)}\left(\frac{\partial}{\partial\pi}(1-\pi)\right)\left(n-\sum_{i=1}^n x_i\right)\\
&= \frac{1}{\pi}{\sum_{i=1}^n x_i} - \frac{1}{1-\pi}\left(n-\sum_{i=1}^n x_i\right)\\
\end{aligned}
$$

Then we set the first derivative of the log-likelihood to 0 and solve for $\pi$.
$$
\begin{aligned}
0 &= \ell'(\pi)\\
0  &= \frac{1}{\pi}{\sum_{i=1}^n x_i} - \frac{1}{1-\pi}\left(n-\sum_{i=1}^n x_i\right)\\
0  &= \frac{1}{\pi}{\sum_{i=1}^n x_i} - \frac{n}{1-\pi} +\frac{1}{1-\pi} \sum_{i=1}^n x_i\\
0  &= \left(\frac{1}{\pi}+\frac{1}{1-\pi}\right){\sum_{i=1}^n x_i} - \frac{n}{1-\pi}\\
0  &= \left(\frac{1-\pi}{\pi(1-\pi)} + \frac{ \pi}{\pi(1-\pi)}\right){\sum_{i=1}^n x_i} - \frac{n}{1-\pi}\\
0  &= \left(\frac{1-\pi + \pi}{\pi(1-\pi)}\right){\sum_{i=1}^n x_i} - \frac{n}{1-\pi}\\
0  &= \frac{1}{\pi(1-\pi)}{\sum_{i=1}^n x_i} - \frac{n}{1-\pi}\\
\frac{n}{1-\pi}  &= \frac{1}{\pi(1-\pi)}{\sum_{i=1}^n x_i}\\
\frac{n\pi(1-\pi)}{1-\pi}  &= {\sum_{i=1}^n x_i}\\
n\pi  &= {\sum_{i=1}^n x_i}\\
\hat{\pi}_{\text{MLE}}  &= \frac{1}{n}{\sum_{i=1}^n x_i} = \bar{x}\\
\end{aligned}
$$

To ensure we have found a value that maximizes the likelihood, we check that the second derivative of the log-likelihood is negative. First we calculate the second derivative

$$
\begin{aligned}
\ell''(\pi) &= \frac{\partial^2}{\partial\pi^2}\ell(\pi) = \frac{\partial}{\partial\pi}\ell'(\pi)\\
&= \frac{-1}{\pi^2}\left(\frac{\partial}{\partial\pi}\pi\right){\sum_{i=1}^n x_i} + \frac{1}{(1-\pi)^2}\left(\frac{\partial}{\partial\pi}(1-\pi)\right)\left(n-\sum_{i=1}^n x_i\right)\\
&= \frac{-1}{\pi^2}{\sum_{i=1}^n x_i} - \frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n x_i\right)\\
\end{aligned}
$$
Then check for negativity of the second derivative

$$
\begin{aligned}
0 &\geq \frac{-1}{\pi^2}{\sum_{i=1}^n x_i} - \frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n x_i\right)\\
0 &\leq \frac{1}{\pi^2}{\sum_{i=1}^n x_i} + \frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n x_i\right)\\
\end{aligned}
$$
Looking at this expression element-wise:
\begin{itemize}
\item $\pi \in (0,1)$, so $\frac{1}{\pi^2}$ and $\frac{1}{(1-\pi)^2}$ are both positive, specifically in $(1,\infty)$
\item $x_i \in \{0,1\}$ for all $i$, so $0\leq\sum_{i=1}^n x_i \leq n$. Therefore both $\sum_{i=1}^nx_i$ and $n-\sum_{i=1}^nx_i$ are non-negative
\end{itemize}

Based on the above, we can conclude that the second derivative of the log-likelihood is negative. Thus the estimator $\hat{\pi}_{MLE} = \bar{x}$ is, in fact, the Maximum Likelihood Estimator of $\pi$.

## Compute the maximum likelihood estimate of $\pi$ from your simulated data.


\  

```{r}
pi_MLE = x_mean = mean(x)
```

We can calculate the MLE of $\pi$ either by calculating the mean of the data $\bar{x} = `r x_mean`$ or by optimizing the likelihood function over $\pi\in(0,1)$ to get $\text{argmax}\mathcal{L}(\pi) = `r pi_MLE`$. These answers are equivalent because the likelihood was calculated using our generated data.

## Find the asymptotic standard error of the maximum likelihood estimator $\hat{\pi}_{ML}$ using calculus.

\  

The asymptotic distribution of the MLE is given by $\hat{\pi}_{\text{ML}} \sim \mathcal{N}\left(\pi, \mathcal{I}^{-1}(\pi)\right)$ where $\mathcal{I}(\pi) = -\text{E}\left[\frac{\partial^2}{\partial\pi^2}\ell_{\mathbb{X}}(\pi)\right]$ is the Fisher Information. We know the second derivative of the log-likelihood from a previous question.
$$
\begin{aligned}
\mathcal{I}(\pi) &= -\text{E}\left[\frac{\partial^2}{\partial\pi^2}\ell_{\mathbb{X}}(\pi)\right]\\
&= -\text{E}\left[\frac{-1}{\pi^2}{\sum_{i=1}^n x_i} - \frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n x_i\right)\right]\\
&= \text{E}\left[\frac{1}{\pi^2}{\sum_{i=1}^n x_i} + \frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n x_i\right)\right]\\
&= \text{E}\left[\frac{1}{\pi^2}{\sum_{i=1}^n x_i}\right] + \text{E}\left[\frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n x_i\right)\right]\\
&= \frac{1}{\pi^2}\text{E}\left[{\sum_{i=1}^n x_i}\right] + \frac{1}{(1-\pi)^2}\text{E}\left[n-\sum_{i=1}^n x_i\right]\\
&= \frac{1}{\pi^2}\sum_{i=1}^n \text{E}[x_i] + \frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n \text{E}[x_i]\right)\\
&= \frac{1}{\pi^2}\sum_{i=1}^n \pi + \frac{1}{(1-\pi)^2}\left(n-\sum_{i=1}^n \pi\right)\\
&= \frac{1}{\pi^2}n\pi + \frac{1}{(1-\pi)^2}(n-n \pi)\\
&= \frac{1}{\pi^2}n\pi + \frac{1}{(1-\pi)^2}n(1 - \pi)\\
&= n\left(\frac{1}{\pi^2}\pi + \frac{1}{(1-\pi)^2}(1 - \pi)\right)\\
&= n\left(\frac{1}{\pi} + \frac{1}{1-\pi}\right)\\
&= n\left(\frac{1-\pi}{\pi(1-\pi)} + \frac{\pi}{\pi(1-\pi)}\right)\\
&= n\left(\frac{1-\pi+\pi}{\pi(1-\pi)}\right)\\
&= \frac{n}{\pi(1-\pi)}\\
\end{aligned}
$$
Now, with the Fisher Information, we know the variance of the MLE is $\frac{1}{\mathcal{I}(\pi)} = \frac{\pi(1-\pi)}{n}$, thus the standard error of the MLE is $\sqrt{\frac{\pi(1-\pi)}{n}}$.

## Plot the asymptotic standard error as a function of sample size for sample sizes of 10 - 10,000 (on a log scale).

\ 

```{r, fig.height = 2}
std_err = function(n, pi = 0.7) sqrt(pi * (1-pi) / n)

std_err_plot = ggplot() + 
  geom_function(fun = std_err) + 
  scale_x_continuous(trans = "log10", limits = c(10, 10^4)) +
  ylab("Standard Error of MLE") +
  xlab('Sample size ("n")') +
  theme_bw()
print(std_err_plot)

```

Here we continue to assume a parameter $\pi = 0.7$ like our simulated data. We can see that the standard error decreases as the sample size gets larger.

### What sample size do you need to achieve a standard error of 1 percentage point?

\ 

```{r}
sample_size = function(se, pi = 0.7) (pi * (1-pi)) / se^2

se1 = sample_size(0.01)
```

For a population with parameter $\pi = 0.7$, the sample size needed to achieve a standard error of 1 percentage point is $`r se1`$.

### What sample size do you need to achieve a standard error of 0.1 percentage point?

\ 

```{r}
se0.1 = sample_size(0.001)
```
The sample size needed to achieve a standard error of 1 percentage point is $`r se0.1`$.

### Estimate the standard error from your simulated data using the asymptotic formula.

```{r}
sample_se = std_err(n=n,pi=x_mean) |> print()
```

### Compare the estimated standard error with the theoretical standard error.

```{r}

theoretical_se = std_err(n=n,pi=0.7)
```

The standard error from the simulated data is $\sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{n}} = `r sample_se`$ and the theoretical standard error for a sample size of 100 is $\sqrt{\frac{\pi(1-\pi)}{n}} = `r theoretical_se`$. The two standard errors are very close. The difference between these two estimators is because of the value of $\pi$ used in the calculation, and the theoretical standard error is smaller because the population value of $\pi$ is farther from the argmax of $0.5$.


## Perform inference

Compute an asymptotic 95% confidence interval for $\pi$, and calculate an asymptotic p-value for the null hypothesis $H_0: \pi = 0.5$. Interpret both results in scientific terms.

\ 

```{r}

lcl = x_mean - qnorm(.975) * sample_se
ucl = x_mean + qnorm(.975) * sample_se

test_stat = (x_mean - 0.5) / sqrt((0.5 * 0.5) / n)
p_val = 2*pnorm(abs(test_stat), lower.tail = F)

```

An asymptotic 95% confidence interval for $pi$ is $\hat{\pi}_{\text{ML}} \pm 1.96 \sqrt{\frac{\hat{\pi}_{\text{ML}}(1-\hat{\pi}_{\text{ML}})}{n}}$ which results in the interval $[`r lcl`, `r ucl`]$. Examples of appropriate interpretations of the interval are:
\begin{itemize}
\item We are 95\% confident that the true population parameter $\pi$ lies in the interval $[`r lcl`, `r ucl`]$.
\item In 95\% of repeated sampling, the true population parameter $\pi$ would be captured by an interval calculated like $[`r lcl`, `r ucl`]$.
\end{itemize}

A p-value for the null hypothesis $H_0: \pi = 0.5$ with a two-sided alternative is $`r p_val`$ which indicates that our data is unlikely under the assumption of the null hypothesis. Because the p-value is smaller than $\alpha = 0.05$, we reject the null hypothesis and conclude that the parameter $\pi$ is not equal to $0.5$.

```{r}
# optional: check against a packaged R function:

prop.test(sum(x), length(x), correct = FALSE)$p.value
library(binom)
binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")
# note that prop.test() uses Wilson's method for confidence intervals, which is more complicated
# details are in ?binom::binom.confint
```

##

Find the set of binomial outcomes (values of $\sum_{i=1}^n X_i$) for which you would reject the null hypothesis.

Compute the probability of rejecting the null hypothesis (power), if the data-generating value of $\pi$ equals your estimate.

\ 

For the first part, assuming a two-sided alternative hypothesis (i.e. $H_A: \pi \neq 0.5$) and a pre-specified level of significance $\alpha = 0.05$, we will reject $H_0$ for any set of binomial outcomes represented by $\sum_{i=1}^n X_i$ outside the range $[a,b]$ where $P(a\leq \sum_{i=1}^n X_i \leq b) = 0.95$ and $P(\sum_{i=1}^n X_i \leq a) = P(\sum_{i=1}^n X_i \geq b) = 0.025$

Using the normal approximation to the binomial, 

$$
\begin{aligned}
P(\sum_{i=1}^n X_i \leq a) = P(\sum_{i=1}^n X_i \geq b) &= 0.025\\
P\left(\frac{\frac{1}{n}\sum_{i=1}^n X_i - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}} \leq \frac{\frac{1}{n}a - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\right) = P\left(\frac{\frac{1}{n}\sum_{i=1}^n X_i - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}} \geq \frac{\frac{1}{n}b - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\right) &= 0.025\\
P\left(Z \leq \frac{\frac{1}{n}a - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\right) = P\left(Z \geq \frac{\frac{1}{n}b - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\right) &= 0.025\\
P\left(Z \leq \frac{\frac{1}{n}a - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\right) = 1- P\left(Z \leq \frac{\frac{1}{n}b - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\right) &= 0.025\\
\end{aligned}
$$

The lower bound of this interval is $a = \left(\Phi^{-1}(0.025)\sqrt{\frac{0.5*0.5}{100}}+0.5\right)\times100 = `r (qnorm(0.025)*sqrt(0.5*0.5/100)+0.5)*100`$, and the upper bound is $a = \left(\Phi^{-1}(0.975)\sqrt{\frac{0.5*0.5}{100}}+0.5\right)\times100 = `r (qnorm(0.975)*sqrt(0.5*0.5/100)+0.5)*100`$. We will reject for any values of $\sum_{i=1}^n X_i$ less than or equal to 40 or greater than or equal to 60.

We can double-check this using the `stats::prop.test()` function:

```{r}

prop.test(40, 100, correct = FALSE)

prop.test(60, 100, correct = FALSE)

```

For the second part, we calculate the power under the condition that the true parameter is $\pi = `r x_mean`$ (i.e. the simple alternative hypothesis $H_A: \pi = `r x_mean`$ )

Let $Y = \sum_{i=1}^n X_i$ be a binomial random variable $Y\sim B(n=100,\pi=`r x_mean`)$

$$
\begin{aligned}
\text{Power} &= 1- \beta\\
&= 1 - P(40 \leq Y \leq 60|\pi=0.68)\\
&= 1 - \left(P(Y \leq 60|\pi=0.68) - P(Y \leq 40|\pi=0.68)\right)\\
&= 1 - \left( \sum_{y = 0}^{60} {100 \choose y}0.68^y(1-0.68)^{100-y} - \sum_{y = 0}^{40} {100 \choose y}0.68^y(1-0.68)^{100-y}\right)\\
&= 1 - \left( `r pbinom(60,100,0.68)` - `r pbinom(40,100,0.68)` \right)\\
&= 1 - \left( `r pbinom(60,100,0.68) - pbinom(40,100,0.68)` \right)\\
&= `r 1 -  (pbinom(60,100,0.68) - pbinom(40,100,0.68))` \\
\end{aligned}
$$

## Analyze the statistical power of your test

Graph the power to reject the null hypothesis as a function of sample size, using your sample estimate as the data-generating value. What sample size would you need to achieve 80% power? 90%? 95%? 99%?

```{r}
power = function(n = 100, null = 0.5, alt = 0.68)
{
  n = floor(n) # there's no such thing as fractional sample size
  se_null = sqrt(null*null/n)
  reject_upper = ceiling(n * (null + qnorm(0.975) * se_null))
  reject_lower = floor(n * (null - qnorm(0.975) * se_null))
  p_reject_high = 
    pbinom(
      q = reject_lower, 
      size = n, 
      prob = alt)
  p_reject_low = 
    pbinom(
      q = reject_upper - 1, # see documentation for pbinom(lower = FALSE,...)
      size = n, 
      prob = alt, 
      lower = FALSE)
  
  p_reject = p_reject_high + p_reject_low
  
  return(p_reject)
  
}

power_plot = 
  ggplot() + 
  geom_function(fun = power,n=200) + 
  xlim(c(1,200)) + 
  ylim(0,1) +
  ylab("Power") +
  xlab("n") +
  theme_bw()

print(power_plot)

```

```{r}

sample_sizes = 1:200
# calculate power for a range of values, 
# which the graph above tells us will be sufficient to search among
pwrs = power(n = sample_sizes)

pwrs.want = c(0.800, 0.900, 0.950, 0.990)

size.needed = numeric(4)

for(i in 1:length(pwrs.want))
{
  target_power_achieved = pwrs >= pwrs.want[i]
  size.needed[i] = sample_sizes[min(whic\haz(target_power_achieved))]
}

# optional:
tibble(
  desired_power = pwrs.want,
  size.needed = size.needed,
  achieved_power = power(size.needed) 
) |> 
  pander()

```

To achieve 80% power, the sample size must be at least $`r size.needed[1]`$. To achieve 90% power, the sample size must be at least $`r size.needed[2]`$. To achieve 95% power, the sample size must be at least $`r size.needed[3]`$. To achieve 80% power, the sample size must be at least $`r size.needed[4]`$.


## Repeat the simulation 1000 times.

Each time, record your estimate and standard error, whether you would reject the null hypothesis at the $\alpha = 0.05$ level, and whether the confidence interval included the true value $\pi = 0.5$.

::: {.callout-tip}

Use a `for` loop:

```r

estimates = numeric(1000)
# create some more vectors to store results here
for (i in 1:1000)
{
# [generate data here]
# [analyze data here]
estimate[i] = est
# save some more results here
}
```
:::


```{r}


binom.MLE.sim = function(pi = 0.7, null = 0.5, n = 100, sim = 1000, alpha = 0.05)
{
  
  # create some more vectors to store results here
  estimates = numeric(sim)
  errors = numeric(sim)
  reject_null = numeric(sim)
  CI_true_val = numeric(sim)
  
  
  for (i in 1:sim)
  {
    # [generate data here]
    x.i = rbinom(n,1,prob = pi)
    # [analyze data here]
    est = mean(x.i)
    se = sqrt(est*(1-est)/n)
    se_null = sqrt(null*(1-null)/n)
    ci_radius = se * qnorm(.975)
    ci = est + c(-1,1) * ci_radius
    z_stat = abs(est-null)/se_null
    pval = pnorm(abs(z_stat), lower = FALSE)*2
    
    # save some more results here
    estimates[i] = est
    errors[i] = se
    reject_null[i] = pval < alpha
    CI_true_val[i] = between(pi, ci[1], ci[2])
    
  }
  
  true_se = std_err(n = n, pi = pi) # using what we built previously
  
  
  simulations = tibble(
    estimates,
    errors,
    reject_null,
    CI_true_val)
  
  res = list(
    
    #return all parameters used  
    p = pi,
    null = null,
    n = n,
    sim = sim,
    alpha = alpha,
    
    #returning the data, MLE, and estimated standard errors for each simulation 
    sim_data = simulations,
    estimates = estimates, 
    errors = errors,
    
    #true_se is calculated with the data-generating parameter
    true_se = true_se,
    
    # returning asymptotic results;
    # these could alternatively be calculated in a postprocessing function
    MLE_bias = mean(estimates) - pi,
    MLE_se = sd(estimates),
    SE_bias = mean(errors) - true_se,
    SE_se = sd(errors),
    p_CI = mean(CI_true_val),
    p_reject = mean(reject_null)
  )
  
  return(res)
}

```

```{r}
set.seed(1)
sim100 = binom.MLE.sim(pi = 0.7, null = 0.5, n = 100, sim = 1000, alpha = 0.05)

#view results from first 10 simulations
#tibble(sim1000$sim_data)
```

## Analyze simulation results

Using your 1000 simulations, estimate:

- the bias of the MLE
- the empirical variance of the MLE
- the empirical standard error of the MLE
- the mean squared error of the MLE
- the mean absolute error of the MLE
- the bias of the estimated standard error
- the empirical standard error of the estimated standard error
- the coverage probability of the confidence intervals
- the power of your hypothesis test to reject the null hypothesis $H_0: \pi = 0.5$

\ 

- the bias of the MLE $`r sim100[["MLE_bias"]]`$
- the standard error of the MLE $`r sim100[["MLE_se"]]`$
- the bias of the estimated standard error $`r sim100[["SE_bias"]]`$
- the empirical standard error of the estimated standard error $`r sim100[["SE_se"]]`$
- the coverage probability of the confidence intervals $`r sim100[["p_CI"]]`$
- the power of your hypothesis test $`r sim100[["p_reject"]]`$


## Visualize simulation results

Create histograms and/or boxplots of the MLEs and estimated standard errors, with lines indicating the theoretical values.

\  
```{r}

ggplot(sim100$sim_data, aes(x=estimates)) + 
  geom_histogram() +
  labs(
    title = "Simulated MLE",
    subtitle = paste(
      "p = ", sim100$p, 
      ", n = ", sim100$n, 
      ", simulations = ", sim100$sim, 
      sep = "")
  ) + 
  xlab("MLE") + #label on the x-axis
  ylab("Count") + #label on the y-axis
  geom_vline(xintercept = sim100$p,color = "red") + 
  # plotting a vertical line where the correct parameter should be
  theme_bw()

```

```{r}
true_se = sqrt(pi*(1-pi)/n)

SE.plot = ggplot(sim100$sim_data, aes(x = errors)) + 
  geom_histogram() +
  labs(
    title = "Simulated Standard Error",
    subtitle = paste(
      "p = ", sim100$p, 
      ", n = ", sim100$n, 
      ", simulations = ", sim100$sim, 
      sep = "")
  ) + 
  xlab("Standard Errors") + #label on the x-axis
  ylab("Count") + #label on the y-axis
  geom_vline(xintercept = sim100$true_se,color = "red") + 
  # plotting a vertical line where the correct parameter should be
  theme_bw()

print(SE.plot)
```


## Summarize simulation results

Summarize the performance of your analyses, comparing empirical and theoretical (asymptotic) results.

\ 
```{r}

summarize_sim = 
  function(sim_results)
  {
    tribble(
      ~characteristic, ~empirical, ~theoretical,
      
      "bias of MLE", 
      sim_results[["MLE_bias"]], 
      0,
      
      "standard error of MLE", 
      sim_results[["MLE_se"]], 
      sim_results[["true_se"]],
      
      "bias of estimated standard error", 
      sim_results[["SE_bias"]], 
      0,
      
      "CI coverage", 
      sim_results[["p_CI"]], 
      .95,
      
      "power", 
      sim_results[["p_reject"]], 
      power(
        n = sim_results$n, 
        null = sim_results$null, 
        alt = sim_results$p)
    )
  }

sim100 |> summarize_sim() |> pander()


```

Overall, the simulated results for mean and standard error were very close to the expected theoretical values with bias on the order of $10^{-4}$ which is very small. This suggests asymptotic consistency of the MLE.

## Large-sample properties

Repeat the simulation with a simulated sample sizes of $10^3$ and $10^5$ binary outcomes, and summarize the results.

Compare the empirical results with the theoretical results above.

::: {.callout-tip}

Instead of copy-pasting code, write a function that takes the data-generating $\pi$, the null hypothesis $\pi_0$, the sample size, and the number of simulations to run as inputs, and returns the performance characteristics above.

:::

\  
```{r}

sim10.3 = binom.MLE.sim(pi = 0.7, null = 0.5, n = 10^3, sim = 1000, alpha = 0.05)
sim10.3 |> summarize_sim() |> pander()
```

```{r}
sim10.5 = binom.MLE.sim(pi = 0.7, null = 0.5, n = 10^5, sim = 1000, alpha = 0.05)

sim10.5 |> summarize_sim() |> pander()
```

With more repeated samples (more simulations), the empirical estimates of the MLE approach the theoretical value of 0.7, and the empirical estimates of the standard error approach 0. The observed power of the test of ($\text{H}_0: \pi_0 = 0.5$) increases as sample size increases, and the frequency of capturing the true value in the confidence interval decreases as sample size increases. In this set of simulations, our data is generated under the alternative hypothesis, so our ability to correctly reject the null hypothesis (power) increases as we collect more data points.

##

Repeat the simulation at all three sample sizes for the scenario where the data-generating $\pi = 0.5$ matches the null hypothesis. Empirically assess the false positive rate of the hypothesis test.

\  
```{r}

sim100_05 = binom.MLE.sim(pi = 0.5, null = 0.5, n = 100, sim = 1000, alpha = 0.05)
sim100_05 |> summarize_sim() |> pander()
```

```{r}
sim10.3_05 = binom.MLE.sim(pi = 0.5, null = 0.5, n = 10^3, sim = 1000, alpha = 0.05)
sim10.3_05 |> summarize_sim() |> pander()
```

```{r}
sim10.5_05 = binom.MLE.sim(pi = 0.5, null = 0.5, n = 10^5, sim = 1000, alpha = 0.05)
sim10.5_05 |> summarize_sim() |> pander()
```

The results of the simulations where the data-generating parameter is the hypothesized parameter give similar results as seen previously. Larger sample sizes yield empirical values for the MLE that are closer to the theoretical value, and the standard errors approach zero with larger sample sizes. We also see that the true value of the parameter is captured in approximately 95% of the intervals and the null hypothesis is falsely rejected approximately 5% of the time. In this set of simulations, our data is generated under the null hypothesis, so our ability to incorrectly reject the null hypothesis ($\alpha$/Type I Error) approaches our chosen 5% error rate as we collect more data points.
