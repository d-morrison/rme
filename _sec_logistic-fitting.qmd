{{< include macros.qmd >}}

### Model

Assume:

- $Y_i|\vX_i \simind \Ber(\pi(X_i))$
- $\pi(\vx) = \expitf{\eta(\vx)}$
- $\eta(\vx) = \vx \cdot \vb$

### Likelihood function

:::{#exr-beetles-likelihood}
Compute and graph the likelihood for the `beetles` data model:

:::{#tbl-beetles-data-recap}

```{r}
#| label: load-beetles-data
library(glmx)
library(dplyr)
data(BeetleMortality)
beetles <- BeetleMortality |>
  mutate(
    pct = died / n,
    survived = n - died,
    dose_c = dose - mean(dose)
  )
beetles_long <-
  beetles |>
  reframe(
    .by = everything(),
    outcome = c(
      rep(1, times = died),
      rep(0, times = survived)
    )
  )
beetles
```

Mortality rates of adult flour beetles after five hoursâ€™ exposure to gaseous carbon disulphide [@bliss1935beetles]

:::

:::

---

:::{#sol-beetles-likelihood}

:::{#fig-beetles-likelihood}

```{r}
#| label: beetles-likelihood-plot
odds_inv <- function(omega) (1 + omega^-1)^-1
lik_beetles0 <- function(beta_0, beta_1) {
  beetles |>
    mutate(
      eta = beta_0 + beta_1 * dose,
      omega = exp(eta),
      pi = odds_inv(omega),
      Lik = (pi^died) * (1 - pi)^(survived)
    ) |>
    pull(Lik) |>
    prod()
}

lik_beetles <- Vectorize(lik_beetles0)

beetles_glm <-
  beetles |>
  glm(
    formula = cbind(died, survived) ~ dose,
    family = "binomial"
  )
ranges <- confint.default(beetles_glm)

n_points <- 100
beta_0s <- seq(ranges["(Intercept)", "2.5 %"],
  ranges["(Intercept)", "97.5 %"],
  length.out = n_points
)
beta_1s <- seq(ranges["dose", "2.5 %"],
  ranges["dose", "97.5 %"],
  length.out = n_points
)
names(beta_0s) <- round(beta_0s, 2)
names(beta_1s) <- round(beta_1s, 2)
lik_mat_beetles <- outer(beta_0s, beta_1s, lik_beetles)
graph_3d <- function(x, y, z) {
  plotly::plot_ly(
    type = "surface",
    x = ~x,
    y = ~y,
    z = ~ t(z)
  ) |> 
  # see https://stackoverflow.com/questions/69472185/correct-use-of-coordinates-to-plot-surface-data-with-plotly 
  # for explanation of why transpose `t()` is needed
    plotly::layout(
      scene = list(
        xaxis = list(nticks = 20),
        zaxis = list(nticks = 10),
        # camera = list(eye = list(x = 0, y = -1, z = 0.5)),
        aspectratio = list(x = .9, y = .8, z = 0.8)
      )
    )
}

graph_3d(beta_0s, beta_1s, lik_mat_beetles) |> print()
```

Likelihood of `beetles` data

:::

:::

---

### Log-likelihood function

$$
\ba
\ell(\vb, \vy)
   &= \logf{\Lik(\vb, \vec y) }
\\ &= \sumin \red{\ell_i}(\pi(\vx_i))
\ea
$$ {#eq-loglik-bernoulli-iid}

---

$$
\ba
\red{\ell_i}(\pi_i)
   &= y_i \logf{\pi_i} + (1 - y_i) \logf{1-\pi_i}
\\ &= y_i \logf{\pi_i} + (1 \cd \logf{1-\pi_i} - y_i \cd \logf{1-\pi_i})
\\ &= y_i \logf{\pi_i} + (\logf{1-\pi_i} - y_i \logf{1-\pi_i})
\\ &= y_i \logf{\pi_i} + \logf{1-\pi_i} - y_i \logf{\blue{1-\pi_i}}
\\ &= y_i \logf{\pi_i} - y_i \logf{\blue{1-\pi_i}} + \logf{1-\pi_i}
\\ &= (y_i \logf{\pi_i} - y_i \logf{\blue{1-\pi_i}}) + \logf{1-\pi_i}
\\ &= y_i (\logf{\red{\pi_i}} - \logf{\blue{1-\pi_i}}) + \logf{1-\pi_i}
\\ &= y_i \paren{\logf{\frac{\red{\pi_i}}{\blue{1-\pi_i}}}} + \logf{1-\pi_i}
\\ &= y_i (\logit(\pi_i)) + \logf{1-\pi_i}
\\ &= y_i (\eta_i) + \logf{1-\pi_i}
\\ &= y_i (\vx_i \cdot \vb) + \logf{1-\pi_i}
\ea
$$


---

:::{#exr-beetles-loglik}
Compute and graph the log-likelihood for the `beetles` data.
:::

---

:::{#sol-beetles-loglik}

:::{#fig-beetles-llik}
```{r}
#| label: beetles-llik-fig1
odds_inv <- function(omega) (1 + omega^-1)^-1
llik_beetles0 <- function(beta_0, beta_1) {
  beetles |>
    mutate(
      eta = beta_0 + beta_1 * dose,
      omega = exp(eta),
      pi = odds_inv(omega), # need for next line:
      llik = died * eta + log(1 - pi)
    ) |>
    pull(llik) |>
    sum()
}

llik_beetles <- Vectorize(llik_beetles0)

# to check that we implemented it correctly:
# ests = coef(beetles_glm_ungrouped)
# logLik(beetles_glm_ungrouped)
# llik_beetles(ests[1], ests[2])

llik_mat_beetles <- outer(beta_0s, beta_1s, llik_beetles)
graph_3d(beta_0s, beta_1s, llik_mat_beetles)
```

log-likelihood of `beetles` data

:::

:::

---

Let's center dose:

```{r}
#| label: beetles-center-dose
beetles_glm_grouped_centered <- beetles |>
  glm(
    formula = cbind(died, survived) ~ dose_c,
    family = "binomial"
  )

beetles_glm_ungrouped_centered <- beetles_long |>
  mutate(died = outcome) |>
  glm(
    formula = died ~ dose_c,
    family = "binomial"
  )

equatiomatic::extract_eq(beetles_glm_ungrouped_centered)
```

---

:::{#fig-beetles-lik-centered}
```{r}
#| label: beetles-lik-centered-fig
odds_inv <- function(omega) (1 + omega^-1)^-1
lik_beetles0 <- function(beta_0, beta_1) {
  beetles |>
    mutate(
      eta = beta_0 + beta_1 * dose_c,
      omega = exp(eta),
      pi = odds_inv(omega),
      Lik = (pi^died) * (1 - pi)^(survived)
    ) |>
    pull(Lik) |>
    prod()
}

lik_beetles <- Vectorize(lik_beetles0)

ranges <- confint.default(beetles_glm_grouped_centered)

n_points <- 100
beta_0s <- seq(ranges["(Intercept)", "2.5 %"],
  ranges["(Intercept)", "97.5 %"],
  length.out = n_points
)
beta_1s <- seq(ranges["dose_c", "2.5 %"],
  ranges["dose_c", "97.5 %"],
  length.out = n_points
)
names(beta_0s) <- round(beta_0s, 2)
names(beta_1s) <- round(beta_1s, 2)
lik_mat_beetles <- outer(beta_0s, beta_1s, lik_beetles)
graph_3d(beta_0s, beta_1s, lik_mat_beetles)
```

Likelihood of `beetles` data (centered model)

:::

---


:::{#fig-beetles-llik-centered}
```{r}
#| label: beetles-llik-centered-fig
odds_inv <- function(omega) (1 + omega^-1)^-1
llik_beetles0 <- function(beta_0, beta_1) {
  beetles |>
    mutate(
      eta = beta_0 + beta_1 * dose_c,
      omega = exp(eta),
      pi = odds_inv(omega),
      llik = died * eta + log(1 - pi)
    ) |>
    pull(llik) |>
    sum()
}

llik_beetles <- Vectorize(llik_beetles0)
llik_mat_beetles <- outer(beta_0s, beta_1s, llik_beetles)
graph_3d(beta_0s, beta_1s, llik_mat_beetles)
```

log-likelihood of `beetles` data (centered model)

:::

### Score function

$$
\ba
\ell'(\vb)
   &\eqdef \deriv{\vb} \ell(\vb)
\\ &=      \deriv{\vb} \sumin \ell_i(\vb)
\\ &=      \sumin \deriv{\vb} \ell_i(\vb)
\\ &=      \sumin \ell'_i(\vb)
\ea
$$

---

$$
\ba
\ell_i'(\vb)
   &= \deriv{\vb} y_i \eta_i + \logf{1-\pi_i}
\\ &= \deriv{\vb}\cb{y_i \eta_i + \logf{1-\pi_i}}
\\ &= \cb{\deriv{\vb}y_i\eta_i + \deriv{\vb}\logf{1-\pi_i}}
\\ &= \cb{\paren{\deriv{\vb}\eta_i}y_i + \deriv{\vb}\logf{1-\expit(\vx_i'\vb)}}
\\ &= \cb{\paren{\deriv{\vb}\vx_i \cdot \vb}y_i + \deriv{\vb}\logf{\inv{1+\exp{\vx_i'\vb}}}}
\\ &= \cb{\vx_i y_i - \blue{\deriv{\vb}\logf{1+\exp{\vx_i'\vb}}}}
\ea
$$

---

Now we need to apply the [chain rule](math-prereqs.qmd#thm-chain-rule):

$$
\blue{\deriv{\beta}\logf{1+\exp{\vx_i'\beta}}} =
\red{\deriv{\beta}\cb{1+\exp{\vx_i'\beta}}} \frac{1}{1+\exp{\vx_i'\beta}} 
$$

$$
\ba
\red{\deriv{\beta}\cb{1+\exp{\vx_i'\beta}}}
&= \deriv{\beta}\exp{\vx_i'\beta}
\\ &= \paren{\deriv{\beta}\vx_i'\beta} \exp{\vx_i'\beta} 
\\ &= \vx_i \exp{\vx_i'\beta}
\\ &= \red{\vx_i \omega_i}
\ea
$${#eq-deriv-expit}

So:

$$
\ba
\blue{\deriv{\beta}\logf{1+\exp{\vx_i'\beta}}}
   &= \red{\vx_i \omega_i} \frac{1}{1+\exp{\vx_i'\beta}} 
\\ &= \frac{\red{\vx_i \omega_i}}{1+\exp{\vx_i'\beta}}  \vx_i
\\ &= \vx_i \expitf{\vx_i'\beta}
\\ &= \blue{\vx_i \pi_i}
\ea
$$

---

So:

$$
\ba
\llik_i'(\vb)
&= \vx_i y_i - \blue{\vx_i \pi_i}
\\ &= \vx_i (y_i - \pi_i)
\\ &= \vx_i (y_i - \mu_i)
\\ &= \vx_i (y_i - \Expp[Y_i|\vX_i=\vx_i])
\\ &= \vx_i \ \err(y_i|\vX_i=\vx_i)
\ea
$$

::: notes

This last expression is essentially the same as we found in [linear regression](Linear-models-overview.qmd#eq-scorefun-linreg).
:::

---

Putting the pieces of $\llik'(\vb)$ back together, we have:

$$
\ba
\llik'(\vb) &= \sumin \cb{\vx_i(y_i - \expitf{\vx_i'\beta}) }
\\
&= \sumin \vx_i(y_i - \pi_i)
\\
&= \sumin \vx_i\eps_i
\ea
$${#eq-logit-score}

---

:::{#exr-beetles-score}

Implement and graph the score function for the beetles data

:::

---

:::{#sol-beetles-score}

:::{#fig-beetles-score-centered}

```{r}
#| label: score-beetles-centered
odds_inv <- function(omega) (1 + omega^-1)^-1
score_beetles0 <- function(beta_0, beta_1) {
  beetles |>
    mutate(
      eta = beta_0 + beta_1 * dose_c,
      omega = exp(eta),
      pi = odds_inv(omega),
      llik = died * log(pi) + survived * log(1 - pi),
      mu = pi * n,
      epsilon = died - mu,
      score = dose_c * epsilon
    ) |>
    pull(score) |>
    sum()
}
score_beetles <- Vectorize(score_beetles0)
score_mat_beetles <- outer(beta_0s, beta_1s, score_beetles)
graph_3d(beta_0s, beta_1s, score_mat_beetles)
```

score function of `beetles` data (centered model) for slope term $\beta_1$.

:::

:::

### Hessian function

$$
\llik''(\vb) = \sumin \llik_i''(\vb)
$${#eq-logit-hessian}

$$
\ba
\llik''_i(\vb) &= \deriv{\vb\'}\llik_i''
\\
&= \deriv{\vb\'}\vx_i\eps_i
\\
&= \vx_i \deriv{\vb\'}\eps_i
\ea
$${#eq-logit-hessian-i}

---

$$
\ba
\deriv{\vb\'}\eps_i &= \deriv{\vb\'}(y_i - \mu_i)
\\
&= -\deriv{\vb\'}\mu_i
\ea
$$
$$
\ba
\deriv{\vb\'}\mu_i &= \derivf{\eta_i}{\vb\'} \derivf{\mu_i}{\eta_i}
\ea
$$

$$\derivf{\eta_i}{\vb\'} = \deriv{\vb\'}{\vx_i \cdot \vb} = \vx_i$$

$$
\ba
\derivf{\mu_i}{\eta_i} &= \derivf{\omega_i}{\eta_i} \derivf{\mu_i}{\omega_i}
\ea
$$

$$
\ba
\derivf{\omega_i}{\eta_i} &= \deriv{\eta_i} \expf{\eta_i}
\\ &= \expf{\eta_i}
\\ &= \omega_i
\ea
$$

$$\derivf{\mu_i}{\omega_i} = \frac{1}{(1+\omega_i)^2} = (1-\pi_i)^2$$

---

Therefore:

$$
\ba
\derivf{\mu_i}{\eta_i} &= (1-\pi_i)^2 \omega_i
\\ &= (1-\pi_i)^2 \frac{\pi_i}{1-\pi_i}
\\ &= \pi_i(1-\pi_i)
\\ &= \Varf{Y_i|X_i=x_i}
\ea
$$

$$
\ba
\llik''(\vb) &= -\sumin \vx_i \vx_i' \Varf{Y_i|X_i=x_i}
\\ &= - \mX\'\matr{D}\mX
\ea
$${#eq-logistic-hess}

where $\matr{D} \eqdef \text{diag}(\Varf{Y_i|X_i=x_i})$
is the diagonal matrix whose $i^{th}$ diagonal element is $\Varf{Y_i|X_i=x_i}$.

Compare with linear regression, where:

$$
\ba
\llik''(\vb) &= -\sumin \vx_i \vx_i' (\sigma^2)^{-1}
\\ &= -\mX\' \matr{D}^{-1} \mX
\ea
$${#eq-linear-hess}

---

Setting $\ell'(\vb; \vy) = 0$ gives us:


$$\sumin \cb{\vx_i(y_i - \expitf{\vx_i'\beta}) } = 0$$ {#eq-score-logistic}


---

::: notes

In general, the estimating equation $\ell'(\vb; \vy) = 0$ 
cannot be solved analytically.

Instead, we can use the [Newton-Raphson method](intro-MLEs.qmd#sec-newton-raphson):

:::

$$
\esttmp{\theta}
\leftarrow \esttmp{\theta} - \inv{\hessf{\vec y;\esttmp{\theta}}}
\scoref{\vec y;\esttmp{\theta}}
$$

::: notes

We make an iterative series of guesses, 
and each guess helps us make the next guess better 
(i.e., higher log-likelihood). 
You can see some information about this process like so:

:::

```{r}
#| label: out-glm-fitting-process
#| code-fold: false

beetles_glm_ungrouped <-
  beetles_long |>
  glm(
    control = glm.control(trace = TRUE),
    formula = outcome ~ dose,
    family = "binomial"
  )
```

::: notes

After each iteration of the fitting procedure, the deviance
($2(\ell_{\text{full}} - \ell(\hat\beta))$ ) is printed. 
You can see that 
the algorithm took 5 iterations to converge to a solution where
the likelihood wasn't changing much anymore.

:::

---

@tbl-beetles-glm-ungrouped and @tbl-beetles-glm-ungrouped-vcov show 
the fitted model and the covariance matrix of the estimates, respectively.

```{r}
#| tbl-cap: Fitted model for `beetles` data
#| label: tbl-beetles-glm-ungrouped
#| code-fold: show

beetles_glm_ungrouped |> summary()
```

```{r}
#| tbl-cap: Parameter estimate covariance matrix for `beetles` data
#| label: tbl-beetles-glm-ungrouped-vcov
#| code-fold: show

beetles_glm_ungrouped |> vcov()
```
