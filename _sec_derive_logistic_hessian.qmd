{{< include macros.qmd >}}

$$
\llik''(\vb) = \sumin \llik_i''(\vb)
$${#eq-logit-hessian}

$$
\ba
\llik''_i(\vb) &= \deriv{\vb\'}\llik_i''
\\
&= \deriv{\vb\'}\vx_i\eps_i
\\
&= \vx_i \deriv{\vb\'}\eps_i
\ea
$${#eq-logit-hessian-i}

---

Using @thm-d_pi_d_beta:

$$
\ba
\deriv{\vb\'}\eps_i &= \deriv{\vb\'}(y_i - \mu_i)
\\
&= \deriv{\vb\'}y_i - \deriv{\vb\'}\mu_i
\\
&= 0 - \deriv{\vb\'}\mu_i
\\
&= -\derivf{\mu_i}{\vb\'}
\\
&= -\derivf{\pi_i}{\vb\'}
\\ &= -\vx_i\' \pi_i (1-\pi_i)
\\ &= - \vx_i\' \Varf{Y_i|X_i=x_i}
\ea
$$

$$
\ba
\llik''(\vb) &= -\sumin \vx_i \vx_i' \Varf{Y_i|X_i=x_i}
\\ &= - \mX\'\matr{D}\mX
\ea
$${#eq-logistic-hess}

where $\matr{D} \eqdef \text{diag}(\Varf{Y_i|X_i=x_i})$
is the diagonal matrix whose $i^{th}$ diagonal element is $\Varf{Y_i|X_i=x_i}$.

Compare with linear regression, where:

$$
\ba
\llik''(\vb) &= -\sumin \vx_i \vx_i' (\sigma^2)^{-1}
\\ &= -\mX\' \matr{D}^{-1} \mX
\ea
$${#eq-linear-hess}
