---
subtitle: "Logistic regression and variations"
---

# Models for Binary Outcomes {#sec-Bernoulli-models}

---

{{< include shared-config.qmd >}}

```{r}
options(digits = 6)
```


### Acknowledgements {.unnumbered}

This content is adapted from:

-   @dobson4e, Chapter 7
-   @vittinghoff2e, Chapter 5
- [David Rocke](https://dmrocke.ucdavis.edu/)'s materials from the [2021 edition of Epi 204](https://dmrocke.ucdavis.edu/Class/EPI204-Spring-2021/EPI204-Spring-2021.html)
-   @NahhasIRMPHR [Chapter 6](https://www.bookdown.org/rwnahhas/RMPH/blr.html)

## Introduction

---

{{< include _sec_overview_bernoulli_models.qmd >}}

{{< include _sec_intro_bernoulli_models.qmd >}}

## Introduction to logistic regression

{{< include _sec_one_cov_logistic.qmd >}}

## Multiple logistic regression

{{< include _sec_exm_wcgs.qmd >}}

### Odds ratios in logistic regression

:::{#thm-logistic-OR}

For the logistic regression model:

- $Y_i|\vX_i \simind \Ber(\pi(\vX_i))$
- $\pi(\vx) = \expitf{\vx'\vb}$

Consider two covariate patterns, $\vx$ and $\vec{x^*}$.

The odds ratio comparing these covariate patterns is:

$$
\omega(\vx,\vec{x^*}) = \exp{(\vx-\vec{x^*})\' \vb}
$$

:::

---

::: proof
$$
\ba
\omega(\vx,\vec{x^*}) 
&= \frac {\odds(Y=1 | \vX = \vx)} {\odds(Y=1 | \vX = \vec{x^*})}
\\ &= \frac{\exp{\vx\'\vb}}{\exp{{\vec{x^*}}\' \vb}}
\\ &= \exp{\vx\'\vb - {\vec{x^*}}\' \vb}
\\ &= \exp{(\vx\' - {\vec{x^*}}\') \vb}
\\ &= \exp{{(\vx - \vec{x^*})}\' \vb}
\ea
$$
:::

## Fitting logistic regression models with maximum likelihood estimation

{{< include _sec_logistic-fitting.qmd >}}

## Model comparisons for logistic models {#sec-gof}

### Deviance test

We can compare the maximized log-likelihood of our model,
$\ell(\hat\beta; \mathbf x)$, versus the log-likelihood of the full
model (aka saturated model aka maximal model), $\ell_{\text{full}}$,
which has one parameter per covariate pattern. With enough data,
$2(\ell_{\text{full}} - \ell(\hat\beta; \mathbf x)) \dot \sim \chi^2(N - p)$,
where $N$ is the number of distinct covariate patterns and $p$ is the
number of $\beta$ parameters in our model. A significant p-value for
this **deviance** statistic indicates that there's some detectable
pattern in the data that our model isn't flexible enough to catch.

::: callout-caution
The deviance statistic needs to have a large amount of data **for each
covariate pattern** for the $\chi^2$ approximation to hold. A guideline
from Dobson is that if there are $q$ distinct covariate patterns
$x_1...,x_q$, with $n_1,...,n_q$ observations per pattern, then the
expected frequencies $n_k \cdot \pi(x_k)$ should be at least 1 for every
pattern $k\in 1:q$.
:::

If you have covariates measured on a continuous scale, you may not be
able to use the deviance tests to assess goodness of fit.

### Hosmer-Lemeshow test

If our covariate patterns produce groups that are too small, a
reasonable solution is to make bigger groups by merging some of the
covariate-pattern groups together.

Hosmer and Lemeshow (1980) proposed that we group the patterns by their
predicted probabilities according to the model of interest. For example,
you could group all of the observations with predicted probabilities of
10% or less together, then group the observations with 11%-20%
probability together, and so on; $g=10$ categories in all.

Then we can construct a statistic
$$X^2 = \sum_{c=1}^g \frac{(o_c - e_c)^2}{e_c}$$ where $o_c$ is the
number of events *observed* in group $c$, and $e_c$ is the number of
events expected in group $c$ (based on the sum of the fitted values
$\hat\pi_i$ for observations in group $c$).

If each group has enough observations in it, you can compare $X^2$ to a
$\chi^2$ distribution; by simulation, the degrees of freedom has been
found to be approximately $g-2$.

For our CHD model, this procedure would be:

```{r}
wcgs <-
  wcgs |>
  mutate(
    pred_probs_glm1 = chd_glm_contrasts |> fitted(),
    pred_prob_cats1 = pred_probs_glm1 |>
      cut(
        breaks = seq(0, 1, by = .1),
        include.lowest = TRUE
      )
  )

HL_table <- # nolint: object_name_linter
  wcgs |>
  summarize(
    .by = pred_prob_cats1,
    n = n(),
    o = sum(chd69 == "Yes"),
    e = sum(pred_probs_glm1)
  )

library(pander)
HL_table |> pander()

X2 <- HL_table |> # nolint: object_name_linter
  summarize(
    `X^2` = sum((o - e)^2 / e)
  ) |>
  pull(`X^2`)
print(X2)

pval1 <- pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)
```

Our statistic is $X^2 = `r X2`$; $p(\chi^2(1) > `r X2`) = `r pval1`$,
which is our p-value for detecting a lack of goodness of fit.

Unfortunately that grouping plan left us with 
just three categories with any observations, 
so instead of grouping by 10% increments of predicted probability, 
typically analysts use deciles of the predicted probabilities:

```{r}
wcgs <-
  wcgs |>
  mutate(
    pred_probs_glm1 = chd_glm_contrasts |> fitted(),
    pred_prob_cats1 = pred_probs_glm1 |>
      cut(
        breaks = quantile(pred_probs_glm1, seq(0, 1, by = .1)),
        include.lowest = TRUE
      )
  )

HL_table <- # nolint: object_name_linter
  wcgs |>
  summarize(
    .by = pred_prob_cats1,
    n = n(),
    o = sum(chd69 == "Yes"),
    e = sum(pred_probs_glm1)
  )

HL_table |> pander()

X2 <- HL_table |> # nolint: object_name_linter
  summarize(
    `X^2` = sum((o - e)^2 / e)
  ) |>
  pull(`X^2`)

print(X2)

pval1 <- pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)
```

Now we have more evenly split categories. The p-value is $`r pval1`$,
still not significant.

Graphically, we have compared:

```{r}
HL_plot <- # nolint: object_name_linter
  HL_table |>
  ggplot(aes(x = pred_prob_cats1)) +
  geom_line(
    aes(y = e, x = pred_prob_cats1, group = "Expected", col = "Expected")
  ) +
  geom_point(aes(y = e, size = n, col = "Expected")) +
  geom_point(aes(y = o, size = n, col = "Observed")) +
  geom_line(aes(y = o, col = "Observed", group = "Observed")) +
  scale_size(range = c(1, 4)) +
  theme_bw() +
  ylab("number of CHD events") +
  theme(axis.text.x = element_text(angle = 45))
```

::: {.content-visible when-format="html"}
```{r}
ggplotly(HL_plot)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(HL_plot)
```
:::

### Comparing models

-   AIC = $-2 * \ell(\hat\theta) + 2 * p$ \[lower is better\]
-   BIC = $-2 * \ell(\hat\theta) + p * \text{log}(n)$ \[lower is
better\]
-   likelihood ratio \[higher is better\]

## Residual-based diagnostics

{{< include _sec_logistic_dx.qmd >}}

## Objections to reporting odds ratios

{{< include _sec_OR_objections.qmd >}}

## Other link functions for Bernoulli outcomes

If you want risk ratios, you can sometimes get them by changing the link
function:

```{r}
data(anthers, package = "dobson")
anthers_sum <- aggregate(
  anthers[c("n", "y")],
  by = anthers[c("storage")], FUN = sum
)

anthers_glm_log <- glm(
  formula = cbind(y, n - y) ~ storage,
  data = anthers_sum,
  family = binomial(link = "log")
)

anthers_glm_log |>
  parameters() |>
  print_md()
```

---

Now $\exp{\beta}$ gives us risk ratios instead of odds ratios:

```{r}
anthers_glm_log |>
  parameters(exponentiate = TRUE) |>
  print_md()
```

---

Let's compare this model with a logistic model:

```{r}
anthers_glm_logit <- glm(
  formula = cbind(y, n - y) ~ storage,
  data = anthers_sum,
  family = binomial(link = "logit")
)

anthers_glm_logit |>
  parameters(exponentiate = TRUE) |>
  print_md()
```

\[to add: fitted plots on each outcome scale\]

---

When I try to use `link ="log"` in practice, I often get errors about
not finding good starting values for the estimation procedure. 
This is likely because the model is producing fitted probabilities greater than
1.

When this happens, you can try to fit Poisson regression models instead
(we will see those soon!). 
But then the outcome distribution isn't quite
right, and you won't get warnings about fitted probabilities greater
than 1. 
In my opinion, the Poisson model for binary outcomes is
confusing and not very appealing.

### WCGS: link functions

```{r}
wcgs_glm_logit_link <- chd_grouped_data |>
  mutate(type = relevel(dibpat, ref = "Type B")) |>
  glm(
    "formula" = cbind(x, `n - x`) ~ dibpat * age,
    "data" = _,
    "family" = binomial(link = "logit")
  )

wcgs_glm_identity_link <-
  chd_grouped_data |>
  mutate(type = relevel(dibpat, ref = "Type B")) |>
  glm(
    "formula" = cbind(x, `n - x`) ~ dibpat * age,
    "data" = _,
    "family" = binomial(link = "identity")
  )
wcgs_glm_identity_link |>
  coef() |>
  pander()
```

```{r}
#| label: fig-diagnostics-glm1
#| fig-cap: "Residuals vs Fitted plot for `wcgs` models"
#| layout-ncol: 2
#| fig-subcap:
#| - "Logistic link"
#| - "Identity link"

library(ggfortify)
wcgs_glm_logit_link |> autoplot(which = c(1), ncol = 1) + facet_wrap(~dibpat)
wcgs_glm_identity_link |> autoplot(which = c(1), ncol = 1) + facet_wrap(~dibpat)
```

```{r}
#| label: fig-diagnostics-beetles
#| fig-cap: "Residuals vs Fitted plot for `BeetleMortality` models"
#| layout-ncol: 2
#| fig-subcap:
#| - "Logistic link"
#| - "Identity link"

beetles_lm <-
  beetles_long |>
  lm(formula = died ~ dose)

beetles <-
  beetles |> mutate(
    resid_logit = beetles_glm_grouped |> resid(type = "response")
  )
beetles_glm_grouped |> autoplot(which = c(1), ncol = 1)
beetles_lm |> autoplot(which = c(1), ncol = 1)
```

## Quasibinomial

See [Hua Zhou](https://hua-zhou.github.io/)'s [lecture notes](https://ucla-biostat-200c-2020spring.github.io/slides/04-binomial/binomial.html#:~:text=0.05%20%27.%27%200.1%20%27%20%27%201-,Quasi%2Dbinomial,-Another%20way%20to)

## Further reading

- @hosmer2013applied is a classic textbook on logistic regression
