{
  "hash": "b1b91d0c90e34a1e13e827344ea81d60",
  "result": {
    "engine": "knitr",
    "markdown": "# Introduction to Maximum Likelihood Inference {#sec-intro-MLEs}\n\n---\n\nThese notes are derived primarily from @dobson4e (mostly chapters 1-5).\n\nSome material was also taken from @mclachlan2007em and @CaseBerg01.\n\n---\n\n\n<!-- ::: {.content-hidden when-format=\"revealjs\"} -->\n\n---\n\n### Configuring R {.unnumbered}\n\nFunctions from these packages will be used throughout this document:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%>%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\n```\n:::\n\n\n\n\n\n\n\nHere are some R settings I use in this document:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9\n```\n:::\n\n\n\n\n\n\n\n<!-- ::: -->\n\n\n\n\\providecommand{\\cbl}[1]{\\left\\{#1\\right.}\n\\providecommand{\\cb}[1]{\\left\\{#1\\right\\}}\n\\providecommand{\\paren}[1]{\\left(#1\\right)}\n\\providecommand{\\sb}[1]{\\left[#1\\right]}\n\\def\\pr{\\text{p}}\n\\def\\am{\\arg \\max}\n\\def\\argmax{\\arg \\max}\n\\def\\p{\\text{p}}\n\\def\\P{\\text{P}}\n\\def\\ph{\\hat{\\text{p}}}\n\\def\\hp{\\hat{\\text{p}}}\n\\def\\ga{\\alpha}\n\\def\\b{\\beta}\n\\providecommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor}\n\\providecommand{\\ceiling}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\providecommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\def\\Ber{\\text{Ber}}\n\\def\\Bernoulli{\\text{Bernoulli}}\n\\def\\Pois{\\text{Pois}}\n\\def\\Poisson{\\text{Poisson}}\n\\def\\Gaus{\\text{Gaussian}}\n\\def\\Normal{\\text{N}}\n\\def\\NB{\\text{NegBin}}\n\\def\\NegBin{\\text{NegBin}}\n\\def\\vbeta{\\vec \\beta}\n\\def\\vb{\\vec \\b}\n\\def\\v0{\\vec{0}}\n\\def\\gb{\\beta}\n\\def\\gg{\\gamma}\n\\def\\gd{\\delta}\n\\def\\eps{\\varepsilon}\n\\def\\om{\\omega}\n\\def\\m{\\mu}\n\\def\\s{\\sigma}\n\\def\\l{\\lambda}\n\\def\\gs{\\sigma}\n\\def\\gm{\\mu}\n\\def\\M{\\text{M}}\n\\def\\gM{\\text{M}}\n\\def\\Mu{\\text{M}}\n\\def\\cd{\\cdot}\n\\def\\cds{\\cdots}\n\\def\\lds{\\ldots}\n\\def\\eqdef{\\stackrel{\\text{def}}{=}}\n\\def\\defeq{\\stackrel{\\text{def}}{=}}\n\\def\\hb{\\hat \\beta}\n\\def\\hl{\\hat \\lambda}\n\\def\\hy{\\hat y}\n\\def\\yh{\\hat y}\n\\def\\V{{\\text{Var}}}\n\\def\\hs{\\hat \\sigma}\n\\def\\hsig{\\hat \\sigma}\n\\def\\hS{\\hat \\Sigma}\n\\def\\hSig{\\hat \\Sigma}\n\\def\\hSigma{\\hat \\Sigma}\n\\def\\hSurv{\\hat{S}}\n\\providecommand{\\hSurvf}[1]{\\hat{S}\\paren{#1}}\n\\def\\dist{\\ \\sim \\ }\n\\def\\ddist{\\ \\dot{\\sim} \\ }\n\\def\\dsim{\\ \\dot{\\sim} \\ }\n\\def\\za{z_{1 - \\frac{\\alpha}{2}}}\n\\def\\cirad{\\za \\cdot \\hse{\\hb}}\n\\def\\ci{\\hb {\\color{red}\\pm} \\cirad}\n\\def\\th{\\theta}\n\\def\\Th{\\Theta}\n\\def\\xbar{\\bar{x}}\n\\def\\hth{\\hat\\theta}\n\\def\\hthml{\\hth_{\\text{ML}}}\n\\def\\ba{\\begin{aligned}}\n\\def\\ea{\\end{aligned}}\n\\def\\ind{⫫}\n\\def\\indpt{⫫}\n\\def\\all{\\forall}\n\\def\\iid{\\text{iid}}\n\\def\\ciid{\\text{ciid}}\n\\def\\simind{\\ \\sim_{\\ind}\\ }\n\\def\\siid{\\ \\sim_{\\iid}\\ }\n\\def\\simiid{\\siid}\n\\def\\distiid{\\siid}\n\\def\\tf{\\therefore}\n\\def\\Lik{\\mathcal{L}}\n\\def\\llik{\\ell}\n\\providecommand{\\llikf}[1]{\\llik \\paren{#1}}\n\\def\\score{\\ell'}\n\\providecommand{\\scoref}[1]{\\score \\paren{#1}}\n\\def\\hess{\\ell''}\n\\def\\hessian{\\ell''}\n\\providecommand{\\hessf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\hessianf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\starf}[1]{#1^*}\n\\def\\lik{\\ell}\n\\providecommand{\\est}[1]{\\widehat{#1}}\n\\providecommand{\\esttmp}[1]{{\\widehat{#1}}^*}\n\\def\\esttmpl{\\esttmp{\\lambda}}\n\\def\\cR{\\mathcal{R}}\n\\def\\range{\\mathcal{R}}\n\\def\\Range{\\mathcal{R}}\n\\providecommand{\\rangef}[1]{\\cR(#1)}\n\\def\\~{\\approx}\n\\def\\dapp{\\dot\\approx}\n\\providecommand{\\red}[1]{{\\color{red}#1}}\n\\providecommand{\\deriv}[1]{\\frac{\\partial}{\\partial #1}}\n\\providecommand{\\derivf}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\providecommand{\\blue}[1]{{\\color{blue}#1}}\n\\providecommand{\\green}[1]{{\\color{green}#1}}\n\\providecommand{\\hE}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hExp}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hmu}[1]{\\hat{\\mu}\\sb{#1}}\n\\def\\Expp{\\mathbb{E}}\n\\def\\Ep{\\mathbb{E}}\n\\def\\expit{\\text{expit}}\n\\providecommand{\\expitf}[1]{\\expit\\cb{#1}}\n\\providecommand{\\dexpitf}[1]{\\expit'\\cb{#1}}\n\\def\\logit{\\text{logit}}\n\\providecommand{\\logitf}[1]{\\logit\\cb{#1}}\n\\providecommand{\\E}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Ef}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Exp}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Expf}[1]{\\mathbb{E}\\sb{#1}}\n\\def\\Varr{\\text{Var}}\n\\providecommand{\\var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\varf}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Varf}[1]{\\text{Var}\\paren{#1}}\n\\def\\Covt{\\text{Cov}}\n\\providecommand{\\covh}[1]{\\widehat{\\text{Cov}}\\paren{#1}}\n\\providecommand{\\Cov}[1]{\\Covt \\paren{#1}}\n\\providecommand{\\Covf}[1]{\\Covt \\paren{#1}}\n\\def\\varht{\\widehat{\\text{Var}}}\n\\providecommand{\\varh}[1]{\\varht\\paren{#1}}\n\\providecommand{\\varhf}[1]{\\varht\\paren{#1}}\n\\providecommand{\\vc}[1]{\\boldsymbol{#1}}\n\\providecommand{\\sd}[1]{\\text{sd}\\paren{#1}}\n\\providecommand{\\SD}[1]{\\text{SD}\\paren{#1}}\n\\providecommand{\\hSD}[1]{\\widehat{\\text{SD}}\\paren{#1}}\n\\providecommand{\\se}[1]{\\text{se}\\paren{#1}}\n\\providecommand{\\hse}[1]{\\hat{\\text{se}}\\paren{#1}}\n\\providecommand{\\SE}[1]{\\text{SE}\\paren{#1}}\n\\providecommand{\\HSE}[1]{\\widehat{\\text{SE}}\\paren{#1}}\n\\renewcommand{\\log}[1]{\\text{log}\\cb{#1}}\n\\providecommand{\\logf}[1]{\\text{log}\\cb{#1}}\n\\def\\dlog{\\text{log}'}\n\\providecommand{\\dlogf}[1]{\\dlog \\cb{#1}}\n\\renewcommand{\\exp}[1]{\\text{exp}\\cb{#1}}\n\\providecommand{\\expf}[1]{\\exp{#1}}\n\\def\\dexp{\\text{exp}'}\n\\providecommand{\\dexpf}[1]{\\dexp \\cb{#1}}\n\\providecommand{\\e}[1]{\\text{e}^{#1}}\n\\providecommand{\\ef}[1]{\\text{e}^{#1}}\n\\providecommand{\\inv}[1]{\\paren{#1}^{-1}}\n\\providecommand{\\invf}[1]{\\paren{#1}^{-1}}\n\\def\\oinf{I}\n\\def\\Nat{\\mathbb{N}}\n\\providecommand{\\oinff}[1]{\\oinf\\paren{#1}}\n\\def\\einf{\\mathcal{I}}\n\\providecommand{\\einff}[1]{\\einf\\paren{#1}}\n\\def\\heinf{\\hat{\\einf}}\n\\providecommand{\\heinff}[1]{\\heinf \\paren{#1}}\n\\providecommand{\\1}[1]{\\mathbb{1}_{#1}}\n\\providecommand{\\set}[1]{\\cb{#1}}\n\\providecommand{\\pf}[1]{\\p \\paren{#1}}\n\\providecommand{\\Bias}[1]{\\text{Bias}\\paren{#1}}\n\\providecommand{\\bias}[1]{\\text{Bias}\\paren{#1}}\n\\def\\ss{\\sigma^2}\n\\providecommand{\\ssqf}[1]{\\sigma^2\\paren{#1}}\n\\providecommand{\\mselr}[1]{\\text{MSE}\\paren{#1}}\n\\providecommand{\\maelr}[1]{\\text{MAE}\\paren{#1}}\n\\providecommand{\\abs}[1]{\\left|#1\\right|}\n\\providecommand{\\sqf}[1]{\\paren{#1}^2}\n\\providecommand{\\sq}{^2}\n\\def\\err{\\eps}\n\\providecommand{\\erf}[1]{\\err\\paren{#1}}\n\\renewcommand{\\vec}[1]{\\tilde{#1}}\n\\providecommand{\\v}[1]{\\vec{#1}}\n\\providecommand{\\matr}[1]{\\mathbf{#1}}\n\\def\\mX{\\matr{X}}\n\\def\\mx{\\matr{x}}\n\\def\\vx{\\vec{x}}\n\\def\\vX{\\vec{X}}\n\\def\\vy{\\vec{y}}\n\\def\\vY{\\vec{Y}}\n\\def\\vpi{\\vec{\\pi}}\n\\providecommand{\\mat}[1]{\\mathbf{#1}}\n\\providecommand{\\dsn}[1]{#1_1, \\ldots, #1_n}\n\\def\\X1n{\\dsn{X}}\n\\def\\Xin{\\dsn{X}}\n\\def\\x1n{\\dsn{x}}\n\\def\\'{^{\\top}}\n\\def\\dpr{\\cdot}\n\\def\\Xx1n{X_1=x_1, \\ldots, X_n = x_n}\n\\providecommand{\\dsvn}[2]{#1_1=#2_1, \\ldots, #1_n = #2_n}\n\\providecommand{\\sumn}[1]{\\sum_{#1=1}^n}\n\\def\\sumin{\\sum_{i=1}^n}\n\\def\\sumi1n{\\sum_{i=1}^n}\n\\def\\prodin{\\prod_{i=1}^n}\n\\def\\prodi1n{\\prod_{i=1}^n}\n\\providecommand{\\lp}[2]{#1 \\' \\beta}\n\\def\\odds{\\omega}\n\\def\\OR{\\text{OR}}\n\\def\\logodds{\\eta}\n\\def\\oddst{\\text{odds}}\n\\def\\probst{\\text{probs}}\n\\def\\probt{\\text{probt}}\n\\def\\probit{\\text{probit}}\n\\providecommand{\\oddsf}[1]{\\oddst\\cb{#1}}\n\\providecommand{\\doddsf}[1]{{\\oddst}'\\cb{#1}}\n\\def\\oddsinv{\\text{invodds}}\n\\providecommand{\\oddsinvf}[1]{\\oddsinv\\cb{#1}}\n\\def\\invoddsf{\\oddsinvf}\n\\providecommand{\\doddsinvf}[1]{{\\oddsinv}'\\cb{#1}}\n\\def\\dinvoddsf{\\doddsinvf}\n\\def\\haz{h}\n\\def\\cuhaz{H}\n\\def\\incidence{\\bar{\\haz}}\n\\def\\phaz{\\Expf{\\haz}}\n\n\n\n\n\n\n\n\n\n```{=html}\n<style>\n.quarto-figure-center > figure {\ntext-align: center;\n}\n</style>\n```\n\n\n\n\n\n\n\n\n\n## Overview of maximum likelihood estimation\n\n### The likelihood function\n\n:::{#def-lik-obs}\n#### Likelihood of a single observation\n\nLet $X$ be a random variable and let $x$ be $X$'s observed data value.\nLet $\\p_{\\Th}(X=x)$ be a probability model for the distribution of $X$, with parameter vector $\\Th$.\n\nThen the **likelihood** of parameter value $\\th$, for model $\\p_{\\Th}(X=x)$ and data $X = x$, is simply the probability of the event $X=x$ given $\\Th = \\th$:\n \n$$\n\\ba\n\\Lik(\\theta) &\\eqdef \\P_{\\theta}(X = x)\n\\ea\n$$\n:::\n\n---\n\n:::{#def-lik}\n#### Likelihood of a dataset\n\nLet $\\vec x$ be a dataset with corresponding random variable $\\vec X$.\nLet $\\p_{\\Th}(\\vec X)$ be a probability model for the distribution of $\\vX$ with unknown parameter vector $\\Th$.\n\nThen the **likelihood** of parameter value $\\th$, for model $\\p_{\\Th}(X)$ and data $\\vX = \\vx$, is the *joint probability* of $\\vX = \\vx$ given $\\Th = \\th$:\n \n$$\n\\ba\n\\Lik(\\theta) &\\eqdef p_{\\theta}(\\vX = \\vx)\n\\\\&=p_{\\theta}(X_1=x_1, ..., X_n = x_n)\n\\ea\n$$\n:::\n\n::: callout-note\n\n#### Notation for the likelihood function\n\nThe likelihood function can be written as:\n\n- $\\Lik(\\theta)$\n- $\\Lik(\\vec x;\\theta)$\n- $\\Lik(\\theta; \\vec x)$\n- $\\Lik_{\\vec x}(\\theta)$\n- $\\Lik_{\\theta}(\\vec x)$\n- $\\Lik(\\vec x | \\theta)$\n\nAll of these notations mean the same thing.\n:::\n\n::: notes\nThe likelihood is a function that takes $\\theta$ (and implicitly, $\\vec X$) as inputs and outputs a single number, the joint probability of $\\vec x$ for model $p_\\Theta(\\vX=\\vx)$ with $\\Theta = \\theta$.\n:::\n\n---\n\n:::{#thm-lik-iid}\n\n#### Likelihood of an independent sample\n\nFor [mutually independent](probability.qmd#def-indpt) data $X_1, ..., X_n$:\n\n$$\\Lik(\\vec x|\\theta) = \\prod_{i=1}^n \\p(X_i=x_i|\\theta)$$ {#eq-Lik}\n\n:::\n\n:::{.proof}\n\n$$\n\\ba\n\\Lik(\\vec x|\\theta) \n&\\eqdef \\p(X_1 = x_1, …,X_n =x_n|\\theta) \n\\\\&= \\prod_{i=1}^n \\p(X_i=x_i|\\theta)\n\\ea\n$$\nThe second equality is by the definition of statistical independence.\n\n:::\n\n---\n\n:::{#def-lik-factor}\n#### Likelihood components\n\nGiven an $\\iid$ dataset $\\vec x$, the **likelihood component** or **likelihood factor** of observation $X_i=x_i$ is the marginal likelihood of $X_i=x_i$: \n\n$$\\Lik_i(\\theta) = \\P(X_i=x_i)$$\n\n:::\n\n---\n\n:::{#thm-ds-lik-obs-lik}\n\nFor $\\iid$ data $\\vx \\eqdef \\x1n$, \nthe likelihood of the dataset is equal to the product of the observation-specific likelihood factors:\n\n$$\\Lik(\\theta) = \\prodin \\Lik_i(\\theta)$$\n:::\n\n---\n\n### The maximum likelihood estimate\n\n:::{#def-mle}\n\n#### Maximum likelihood estimate\n\nThe **maximum likelihood estimate** of a parameter vector $\\Theta$, denoted $\\hthml$, is the value of $\\Theta$ that maximizes the likelihood:\n\n$$\n\\hthml \\eqdef \\arg \\max_\\Th \\Lik(\\Th)\n$$ {#eq-mle}\n:::\n\n### Finding the maximum of a function\n\nRecall from calculus: the maxima of a continuous function $f(x)$ over a range of input values $\\rangef{x}$ can be found either:\n\n- at the edges of the range of input values, *OR*:\n- where the function is flat (i.e. where the gradient function $f'(x) = 0$) *AND* the second derivative is negative definite ($f''(x) < 0$).\n\n### Directly maximizing the likelihood function for *iid* data\n\nTo find the maximizer(s) of the likelihood function, we need to solve $\\Lik'(\\th) = 0$ for $\\theta$. However, even for mutually independent data, we quickly run into a problem:\n\n$$\n\\ba\n\\Lik'(\\th) \n&= \\deriv{\\th} \\Lik(\\th)\n\\\\ &= \\deriv{\\th} \\prod_{i=1}^n p(X_i=x_i|\\theta)\n\\ea\n$$ {#eq-deriv-Lik}\n\nThe derivative of the likelihood of independent data is the derivative of a product. \nWe will have to perform a massive application of the product rule to evaluate this derivative.\n\n### The log-likelihood function\n\nIt is typically easier to work with the log of the likelihood function:\n\n:::{#def-loglik}\n\n#### Log-likelihood\n\nThe **log-likelihood** of parameter value $\\theta$, for model $\\p_{\\Theta}(\\vX)$ and data $\\vX = \\vx$, is the natural logarithm of the likelihood^[ <https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin>]:\n\n$$\\lik(\\th) \\eqdef \\logf{\\Lik(\\th)}$$\n:::\n\n---\n\n:::{#def-loglik}\n\n#### Log-likelihood components\n\nGiven a dataset $\\vX = \\vx$, the **log-likelihood component of observation $X_i=x_i$** is the natural logarithm of the likelihood component:\n\n$$\\lik_i(\\th) \\eqdef \\logf{\\Lik_i(\\th)}$$\n:::\n\n---\n\n:::{#thm-mle-use-log}\n####\n\nThe likelihood and log-likelihood have the same maximizer:\n\n$$\n\\am_\\th \\Lik(\\th) = \\am_\\th \\lik(\\th)\n$$\n::: \n\n::: proof\nLeft to the reader.\n:::\n\n---\n\n:::{#thm-llik-iid}\n\n#### Log-likelihood of an $\\iid$ sample\n\nFor $\\iid$ data $X_1, ..., X_n$ with shared distribution $\\p(X=x)$:\n\n$$\\ell(x|\\theta) = \\sum_{i=1}^n \\log{p(X=x_i|\\theta)}$$ {#eq-loglik}\n:::\n\n:::{.proof}\n$$\n\\ba\n\\ell(x|\\theta) \n&\\eqdef \\log{\\Lik(\\vec x|\\theta)}\n\\\\&= \\log{\\prod_{i=1}^n \\p(X_i=x_i|\\theta)}\n\\\\&= \\sum_{i=1}^n \\log{p(X=x_i|\\theta)}\n\\ea\n$$\n:::\n\n---\n\n::: notes\n\nFor $\\iid$ data, we will have a much easier time taking the derivative of the log-likelihood:\n\n:::\n\n:::{#thm-deriv-llik-iid}\n#### Derivative of the log-likelihood function for $\\iid$ data\n\nFor $\\iid$ data:\n\n$$\\ell'(\\theta) = \\sumin \\deriv{\\theta} \\log{\\p(X=x_i|\\theta)}$$ {#eq-deriv-llik}\n\n:::\n\n:::{.proof}\n$$\n\\ba\n\\lik'(\\th) \n&= \\deriv{\\th} \\lik(\\th)\n\\\\ &= \\deriv{\\th} \\sum_{i=1}^n \\log{\\p(X=x_i|\\theta)}\n\\\\ &= \\sum_{i=1}^n \\deriv{\\th} \\log{\\p(X=x_i|\\theta)}\n\\ea\n$$\n:::\n\n---\n\n### The score function\n\nThe first derivative^[a.k.a. the [gradient](https://en.wikipedia.org/wiki/Gradient)] of the log-likelihood, $\\lik'(\\th)$, is important enough to have its own name: the *score function*.\n\n:::{#def-score}\n#### Score function\n\nThe **score function** of a statistical model $\\pr(\\vec X=\\vec x)$ is the gradient (i.e., first derivative) of the log-likelihood of that model:\n\n$$\\lik'(\\th) \\eqdef \\deriv{\\th} \\lik(\\th)$$\n:::\n\n::: notes\nWe often\nskip writing the arguments $x$ and/or $\\theta)$, so\n$\\ell' \\eqdef \\ell'(\\vec x;\\theta) \\eqdef \\ell'(\\theta)$.[^1] Some statisticians\nuse $U$ or $S$ instead of $\\ell'$. I prefer $\\ell'$.\nWhy use up extra letters?\n:::\n\n### Asymptotic distribution of the maximum likelihood estimate\n\n::: notes\nWe learned how to quantify our uncertainty about these maximum likelihood estimates; with sufficient sample size, $\\hthml$ has an approximately Gaussian distribution [@newey1994large]:\n:::\n$$\n\\hat\\theta_{ML} \\dot \\sim N(\\theta,\\mathcal I(\\theta)^{-1})\n$$\n\nRecall:\n\n- $\\einf(\\theta) \\eqdef \\E{\\oinf(\\vX;\\theta)}$\n- $\\oinf(\\vX,\\theta) \\eqdef -\\ell''(\\vX;\\theta)$\n\nWe can estimate $\\einf(\\th)$ using either $\\einf(\\hthml)$ or $\\oinf(\\vec x; \\hthml)$.\n\nSo we can estimate the standard error of $\\hth_k$ as:\n\n$$\n\\HSE{\\hth_k} = \\sqrt{\\sb{\\inv{\\heinff{\\hthml}}}_{kk}}\n$$\n\n### The (Fisher) (expected) information matrix\n\nThe variance of $\\ell'(x,\\theta)$,\n${Cov}\\left\\{ \\ell'(x,\\theta) \\right\\}$, is also very\nimportant; we call it the \"expected information matrix\", \"Fisher\ninformation matrix\", or just \"information matrix\", and we represent it\nusing the symbol $\\einff{I}$ (`\\frakturI` in Unicode, `\\mathfrak{I}` in LaTeX).\n\n$$\n\\ba\n\\einf \n\\eqdef \\einf(\\theta) \n\\\\ &\\eqdef \\Covf{\\ell'|\\theta} \n\\\\ &= \\Expp[ \\ell'{\\ell'}\\' ] - \\Expp[ \\ell' ] \\ \\Expp[ \\ell' ]\\'\n\\ea\n$$\n\nThe elements of $\\mathfrak{I}$ are:\n\n$$\n\\ba\n\\mathfrak{I}_{ij} \n&\\eqdef \\Covf{{\\ell'}_{i},{\\ell'}_{j}}\n\\\\ &= \\Expp[ \\ell_{i}'\\ell_{j}' ] - \\Expp[ {\\ell'}_{i} ] \\Expp[ {\\ell'}_{j} ]\n\\ea\n$$\n\nHere, \n\n$$\n\\ba\n\\E{\\ell'}\n&\\eqdef \\int_{x \\in \\rangef{x}}\n{\n\\ell'(x,\\th) \\p(X = x | \\th) dx\n}\n\\\\ &= \\int_{x \\in \\rangef{X}}\n{\n\\paren\n{\n\\deriv{\\th}\n\\log{\\p(X = x | \\th)}\n}\n\\p(X = x | \\theta) dx\n}\n\\\\ &= \n\\int_{x \\in \\rangef{X}}\n{\n\\frac\n{\\deriv{\\theta} \\p(X = x | \\th)}\n{\\p(X = x | \\theta)}\n\\p(X = x | \\theta) dx\n}\n\\\\ &= \n\\int_{x \\in \\rangef{X}}\n{\n\\deriv{\\theta} \\p(X = x | \\th) dx\n}\n\\ea\n$$\n\n\n\nAnd similarly\n\n$$\n\\Exp{\\ell' \\ell'^{\\top}} \n\\eqdef \n\\int_{x \\in R(x)}\n{\\ell'(x,\\theta)\\ell'(x,\\theta)^{\\top}\\ \n\\pf{X = x | \\th}\\ dx}\n$$\n\nNote that $\\Exp{\\ell'}$ and\n$\\Exp{\\ell'{\\ell'}^{\\top}}$\nare functions of $\\theta$ but not of $x$; \nthe expectation operator removed $x$.\n\nAlso note that for most of the distributions you are familiar with\n(including Gaussian, binomial, Poisson, exponential):\n\n$$\\Exp{\\ell'} = 0$$\n\nSo\n\n$$\\einff{\\theta} = \\Exp{\\ell'{\\ell'}^{\\top} }$$\n\nMoreover, for those distributions (called the \"exponential family\"), we\nhave:\n\n$$\n\\mathfrak{I} = -\\Exp{\\ell''}\n= \\Exp{- \\ell''}\n$$\n\n(see @dobson4e, §3.17), where\n\n$$\\ell'' \\eqdef \\deriv{\\theta}\\ell^{'(x,\\theta)^{\\top}} = \\deriv{\\theta}\\deriv{\\theta^{\\top}}\\ell(x,\\theta)$$\n\nis the $p \\times p$ matrix whose elements are:\n\n$$\\ell_{ij}'' \\eqdef \\deriv{\\theta_{i}}\\deriv{\\theta_{j}}\\log{ p\\left( X = x \\mid \\theta \\right)}$$\n\n$\\ell''$ is called the \"Hessian\"^[named after mathematician [Otto Hesse](https://en.wikipedia.org/wiki/Otto_Hesse)] of the log-likelihood\nfunction.\n\nSometimes, we use $I(\\theta;x) \\eqdef - \\ell''$ (note the\nstandard-font \"I\" here). $I(\\theta;x)$ is the observed information, precision, or concentration\nmatrix (Negative Hessian).\n\n:::{.callout-important}\n#### Key point\n The asymptotics of MLEs gives us\n${\\widehat{\\theta}}_{ML} \\sim N\\left( \\theta,\\mathfrak{I}^{- 1}(\\theta) \\right)$,\napproximately, for large sample sizes.\n:::\n\nWe can estimate $\\einf^{- 1}(\\theta)$ by working out\n$\\Ef{-\\ell''}$ or\n$\\Ef{\\ell'{\\ell'}^{\\top}}$\nand plugging in $\\hthml$, but sometimes we instead use\n$\\oinf(\\hthml,\\vx)$ for convenience; there are\nsome cases where it’s provably better according to some criteria (@efron1978assessing).\n\n### Iterative maximization {#sec-newton-raphson}\n\n(c.f., @dobson4e, Chapter 4)\n\n::: notes\nLater, \nwhen we are trying to find MLEs for likelihoods which we can’t easily differentiate, \nwe will \"hill-climb\" using the Newton-Raphson algorithm:\n:::\n\n$$\n\\begin{aligned}\n\\esttmp{\\theta} \n&\\leftarrow \\esttmp{\\theta} + \\inv{\\oinff{\\vec y;\\esttmp{\\theta}}}\n\\scoref{\\vec y;\\esttmp{\\theta}}\n\\\\ &= \\esttmp{\\theta} - \\inv{\\hessf{\\vec y;\\esttmp{\\theta}}} \n\\scoref{\\vec y;\\esttmp{\\theta}}\n\\end{aligned}\n$$\n\n---\n\n::: notes\nThe reasoning for this algorithm is that we can approximate the the score function using the first-order [Taylor polynomial](https://en.wikipedia.org/wiki/Taylor%27s_theorem):\n:::\n\n$$\n\\ba\n\\score(\\th) \n&\\approx \\score^*(\\th)\n\\\\ &\\eqdef \\score(\\esttmp{\\th}) + \\hessian(\\esttmp{\\th})(\\th - \\esttmp{\\th})\n\\ea\n$$\n\n---\n\n::: notes\nThe approximate score function, $\\score^*(\\th)$, is a linear function of $\\th$, so it is easy to solve the corresponding approximate score equation, $\\score^*(\\th) = 0$, for $\\th$:\n\n:::\n\n$$\n\\ba\n\\th \n&= \\esttmp{\\th} - \\score(\\esttmp{\\th}) \\cd \\inv{\\hessian(\\esttmp{\\th})}\n\\ea\n$$\n\n---\n\nFor computational simplicity, we will sometimes use\n$\\mathfrak{I}^{- 1}(\\theta)$ in place of\n$I\\left( \\widehat{\\theta},y \\right)$; \ndoing so is called \"Fisher scoring\" or the \"method of scoring\". \nNote that this is the opposite of the substitution that we are making for estimating the variance of the MLE; \nthis time we should technically use the observed information but we use the expected information instead.\n\n\n---\n\nThere’s also an \"empirical information matrix\" (see @mclachlan2007em):\n\n$$I_{e}(\\theta,y) \\eqdef \\sum_{i = 1}^{n}{\\ell_{i}'\\ {\\ell_{i}'}^{\\top}} - \\frac{1}{n}\\ell'{\\ell'}^{\\top}$$\n\nwhere $\\ell_{i}$ is the log-likelihood of the ith observation.\nNote that $\\ell' = \\sum_{i = 1}^{n}\\ell_{i}'$.\n\n$\\frac{1}{n}I_{e}(\\theta,y)$ is the sample equivalent of\n\n$$\\mathfrak{I \\eqdef I(}\\theta) \\eqdef {Cov}\\left( \\ell'|\\theta \\right) = E[ \\ell'{\\ell'}^{\\top} ] - E[ \\ell' ]\\ E[ \\ell' ]^{\\top}$$\n\n$$\\left\\{ \\mathfrak{I}_{jk} \\eqdef {Cov}\\left( {\\ell'}_{j},{\\ell'}_{k} \\right) = E[ \\ell_{j}'\\ell_{k}' ] - E[ {\\ell'}_{j} ] E[ {\\ell'}_{k} ] \\right\\}$$\n\n$I_{e}(\\theta,y)$ is sometimes computationally easier to compute for\nNewton-Raphson-type maximization algorithms.\n\nc.f. <https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization>\n\n### Quantifying (un)certainty of MLEs\n\n#### Confidence intervals for MLEs\n\nAn asymptotic approximation of a 95% confidence interval for $\\theta_k$ is\n\n$$\n\\hthml \\pm z_{0.975} \\times \\HSE{\\hth_k}\n$$\n\nwhere $z_\\beta$ the $\\beta$ quantile of the standard Gaussian distribution.\n\n#### p-values and hypothesis tests for MLEs\n\n(to add)\n\n#### Likelihood ratio tests for MLEs\n\nlog(likelihood ratio) tests [c.f. @dobson4e §5.7]:\n\n$$-2\\ell_{0} \\sim \\chi^{2}(p - q)$$\n\nSee also <https://online.stat.psu.edu/stat504/book/export/html/657>\n\n#### Prediction intervals for MLEs\n\n$$\\overline{X} \\in [ \\widehat{\\mu} \\pm z_{1 - \\alpha\\text{/}2}\\frac{\\sigma}{m} ]$$\n\nWhere $m$ is the sample size of the new data to be predicted (typically\n1, except for binary outcomes, where it needs to be bigger for\nprediction intervals to make sense)\n\n[^1]: I might sometimes switch the order of $x,$ $\\theta$; this is\n    unintentional and not meaningful.\n\n## Example: Maximum likelihood for Tropical Cyclones in Australia\n\n\n(Adapted from @dobson4e §1.6.5)\n\n### Data {#sec-dobson-cyclones-data}\n\n\\providecommand{\\cbl}[1]{\\left\\{#1\\right.}\n\\providecommand{\\cb}[1]{\\left\\{#1\\right\\}}\n\\providecommand{\\paren}[1]{\\left(#1\\right)}\n\\providecommand{\\sb}[1]{\\left[#1\\right]}\n\\def\\pr{\\text{p}}\n\\def\\am{\\arg \\max}\n\\def\\argmax{\\arg \\max}\n\\def\\p{\\text{p}}\n\\def\\P{\\text{P}}\n\\def\\ph{\\hat{\\text{p}}}\n\\def\\hp{\\hat{\\text{p}}}\n\\def\\ga{\\alpha}\n\\def\\b{\\beta}\n\\providecommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor}\n\\providecommand{\\ceiling}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\providecommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil}\n\\def\\Ber{\\text{Ber}}\n\\def\\Bernoulli{\\text{Bernoulli}}\n\\def\\Pois{\\text{Pois}}\n\\def\\Poisson{\\text{Poisson}}\n\\def\\Gaus{\\text{Gaussian}}\n\\def\\Normal{\\text{N}}\n\\def\\NB{\\text{NegBin}}\n\\def\\NegBin{\\text{NegBin}}\n\\def\\vbeta{\\vec \\beta}\n\\def\\vb{\\vec \\b}\n\\def\\v0{\\vec{0}}\n\\def\\gb{\\beta}\n\\def\\gg{\\gamma}\n\\def\\gd{\\delta}\n\\def\\eps{\\varepsilon}\n\\def\\om{\\omega}\n\\def\\m{\\mu}\n\\def\\s{\\sigma}\n\\def\\l{\\lambda}\n\\def\\gs{\\sigma}\n\\def\\gm{\\mu}\n\\def\\M{\\text{M}}\n\\def\\gM{\\text{M}}\n\\def\\Mu{\\text{M}}\n\\def\\cd{\\cdot}\n\\def\\cds{\\cdots}\n\\def\\lds{\\ldots}\n\\def\\eqdef{\\stackrel{\\text{def}}{=}}\n\\def\\defeq{\\stackrel{\\text{def}}{=}}\n\\def\\hb{\\hat \\beta}\n\\def\\hl{\\hat \\lambda}\n\\def\\hy{\\hat y}\n\\def\\yh{\\hat y}\n\\def\\V{{\\text{Var}}}\n\\def\\hs{\\hat \\sigma}\n\\def\\hsig{\\hat \\sigma}\n\\def\\hS{\\hat \\Sigma}\n\\def\\hSig{\\hat \\Sigma}\n\\def\\hSigma{\\hat \\Sigma}\n\\def\\hSurv{\\hat{S}}\n\\providecommand{\\hSurvf}[1]{\\hat{S}\\paren{#1}}\n\\def\\dist{\\ \\sim \\ }\n\\def\\ddist{\\ \\dot{\\sim} \\ }\n\\def\\dsim{\\ \\dot{\\sim} \\ }\n\\def\\za{z_{1 - \\frac{\\alpha}{2}}}\n\\def\\cirad{\\za \\cdot \\hse{\\hb}}\n\\def\\ci{\\hb {\\color{red}\\pm} \\cirad}\n\\def\\th{\\theta}\n\\def\\Th{\\Theta}\n\\def\\xbar{\\bar{x}}\n\\def\\hth{\\hat\\theta}\n\\def\\hthml{\\hth_{\\text{ML}}}\n\\def\\ba{\\begin{aligned}}\n\\def\\ea{\\end{aligned}}\n\\def\\ind{⫫}\n\\def\\indpt{⫫}\n\\def\\all{\\forall}\n\\def\\iid{\\text{iid}}\n\\def\\ciid{\\text{ciid}}\n\\def\\simind{\\ \\sim_{\\ind}\\ }\n\\def\\siid{\\ \\sim_{\\iid}\\ }\n\\def\\simiid{\\siid}\n\\def\\distiid{\\siid}\n\\def\\tf{\\therefore}\n\\def\\Lik{\\mathcal{L}}\n\\def\\llik{\\ell}\n\\providecommand{\\llikf}[1]{\\llik \\paren{#1}}\n\\def\\score{\\ell'}\n\\providecommand{\\scoref}[1]{\\score \\paren{#1}}\n\\def\\hess{\\ell''}\n\\def\\hessian{\\ell''}\n\\providecommand{\\hessf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\hessianf}[1]{\\hess \\paren{#1}}\n\\providecommand{\\starf}[1]{#1^*}\n\\def\\lik{\\ell}\n\\providecommand{\\est}[1]{\\widehat{#1}}\n\\providecommand{\\esttmp}[1]{{\\widehat{#1}}^*}\n\\def\\esttmpl{\\esttmp{\\lambda}}\n\\def\\cR{\\mathcal{R}}\n\\def\\range{\\mathcal{R}}\n\\def\\Range{\\mathcal{R}}\n\\providecommand{\\rangef}[1]{\\cR(#1)}\n\\def\\~{\\approx}\n\\def\\dapp{\\dot\\approx}\n\\providecommand{\\red}[1]{{\\color{red}#1}}\n\\providecommand{\\deriv}[1]{\\frac{\\partial}{\\partial #1}}\n\\providecommand{\\derivf}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\providecommand{\\blue}[1]{{\\color{blue}#1}}\n\\providecommand{\\green}[1]{{\\color{green}#1}}\n\\providecommand{\\hE}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hExp}[1]{\\hat{\\text{E}}\\sb{#1}}\n\\providecommand{\\hmu}[1]{\\hat{\\mu}\\sb{#1}}\n\\def\\Expp{\\mathbb{E}}\n\\def\\Ep{\\mathbb{E}}\n\\def\\expit{\\text{expit}}\n\\providecommand{\\expitf}[1]{\\expit\\cb{#1}}\n\\providecommand{\\dexpitf}[1]{\\expit'\\cb{#1}}\n\\def\\logit{\\text{logit}}\n\\providecommand{\\logitf}[1]{\\logit\\cb{#1}}\n\\providecommand{\\E}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Ef}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Exp}[1]{\\mathbb{E}\\sb{#1}}\n\\providecommand{\\Expf}[1]{\\mathbb{E}\\sb{#1}}\n\\def\\Varr{\\text{Var}}\n\\providecommand{\\var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\varf}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Var}[1]{\\text{Var}\\paren{#1}}\n\\providecommand{\\Varf}[1]{\\text{Var}\\paren{#1}}\n\\def\\Covt{\\text{Cov}}\n\\providecommand{\\covh}[1]{\\widehat{\\text{Cov}}\\paren{#1}}\n\\providecommand{\\Cov}[1]{\\Covt \\paren{#1}}\n\\providecommand{\\Covf}[1]{\\Covt \\paren{#1}}\n\\def\\varht{\\widehat{\\text{Var}}}\n\\providecommand{\\varh}[1]{\\varht\\paren{#1}}\n\\providecommand{\\varhf}[1]{\\varht\\paren{#1}}\n\\providecommand{\\vc}[1]{\\boldsymbol{#1}}\n\\providecommand{\\sd}[1]{\\text{sd}\\paren{#1}}\n\\providecommand{\\SD}[1]{\\text{SD}\\paren{#1}}\n\\providecommand{\\hSD}[1]{\\widehat{\\text{SD}}\\paren{#1}}\n\\providecommand{\\se}[1]{\\text{se}\\paren{#1}}\n\\providecommand{\\hse}[1]{\\hat{\\text{se}}\\paren{#1}}\n\\providecommand{\\SE}[1]{\\text{SE}\\paren{#1}}\n\\providecommand{\\HSE}[1]{\\widehat{\\text{SE}}\\paren{#1}}\n\\renewcommand{\\log}[1]{\\text{log}\\cb{#1}}\n\\providecommand{\\logf}[1]{\\text{log}\\cb{#1}}\n\\def\\dlog{\\text{log}'}\n\\providecommand{\\dlogf}[1]{\\dlog \\cb{#1}}\n\\renewcommand{\\exp}[1]{\\text{exp}\\cb{#1}}\n\\providecommand{\\expf}[1]{\\exp{#1}}\n\\def\\dexp{\\text{exp}'}\n\\providecommand{\\dexpf}[1]{\\dexp \\cb{#1}}\n\\providecommand{\\e}[1]{\\text{e}^{#1}}\n\\providecommand{\\ef}[1]{\\text{e}^{#1}}\n\\providecommand{\\inv}[1]{\\paren{#1}^{-1}}\n\\providecommand{\\invf}[1]{\\paren{#1}^{-1}}\n\\def\\oinf{I}\n\\def\\Nat{\\mathbb{N}}\n\\providecommand{\\oinff}[1]{\\oinf\\paren{#1}}\n\\def\\einf{\\mathcal{I}}\n\\providecommand{\\einff}[1]{\\einf\\paren{#1}}\n\\def\\heinf{\\hat{\\einf}}\n\\providecommand{\\heinff}[1]{\\heinf \\paren{#1}}\n\\providecommand{\\1}[1]{\\mathbb{1}_{#1}}\n\\providecommand{\\set}[1]{\\cb{#1}}\n\\providecommand{\\pf}[1]{\\p \\paren{#1}}\n\\providecommand{\\Bias}[1]{\\text{Bias}\\paren{#1}}\n\\providecommand{\\bias}[1]{\\text{Bias}\\paren{#1}}\n\\def\\ss{\\sigma^2}\n\\providecommand{\\ssqf}[1]{\\sigma^2\\paren{#1}}\n\\providecommand{\\mselr}[1]{\\text{MSE}\\paren{#1}}\n\\providecommand{\\maelr}[1]{\\text{MAE}\\paren{#1}}\n\\providecommand{\\abs}[1]{\\left|#1\\right|}\n\\providecommand{\\sqf}[1]{\\paren{#1}^2}\n\\providecommand{\\sq}{^2}\n\\def\\err{\\eps}\n\\providecommand{\\erf}[1]{\\err\\paren{#1}}\n\\renewcommand{\\vec}[1]{\\tilde{#1}}\n\\providecommand{\\v}[1]{\\vec{#1}}\n\\providecommand{\\matr}[1]{\\mathbf{#1}}\n\\def\\mX{\\matr{X}}\n\\def\\mx{\\matr{x}}\n\\def\\vx{\\vec{x}}\n\\def\\vX{\\vec{X}}\n\\def\\vy{\\vec{y}}\n\\def\\vY{\\vec{Y}}\n\\def\\vpi{\\vec{\\pi}}\n\\providecommand{\\mat}[1]{\\mathbf{#1}}\n\\providecommand{\\dsn}[1]{#1_1, \\ldots, #1_n}\n\\def\\X1n{\\dsn{X}}\n\\def\\Xin{\\dsn{X}}\n\\def\\x1n{\\dsn{x}}\n\\def\\'{^{\\top}}\n\\def\\dpr{\\cdot}\n\\def\\Xx1n{X_1=x_1, \\ldots, X_n = x_n}\n\\providecommand{\\dsvn}[2]{#1_1=#2_1, \\ldots, #1_n = #2_n}\n\\providecommand{\\sumn}[1]{\\sum_{#1=1}^n}\n\\def\\sumin{\\sum_{i=1}^n}\n\\def\\sumi1n{\\sum_{i=1}^n}\n\\def\\prodin{\\prod_{i=1}^n}\n\\def\\prodi1n{\\prod_{i=1}^n}\n\\providecommand{\\lp}[2]{#1 \\' \\beta}\n\\def\\odds{\\omega}\n\\def\\OR{\\text{OR}}\n\\def\\logodds{\\eta}\n\\def\\oddst{\\text{odds}}\n\\def\\probst{\\text{probs}}\n\\def\\probt{\\text{probt}}\n\\def\\probit{\\text{probit}}\n\\providecommand{\\oddsf}[1]{\\oddst\\cb{#1}}\n\\providecommand{\\doddsf}[1]{{\\oddst}'\\cb{#1}}\n\\def\\oddsinv{\\text{invodds}}\n\\providecommand{\\oddsinvf}[1]{\\oddsinv\\cb{#1}}\n\\def\\invoddsf{\\oddsinvf}\n\\providecommand{\\doddsinvf}[1]{{\\oddsinv}'\\cb{#1}}\n\\def\\dinvoddsf{\\doddsinvf}\n\\def\\haz{h}\n\\def\\cuhaz{H}\n\\def\\incidence{\\bar{\\haz}}\n\\def\\phaz{\\Expf{\\haz}}\n\n\n\nThe `cyclones` dataset in the `dobson` package (@tbl-cyclones-data) records the number of tropical cyclones in Northeastern Australia during 13 November-to-April cyclone seasons (more details in @dobson4e §1.6.5 and `help(cyclones, package = \"dobson\")`). @fig-dobson-cyclone-time-series graphs the number of cyclones (y-axis) by season (x-axis). Let's use $Y_i$ to represent these counts, where $i$ is an indexing variable for the seasons and $Y_i$ is the number of cyclones in season $i$.\n\n### Exploratory analysis\n\nSuppose we want to learn about how many cyclones to expect per season.\n\n\n\n\n\n\n\n::: {#tbl-cyclones-data .cell tbl-cap='Number of tropical cyclones during a season from November to April in Northeastern Australia'}\n\n```{.r .cell-code}\nlibrary(dobson)\nlibrary(dplyr)\ndata(cyclones)\nlibrary(pander)\npander(cyclones |> relocate(season, .before = everything()))\n```\n\n::: {.cell-output-display}\n\n---------------------------\n season    years    number \n-------- --------- --------\n   1      1956/7      6    \n\n   2      1957/8      5    \n\n   3      1958/9      4    \n\n   4      1959/60     6    \n\n   5      1960/1      6    \n\n   6      1961/2      3    \n\n   7      1962/3      12   \n\n   8      1963/4      7    \n\n   9      1964/5      4    \n\n   10     1965/6      2    \n\n   11     1966/7      6    \n\n   12     1967/8      7    \n\n   13     1968/9      4    \n---------------------------\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\ncyclones |>\n  mutate(years = years |> factor(levels = years)) |>\n  ggplot(aes(x = years, y = number, group = 1)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"Season\") +\n  ylab(\"Number of cyclones\") +\n  expand_limits(y = 0) +\n  theme(axis.text.x = element_text(vjust = .5, angle = 45))\n```\n\n::: {.cell-output-display}\n![Number of tropical cyclones per season in northeastern Australia, 1956-1969](intro-MLEs_files/figure-html/fig-dobson-cyclone-time-series-1.png){#fig-dobson-cyclone-time-series width=672}\n:::\n:::\n\n\n\n\n\n\n\nThere's no obvious correlation between adjacent seasons, so let's assume that each season is independent of the others.\n\nLet's also assume that they are identically distributed; let's denote this distribution as $P(Y=y)$ (note that there's no index $i$ in this expression, since we are assuming the $Y_i$s are identically distributed). We can visualize the distribution using a bar plot (@fig-cyclones-bar-plot). @tbl-dobson-cyclones-sumstat provides summary statistics.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncyclones |>\n  ggplot() +\n  geom_histogram(aes(x = number)) +\n  expand_limits(x = 0) +\n  xlab(\"Number of cyclones\") +\n  ylab(\"Count (number of seasons)\")\n```\n\n::: {.cell-output-display}\n![Bar plot of cyclones per season](intro-MLEs_files/figure-html/fig-cyclones-bar-plot-1.png){#fig-cyclones-bar-plot width=672}\n:::\n:::\n\n::: {#tbl-dobson-cyclones-sumstat .cell tbl-cap='Summary statistics for `cyclones` data'}\n\n```{.r .cell-code}\nn = nrow(cyclones)\nsumx = cyclones |> pull(number) |> sum()\nxbar =  cyclones |> pull(number) |> mean()\n\ncyclones |> table1::table1(x = ~ number)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"Rtable1\"><table class=\"Rtable1\">\n<thead>\n<tr>\n<th class='rowlabel firstrow lastrow'></th>\n<th class='firstrow lastrow'><span class='stratlabel'>Overall<br><span class='stratn'>(N=13)</span></span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class='rowlabel firstrow'>number</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.54 (2.47)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>6.00 [2.00, 12.0]</td>\n</tr>\n</tbody>\n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\n\n\n\n### Model\n\nWe want to estimate $P(Y=y)$; that is, $P(Y=y)$ is our [estimand](estimation.qmd#def-estimand).\n\nWe could estimate $P(Y=y)$ for each value of $y$ in $0:\\infty$ separately (\"nonparametrically\") using the fraction of our data with $Y_i=y$, but then we would be estimating an infinitely large set of parameters, and we would have low precision. We will probably do better with a parametric model.\n\n:::{#exr-cyclone-choose-dist}\nWhat parametric probability distribution family might we use to model this empirical distribution?\n\n:::\n\n::::{.solution}\nLet's use the Poisson. The Poisson distribution is appropriate for this data , because the data are counts that could theoretically take any integer value (discrete) in the range $0:\\infty$. Visually, the plot of our data closely resembles a Poisson or binomial distribution. Since cyclones do not have an \"upper limit\" on the number of events we could potentially observe in one season, the Poisson distribution is more appropriate than the binomial.\n::::\n\n:::{#exr-def-poisson}\n\nWrite down the Poisson distribution's probability mass function.\n\n:::\n\n::::{.solution}\n\n$$P(Y=y) = \\frac{\\lambda^{y} e^{-\\lambda}}{y!}$${#eq-iid-model}\n::::\n\n\n### Estimating the model parameters using maximum likelihood\n\nNow, we can estimate the parameter $\\lambda$ for this distribution using maximum likelihood estimation.\n\nWhat is the likelihood?\n\n:::{#exr-poisson-likelihood}\nWrite down the likelihood (probability mass function or probability density function) of a single observation $x$, according to your model.\n:::\n\n::::{.solution}\n\n$$\n\\ba\n\\mathcal{L}(\\lambda; x) \n&= p(X=x|\\Lambda = \\lambda)\\\\\n&= \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\n\\ea\n$$\n\n::::\n\n\n::: {#exr-poisson-parameters}\n\nWrite down the vector of parameters in your model.\n:::\n::::{.solution}\n\nThere is only one parameter, $\\lambda$:\n\n$$\\theta = (\\lambda)$$\n::::\n\n\n::: {#exr-poisson-mean-and-variance}\nWrite down the population mean and variance of a single observation from your chosen probability model, as a function of the parameters (extra credit - derive them).\n\n:::\n::::{.solution}\n* Population mean: $\\text{E}[X]=\\lambda$ \n* Population variance: $\\text{Var}(X)=\\lambda$\n\n::::\n\n\n::: {#exr-sample-likelihood}\n\nWrite down the likelihood of the full dataset.\n\n:::\n\n::::{.solution}\n\n$$\n\\ba\n\\Lik(\\lambda; \\vx) \n&= \\P(\\vX = \\vx) \\\\\n&= \\P(X_1 = x_1, X_2 = x_2, ..., X_{13} = x_{13}) \\\\\n&= \\prod_{i=1}^{13} \\P(X_i = x_i) \\\\\n&= \\prod_{i=1}^{13} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\n\\ea\n$$\n\n::::\n\n::: {#exr-graph-likelihood}\n\nGraph the likelihood as a function of $\\lambda$.\n\n:::: {.solution}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlik = function(lambda, y = cyclones$number, n = length(y)) \n{\nlambda^sum(y) * exp(-n*lambda) / prod(factorial(y))\n}\n\nlibrary(ggplot2)\nlik_plot = \nggplot() +\ngeom_function(fun = lik, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"likelihood\") +\nxlab('lambda')\n\nprint(lik_plot)\n```\n\n::: {.cell-output-display}\n![Likelihood of Dobson cyclone data](intro-MLEs_files/figure-html/fig-cyclone-lik-1.png){#fig-cyclone-lik width=672}\n:::\n:::\n\n\n\n\n\n\n\n::::\n\n:::\n\n::: {#exr-sample-llik}\n\nWrite down the log-likelihood of the full dataset.\n\n::::{.solution}\n\n$$\n\\begin{aligned}\n\\ell(\\lambda; \\vec x) &= \\log{\\mathcal{L}(\\lambda;\\vec{x})}\\\\\n&= \\log{\\prod_{i = 1}^n\\frac{\\lambda^{x_i}\\text{e}^{-\\lambda}}{x_i!}}\\\\\n&= \\sum_{i = 1}^n\\log{\\frac{\\lambda^{x_i}\\text{e}^{-\\lambda}}{x_i!}}\\\\\n&= \\sum_{i = 1}^n{\\log{\\lambda^{x_i}} +\\log{\\text{e}^{-\\lambda}} - \\log{x_i!}}\\\\\n&= \\sum_{i = 1}^n{x_i\\log{\\lambda} -\\lambda - \\log{x_i!}}\\\\\n&= \\sum_{i = 1}^nx_i\\log{\\lambda} - \\sum_{i = 1}^n\\lambda - \\sum_{i = 1}^n\\log{x_i!}\\\\\n&= \\sum_{i = 1}^nx_i\\log{\\lambda} - n\\lambda - \\sum_{i = 1}^n\\log{x_i!}\\\\\n\\end{aligned}\n$$\n\n::::\n\n:::\n\n::: {#exr-graph-loglikelihood}\n\nGraph the log-likelihood as a function of $\\lambda$.\n\n:::: {.solution}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nloglik = function(lambda, y = cyclones$number, n = length(y))\n{\nsum(y) * log(lambda) - n*lambda - sum(log(factorial(y)))\n}\n\nll_plot = ggplot() +\ngeom_function(fun = loglik, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"log-likelihood\") +\nxlab('lambda')\nll_plot\n\n```\n\n::: {.cell-output-display}\n![log-likelihood of Dobson cyclone data](intro-MLEs_files/figure-html/fig-cyclone-llik-1.png){#fig-cyclone-llik width=672}\n:::\n:::\n\n\n\n\n\n\n\n::::\n\n:::\n\n#### The score function\n\n::: {#exr-cyclone-score-fn}\n\nDerive the score function for the dataset.\n\n::::{.solution}\n\nThe score function is the first derivative of the log-likelihood:\n\n$$\n\\begin{aligned}\n\\ell'( \\lambda; \\vec x ) &= \n\\deriv{\\lambda}{\\sum_{i = 1}^nx_i\\log{\\lambda} - n\\lambda - \\sum_{i = 1}^n\\log{x_i!}}\\\\\n&= \\deriv{\\lambda}\\sum_{i = 1}^nx_i\\log{\\lambda} - \\deriv{\\lambda}n\\lambda - \\deriv{\\lambda}\\sum_{i = 1}^n\\log{x_i!}\\\\\n&= \\sum_{i = 1}^nx_i\\deriv{\\lambda}\\log{\\lambda} - n\\deriv{\\lambda}\\lambda - \\sum_{i = 1}^n\\deriv{\\lambda}\\log{x_i!}\\\\\n&= \\sum_{i = 1}^nx_i\\frac{1}{\\lambda} - n - 0\\\\\n&= \\frac{1}{\\lambda} \\sum_{i = 1}^nx_i- n\n\\\\&= \\paren{\\frac{1}{\\lambda} n \\xbar} - n\n\\\\&= \\paren{\\frac{1}{\\lambda} 72} - 13\n\\end{aligned}\n$$\n\n::::\n\n:::\n\n:::{#exr-graph-score-function}\nGraph the score function.\n\n\n::::{.solution}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nscore = function(lambda, y = cyclones$number, n = length(y))\n{\n  (sum(y) / lambda) - n\n}\n\nggplot() +\ngeom_function(fun = score, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\n\nylab(\"l'(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0, col = 'red')\n\n```\n\n::: {.cell-output-display}\n![score function of Dobson cyclone data](intro-MLEs_files/figure-html/fig-cyclone-score-1.png){#fig-cyclone-score width=672}\n:::\n:::\n\n\n\n\n\n\n\n::::\n\n:::\n\n#### The Hessian matrix {#sec-hessian}\n\n:::{#exr-hessian}\n\nDerive the Hessian matrix.\n\n::::{.solution}\n\nThe Hessian function for an iid sample is the 2nd derivative(s) of the log-likelihood:\n\n$$\n\\begin{aligned}\n\\ell''( \\lambda; \\vec x ) &= \\deriv{\\lambda}\\left(\\frac{1}{\\lambda} \\sum_{i = 1}^nx_i- n\\right)\\\\\n&= \\deriv{\\lambda}\\frac{1}{\\lambda} \\sum_{i = 1}^nx_i- \\deriv{\\lambda}n\\\\\n&= -\\frac{1}{\\lambda^2} \\sum_{i = 1}^nx_i\\\\\n&= -\\frac{1}{\\lambda^2} n \\bar x\n\\\\&= -\\frac{1}{\\lambda^2} \\cd 72\n\\end{aligned}\n$$\n::::\n\n:::\n\n:::{#exr-graph-hession}\n\nGraph the Hessian.\n\n::::{.solution}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nhessian = function(lambda, y = cyclones$number, n = length(y))\n{\n-sum(y)/(lambda^2)\n}\n\nggplot() +\ngeom_function(fun = hessian, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\n\nylab(\"l''(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0, col = 'red')\n\n```\n\n::: {.cell-output-display}\n![Hessian function of Dobson cyclone data](intro-MLEs_files/figure-html/fig-cyclone-hessian-1.png){#fig-cyclone-hessian width=672}\n:::\n:::\n\n\n\n\n\n\n\n::::\n\n:::\n\n:::{#exr-score-equation}\nWrite the score equation (estimating equation).\n\n::::{.solution}\n\n$$\\ell'( \\lambda; \\vec x ) = 0$$\n\n::::\n\n:::\n\n:::{#exr-solve-score-equation}\nSolve the estimating equation for $\\lambda$:\n\n::::{.solution}\n$$\n\\begin{aligned}\n0 &= \\frac{1}{\\lambda}\\sum_{i = 1}^nx_i - n\\\\\nn &= \\frac{1}{\\lambda}\\sum_{i = 1}^nx_i\\\\\nn\\lambda &= \\sum_{i = 1}^nx_i\\\\\n\\lambda &= \n\\frac{1}{n}\\sum_{i = 1}^nx_i\\\\\n&=\\bar x\n\\end{aligned}\n$$\n::::\n\n:::\n\nLet's call this solution of the estimating equation $\\tilde \\lambda$ for now:\n\n$$\\tilde \\lambda \\eqdef \\bar x$$\n\n\n\n:::{#exr-check-hessian}\n\nConfirm that the Hessian $\\ell''(\\lambda; \\vec x)$ is negative when evaluated at $\\tilde \\lambda$.\n\n::::{.solution}\n\n$$\n\\begin{aligned}\n\\ell''( \\tilde\\lambda; \\vec x ) &= \n-\\frac{1}{\\tilde\\lambda^2} n \\bar x\\\\\n&= -\\frac{1}{\\bar x^2} n\\bar x\\\\\n&= -\\frac{n}{\\bar x}\\\\\n&<0\\\\\n\\end{aligned}\n$$\n\n::::\n\n:::\n\n::: {#exr-find-mle}\n\nFind the MLE of $\\lambda$.\n\n:::: {.solution}\n\nSince $\\ell''(\\tilde \\lambda; \\vec x)<0$, $\\tilde \\lambda$ is at least a local maximizer of the likelihood function $\\mathcal L(\\lambda)$. Since there is only one solution to the estimating equation and the Hessian is negative definite everywhere, $\\tilde \\lambda$ must also be the global maximizer of $\\mathcal L(\\lambda; \\vec x)$:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmle = mean(cyclones$number)\n```\n:::\n\n\n\n\n\n\n\n$$\\hat{\\lambda}_{\\text{ML}} = \\bar x = 5.5385$$\n\n::::\n\n:::\n\n:::{#exr-graph-mle}\n\nGraph the log-likelihood with the MLE superimposed.\n\n::::{.solution}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nmle_data = tibble(x = mle, y = loglik(mle))\nll_plot + geom_point(data = mle_data, aes(x = x, y = y), col = 'red')\n```\n\n::: {.cell-output-display}\n![log-likelihood of Dobson cyclone data with MLE](intro-MLEs_files/figure-html/fig-cyclone-llik-mle-1.png){#fig-cyclone-llik-mle width=672}\n:::\n:::\n\n\n\n\n\n\n\n::::\n:::\n\n#### Information matrices\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_inf = function(...) -hessian(...)\nggplot() +\ngeom_function(fun = obs_inf, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"I(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0, col = 'red') \n```\n\n::: {.cell-output-display}\n![Observed information function of Dobson cyclone data](intro-MLEs_files/figure-html/fig-obs-inf-matrix-1.png){#fig-obs-inf-matrix width=672}\n:::\n:::\n\n\n\n\n\n\n\n---\n\n:::::::{#exm-dobson-cyclones-Newton}\n\n##### Finding the MLE using the Newton-Raphson algorithm\n\n::: notes\n\nWe found that the MLE was $\\hat{\\lambda} = \\bar{x}$, \nby solving the score equation $\\ell'(\\lambda)=0$ for $\\lambda$.\n\nWhat if we hadn't been able to solve the score equation?\n\nThen we could start with some initial guess $\\esttmpl$, \nsuch as $\\esttmpl = 3$, and use the [Newton-Raphson algorithm](#sec-newton-raphson).\n\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify initial guess:\ncur_lambda_est = 3\n```\n:::\n\n\n\n\n\n\n\n:::::::\n\n---\n\n::: notes\n\nIn @exr-cyclone-score-fn, we found that the score function was:\n\n:::\n\n$$\n\\score( \\lambda; \\vec x ) = \\paren{\\frac{72}{\\lambda} } - n\n$$\n\n::: notes\nIn @exr-hessian, we found that the Hessian was:\n:::\n\n$$\n\\hessian( \\lambda; \\vec x ) = -\\frac{72}{\\lambda^2} \n$$\n\n---\n\n::: notes\nSo we can approximate the the score function using the first-order [Taylor polynomial](https://en.wikipedia.org/wiki/Taylor%27s_theorem):\n:::\n\n$$\n\\ba\n\\score(\\lambda) \n&\\approx \\score^*(\\lambda)\n\\\\ &\\eqdef \\score(\\esttmpl) + \\hessian(\\esttmpl)(\\lambda - \\esttmpl)\n\\\\ &= \\paren{\\frac{72}{\\esttmpl}  - n} + \\paren{-\\frac{72}{\\sqf{\\esttmpl}}} (\\lambda - \\esttmpl)\n\\ea\n$$\n\n---\n\n::: notes\n\n@fig-cyclone-newton-step1 compares the true score function and the approximate score function at $\\esttmpl = 3$.\n\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\napprox_score = function(lambda, lhat, ...)\n{\n  score(lambda = lhat, ...) +\n  hessian(lambda = lhat, ...) * (lambda - lhat)\n}\n\npoint_size = 5\n\nplot1 = ggplot() +\ngeom_function(\n  fun = score, \n  aes(col = \"true score function\"), \n  n = 1001) +\ngeom_function(\n  fun = approx_score, \n  aes(col = \"approximate score function\"),\n  n = 1001, \n  args = list(lhat = cur_lambda_est)) +\ngeom_point(\n  size = point_size,\n  aes(x = cur_lambda_est, y = score(lambda = cur_lambda_est),\n      col = \"current estimate\")\n) +\ngeom_point(\nsize = point_size,\naes(\nx = xbar,\ny = 0,\ncol = \"true MLE\"\n)\n) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"l'(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0)\n\nprint(plot1)\n```\n\n::: {.cell-output-display}\n![score function of Dobson cyclone data and approximate score function](intro-MLEs_files/figure-html/fig-cyclone-newton-step1-1.png){#fig-cyclone-newton-step1 width=672}\n:::\n:::\n\n\n\n\n\n\n\n---\n\n::: notes\n\nThis is equivalent to estimating the log-likelihood with a second-order Taylor polynomial:\n\n:::\n\n$$\n\\llik^*(\\lambda) = \n\\llik(\\esttmpl) + \n(\\lambda - \\esttmpl) \\score(\\esttmpl) +\n\\frac{1}{2}\\hessian(\\esttmpl)(\\lambda-\\esttmpl)^2\n$$\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\napprox_loglik = function(lambda, lhat, ...)\n{\nloglik(lambda = lhat, ...) +\nscore(lambda = lhat, ...) * (lambda - lhat) +\n  1/2 * hessian(lambda = lhat, ...) * (lambda - lhat)^2\n}\n\nplot_loglik = ggplot() +\ngeom_function(\n  fun = loglik, \n  aes(col = \"true log-likelihood\"), \n  n = 1001) +\ngeom_function(\n  fun = approx_loglik, \n  aes(col = \"approximate log-likelihood\"),\n  n = 1001, \n  args = list(lhat = cur_lambda_est)) +\ngeom_point(\n  size = point_size,\n  aes(x = cur_lambda_est, y = loglik(lambda = cur_lambda_est),\n      col = \"current estimate\")\n) +\ngeom_point(\nsize = point_size,\naes(\nx = xbar,\ny = loglik(xbar),\ncol = \"true MLE\"\n)\n) +\nxlim(min(cyclones$number) - 1, max(cyclones$number)) +\nylab(\"l'(lambda)\") +\nxlab('lambda')\n\nprint(plot_loglik)\n```\n\n::: {.cell-output-display}\n![log-likelihood of Dobson cyclone data and approximate log-likelihood function](intro-MLEs_files/figure-html/fig-cyclone-newton-step1-loglik-1.png){#fig-cyclone-newton-step1-loglik width=672}\n:::\n:::\n\n\n\n\n\n\n\n---\n\n::: notes\nThe approximate score function, $\\score^*(\\lambda)$, is a linear function of $\\lambda$, so it is easy to solve the corresponding approximate score equation, $\\score^*(\\lambda) = 0$, for $\\lambda$:\n:::\n\n$$\n\\ba\n\\lambda \n&= \\esttmpl - \\score(\\esttmpl) \\cd \\inv{\\hessian(\\esttmpl)}\n\\\\ &= 4.375\n\\ea\n$$\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_lambda_est <- \n   cur_lambda_est - \n   score(cur_lambda_est) * hessian(cur_lambda_est)^-1\n```\n:::\n\n\n\n\n\n\n\n---\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot2 = plot1 +\n  geom_point(size = point_size,\n             aes(x = new_lambda_est,\n                 y = 0,\n                 col = \"new estimate\")) +\n  geom_segment(\n    arrow = grid::arrow(),\n    linewidth = 2,\n    alpha = .7,\n    aes(\n      x = cur_lambda_est,\n      y = approx_score(lhat = cur_lambda_est,\n                       lambda = cur_lambda_est),\n      xend = new_lambda_est,\n      yend = 0,\n      col = \"update\"\n    )\n  )\nprint(plot2)\n\n```\n\n::: {.cell-output-display}\n![score function of Dobson cyclone data and approximate score function](intro-MLEs_files/figure-html/fig-cyclone-newton-step1-with-new-est-1.png){#fig-cyclone-newton-step1-with-new-est width=672}\n:::\n:::\n\n\n\n\n\n\n\n---\n\n::: notes\n\nSo we update $\\esttmpl \\leftarrow 4.375$ and repeat our estimation process:\n\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot2 +\ngeom_function(\n  fun = approx_score, \n  aes(col = \"new approximate score function\"),\n  n = 1001, \n  args = list(lhat = new_lambda_est)) +\ngeom_point(\n  size = point_size,\n  aes(x = new_lambda_est, y = score(lambda = new_lambda_est),\n      col = \"new estimate\")\n)\n\n```\n\n::: {.cell-output-display}\n![score function of Dobson cyclone data and approximate score function](intro-MLEs_files/figure-html/fig-cyclone-newton-step2-1.png){#fig-cyclone-newton-step2 width=672}\n:::\n:::\n\n\n\n\n\n\n\n---\n\n::: notes\nWe repeat this process until the likelihood converges:\n:::\n\n\n\n\n\n\n\n::: {#tbl-mle-converge .cell tbl-cap='Convergence of Newton-Raphson Algorithm for finding MLE of `cyclone` data'}\n\n```{.r .cell-code}\n\nlibrary(tibble)\ncur_lambda_est = 3 # restarting\ndiff_loglik = Inf\ntolerance = 10 ^ -4\nmax_iter = 100\nNR_info = tibble(\n  iteration = 0,\n  lambda = cur_lambda_est |> num(digits = 4),\n  likelihood = lik(cur_lambda_est),\n  `log(likelihood)` = loglik(cur_lambda_est)  |> num(digits = 4),\n  score = score(cur_lambda_est),\n  hessian = hessian(cur_lambda_est)\n)\n\nfor (cur_iter in 1:max_iter)\n{\n  new_lambda_est <-\n    cur_lambda_est - score(cur_lambda_est) * hessian(cur_lambda_est) ^ -1\n  \n  diff_loglik = loglik(new_lambda_est) - loglik(cur_lambda_est)\n  \n  new_NR_info = tibble(\n    iteration = cur_iter,\n    lambda = new_lambda_est,\n    likelihood = lik(new_lambda_est),\n    `log(likelihood)` = loglik(new_lambda_est),\n    score = score(new_lambda_est),\n    hessian = hessian(new_lambda_est),\n    `diff(loglik)` = diff_loglik\n  )\n  \n  NR_info = NR_info |> bind_rows(new_NR_info)\n  \n  cur_lambda_est = new_lambda_est\n  \n  if (abs(diff_loglik) < tolerance)\n    break\n  \n}\n\nNR_info\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"iteration\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lambda\"],\"name\":[2],\"type\":[\"num:.4!\"],\"align\":[\"right\"]},{\"label\":[\"likelihood\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"log(likelihood)\"],\"name\":[4],\"type\":[\"num:.4!\"],\"align\":[\"right\"]},{\"label\":[\"score\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"hessian\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"diff(loglik)\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0\",\"2\":\"3.0000\",\"3\":\"3.997e-18\",\"4\":\"-40.0610\",\"5\":\"1.100e+01\",\"6\":\"-8.000\",\"7\":\"NA\"},{\"1\":\"1\",\"2\":\"4.3750\",\"3\":\"4.329e-14\",\"4\":\"-30.7708\",\"5\":\"3.457e+00\",\"6\":\"-3.762\",\"7\":\"9.290e+00\"},{\"1\":\"2\",\"2\":\"5.2941\",\"3\":\"2.570e-13\",\"4\":\"-28.9897\",\"5\":\"6.002e-01\",\"6\":\"-2.569\",\"7\":\"1.781e+00\"},{\"1\":\"3\",\"2\":\"5.5277\",\"3\":\"2.762e-13\",\"4\":\"-28.9176\",\"5\":\"2.537e-02\",\"6\":\"-2.356\",\"7\":\"7.210e-02\"},{\"1\":\"4\",\"2\":\"5.5384\",\"3\":\"2.762e-13\",\"4\":\"-28.9175\",\"5\":\"4.930e-05\",\"6\":\"-2.347\",\"7\":\"1.367e-04\"},{\"1\":\"5\",\"2\":\"5.5385\",\"3\":\"2.762e-13\",\"4\":\"-28.9175\",\"5\":\"1.870e-10\",\"6\":\"-2.347\",\"7\":\"5.177e-10\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\n::: notes\nCompare with @exr-find-mle\n:::\n\n---\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nll_plot +\n  geom_segment(\n    data = NR_info,\n    arrow = grid::arrow(),\n    # linewidth = 2,\n    alpha = .7,\n    aes(\n      x = lambda,\n      xend = lead(lambda),\n      y = `log(likelihood)`,\n      yend = lead(`log(likelihood)`),\n      col = factor(iteration)\n    )\n  )\n\n```\n\n::: {.cell-output-display}\n![Newton-Raphson algorithm for finding MLE of [model @eq-iid-model] for `cyclone` data](intro-MLEs_files/figure-html/fig-NR-converge-1.png){#fig-NR-converge width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Maximum likelihood inference for univariate Gaussian models\n\nSuppose $X_{1}, ..., X_{n} \\siid N(\\mu, \\sigma^{2})$.\nLet $X = (X_{1},\\ldots,X_{n})^{\\top}$ be these random\nvariables in vector format. Let $x_{i}$ and $x$ denote the corresponding\nobserved data. Then $\\theta = (\\mu,\\sigma^{2})$ is\nthe vector of true parameters, and $\\Theta = (\\Mu, \\Sigma^2)$ is the vector of parameters as a random\nvector.\n\nThen the log-likelihood\nis:\n\n$$\n\\begin{aligned}\n\\ell \n&\\propto - \\frac{n}{2}\\log{\\sigma^{2}} - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{( x_{i} - \\mu)^{2}}{\\sigma^{2}}\\\\\n&= - \\frac{n}{2}\\log{\\sigma^{2}} - \\frac{1}{2\\sigma^{2}}\\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\\mu + \\mu^{2}}\n\\end{aligned}\n$$\n\n\n### The score function\n\n$$\\ell'(x,\\theta) \\eqdef \\deriv{\\theta}\\ell(x,\\theta) = \\left( \\begin{array}{r}\n\\deriv{\\mu}\\ell(\\theta;x) \\\\\n\\deriv{\\sigma^{2}}\\ell(\\theta;x)\n\\end{array} \\right) = \\left( \\begin{array}{r}\n\\ell_{\\mu}'(\\theta;x) \\\\\n\\ell_{\\sigma^{2}}'(\\theta;x)\n\\end{array} \\right)$$.\n\n$\\ell'(x,\\theta)$ is the function we set equal to 0 and solve\nto find the MLE:\n\n$${\\widehat{\\theta}}_{ML} = \\left\\{ \\theta:\\ell'(x,\\theta) = 0 \\right\\}$$\n\n### MLE of $\\mu$\n\n$$\n\\ba\n\\frac{d\\ell}{d\\mu} \n&= - \\frac{1}{2}\\sum_{i = 1}^{n}\n\\frac{- 2(x_{i} - \\mu)}{\\sigma^{2}}\n\\\\ &= \\frac{1}{\\sigma^{2}}\n\\sb{\n    \\paren{\n        \\sum_{i = 1}^{n}x_{i}\n    } \n    - n\\mu\n}\n\\ea\n$$\n\nIf $\\frac{d\\ell}{d\\mu} = 0$, then\n$\\mu = \\overline{x} \\eqdef \\frac{1}{n}\\sum_{i = 1}^{n}x_{i}$.\n\n$$\\frac{d^{2}\\ell}{(d\\mu)^{2}} = \\frac{- n}{\\sigma^{2}} < 0$$\n\nSo ${\\widehat{\\mu}}_{ML} = \\overline{x}$.\n\n### MLE of $\\sigma^{2}$\n\n:::{.callout-tip}\n#### Reparametrizing the Gaussian distribution\n\nWhen solving for ${\\widehat{\\sigma}}_{ML}$, you can treat\n$\\sigma^{2}$ as an atomic variable (don’t differentiate with respect to\n$\\sigma$ or things get messy). In fact, you can replace $\\sigma^{2}$\nwith $1/\\tau$ and differentiate with respect to $\\tau$ instead, and the\nprocess might be even easier.\n:::\n\n$$\\frac{d\\ell}{d\\sigma^{2}} = \\deriv{\\sigma^{2}}\\left( - \\frac{n}{2}\\log{\\sigma^{2}} - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{\\left( x_{i} - \\mu \\right)^{2}}{\\sigma^{2}} \\right)\\ $$\n\n$$= - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}$$\n\nIf $\\frac{d\\ell}{d\\sigma^{2}} = 0$, then:\n\n$$\\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} = \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}$$\n\n$$\\sigma^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}$$\n\nWe plug in ${\\widehat{\\mu}}_{ML} = \\overline{x}$ to maximize globally (a\ntechnique called profiling):\n\n$$\\sigma_{ML}^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}$$\n\nNow:\n\n$$\n\\begin{aligned}\n\\frac{d^{2}\\ell}{\\left( d\\sigma^{2} \\right)^{2}} \n&= \\deriv{\\sigma^{2}}\\left\\{ - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left\\{ - \\frac{n}{2}\\deriv{\\sigma^{2}}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\deriv{\\sigma^{2}}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left\\{ \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 2} - \\left( \\sigma^{2} \\right)^{- 3}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left( \\sigma^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( \\sigma^{2} \\right)^{- 1}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\n\\end{aligned}\n$$\n\nEvaluated at\n$\\mu = \\overline{x},\\sigma^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}$,\nwe have:\n\n$$\n\\begin{aligned}\n\\frac{d^{2}\\ell}{\\left( d\\sigma^{2} \\right)^{2}} \n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 1}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2} \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 1}n{\\widehat{\\sigma}}^{2} \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - n \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left\\{ \\frac{1}{2} - 1 \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left( - \\frac{1}{2} \\right) < 0\n\\end{aligned}\n$$\n\nFinally, we have:\n\n$$\n\\begin{aligned}\n\\frac{d^{2}\\ell}{d\\mu\\ d\\sigma^{2}} \n&= \\deriv{\\mu}\\left\\{ - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\deriv{\\mu}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\\\\n&= \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}{- 2(x_{i} - \\mu)}\\\\\n&= - \\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}{(x_{i} - \\mu)}\n\\end{aligned}\n$$\n\nEvaluated at\n$\\mu = \\widehat{\\mu} = \\overline{x},\\sigma^{2} = {\\widehat{\\sigma}}^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}$,\nwe have:\n\n$$\\frac{d^{2}\\ell}{d\\mu\\ d\\sigma^{2}} = - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left( n\\overline{x} - n\\overline{x} \\right) = 0$$\n\n### Covariance matrix\n\n$$I = \\begin{bmatrix}\n\\frac{n}{\\sigma^{2}} & 0 \\\\\n0 & \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left( - \\frac{1}{2} \\right)\n\\end{bmatrix} = \\begin{bmatrix}\na & 0 \\\\\n0 & d\n\\end{bmatrix}$$\n\nSo:\n\n$$I^{- 1} = \\frac{1}{ad}\\begin{bmatrix}\nd & 0 \\\\\n0 & a\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{1}{a} & 0 \\\\\n0 & \\frac{1}{d}\n\\end{bmatrix}$$\n\n$$I^{- 1} = \\begin{bmatrix}\n\\frac{{\\widehat{\\sigma}}^{2}}{n} & 0 \\\\\n0 & \\frac{{2\\left( {\\widehat{\\sigma}}^{2} \\right)}^{2}}{n}\n\\end{bmatrix}$$\n\nSee @CaseBerg01 p322, example 7.2.12.\n\nTo prove it’s a maximum, we need:\n\n- $\\ell' = 0$\n\n- At least one diagonal element of $\\ell''$ is negative.\n\n- Determinant of $\\ell''$ is positive.\n\n\n\n## Example: hormone therapy study\n\n::: notes\n\nNow, we're going to analyze some real-world data using a Gaussian model, and then we're going to do a simulation to examine the properties of maximum likelihood estimation for that Gaussian model.\n\nHere we look at the \"heart and estrogen/progestin study\" (HERS), a clinical \ntrial of hormone therapy for prevention of recurrent heart attacks and \ndeath among 2,763 post-menopausal women with existing coronary heart disease \n(CHD) (Hulley et al. 1998).\n\nWe are going to model the distribution of fasting glucose among nondiabetics who don't exercise.\n\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data directly from a UCSF website\nhers = haven::read_dta(\n  paste0( # I'm breaking up the url into two chunks for readability\n    \"https://regression.ucsf.edu/sites/g/files\",\n    \"/tkssra6706/f/wysiwyg/home/data/hersdata.dta\"\n  )\n)\n\n```\n:::\n\n\n\n::: {#tbl-HERS .cell tbl-cap='HERS dataset'}\n\n```{.r .cell-code}\nhers |> head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"HT\"],\"name\":[1],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"raceth\"],\"name\":[3],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"nonwhite\"],\"name\":[4],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"smoking\"],\"name\":[5],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"drinkany\"],\"name\":[6],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"exercise\"],\"name\":[7],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"physact\"],\"name\":[8],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"globrat\"],\"name\":[9],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"poorfair\"],\"name\":[10],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"medcond\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"htnmeds\"],\"name\":[12],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"statins\"],\"name\":[13],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"diabetes\"],\"name\":[14],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"dmpills\"],\"name\":[15],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"insulin\"],\"name\":[16],\"type\":[\"dbl+lbl\"],\"align\":[\"right\"]},{\"label\":[\"weight\"],\"name\":[17],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BMI\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"waist\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"WHR\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"glucose\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"weight1\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BMI1\"],\"name\":[23],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"waist1\"],\"name\":[24],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"WHR1\"],\"name\":[25],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"glucose1\"],\"name\":[26],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tchol\"],\"name\":[27],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LDL\"],\"name\":[28],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"HDL\"],\"name\":[29],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"TG\"],\"name\":[30],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tchol1\"],\"name\":[31],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LDL1\"],\"name\":[32],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"HDL1\"],\"name\":[33],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"TG1\"],\"name\":[34],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SBP\"],\"name\":[35],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DBP\"],\"name\":[36],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age10\"],\"name\":[37],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0\",\"2\":\"70\",\"3\":\"2\",\"4\":\"1\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"5\",\"9\":\"3\",\"10\":\"0\",\"11\":\"0\",\"12\":\"1\",\"13\":\"1\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"73.8\",\"18\":\"23.69\",\"19\":\"96.0\",\"20\":\"0.932\",\"21\":\"84\",\"22\":\"73.6\",\"23\":\"23.63\",\"24\":\"93.0\",\"25\":\"0.912\",\"26\":\"94\",\"27\":\"189\",\"28\":\"122.4\",\"29\":\"52\",\"30\":\"73\",\"31\":\"201\",\"32\":\"137.6\",\"33\":\"48\",\"34\":\"77\",\"35\":\"138\",\"36\":\"78\",\"37\":\"7.0\"},{\"1\":\"0\",\"2\":\"62\",\"3\":\"2\",\"4\":\"1\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"1\",\"9\":\"3\",\"10\":\"0\",\"11\":\"1\",\"12\":\"1\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"70.9\",\"18\":\"28.62\",\"19\":\"93.0\",\"20\":\"0.964\",\"21\":\"111\",\"22\":\"73.4\",\"23\":\"28.89\",\"24\":\"95.0\",\"25\":\"0.964\",\"26\":\"78\",\"27\":\"307\",\"28\":\"241.6\",\"29\":\"44\",\"30\":\"107\",\"31\":\"216\",\"32\":\"150.6\",\"33\":\"48\",\"34\":\"87\",\"35\":\"118\",\"36\":\"70\",\"37\":\"6.2\"},{\"1\":\"1\",\"2\":\"69\",\"3\":\"1\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"3\",\"9\":\"3\",\"10\":\"0\",\"11\":\"0\",\"12\":\"1\",\"13\":\"0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"102.0\",\"18\":\"42.51\",\"19\":\"110.2\",\"20\":\"0.782\",\"21\":\"114\",\"22\":\"96.1\",\"23\":\"40.73\",\"24\":\"103.0\",\"25\":\"0.774\",\"26\":\"98\",\"27\":\"254\",\"28\":\"166.2\",\"29\":\"57\",\"30\":\"154\",\"31\":\"254\",\"32\":\"156.0\",\"33\":\"66\",\"34\":\"160\",\"35\":\"134\",\"36\":\"78\",\"37\":\"6.9\"},{\"1\":\"0\",\"2\":\"64\",\"3\":\"1\",\"4\":\"0\",\"5\":\"1\",\"6\":\"1\",\"7\":\"0\",\"8\":\"1\",\"9\":\"3\",\"10\":\"0\",\"11\":\"1\",\"12\":\"1\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"64.4\",\"18\":\"24.39\",\"19\":\"87.0\",\"20\":\"0.877\",\"21\":\"94\",\"22\":\"58.6\",\"23\":\"22.52\",\"24\":\"77.0\",\"25\":\"0.802\",\"26\":\"93\",\"27\":\"204\",\"28\":\"116.2\",\"29\":\"56\",\"30\":\"159\",\"31\":\"207\",\"32\":\"122.6\",\"33\":\"57\",\"34\":\"137\",\"35\":\"152\",\"36\":\"72\",\"37\":\"6.4\"},{\"1\":\"0\",\"2\":\"65\",\"3\":\"1\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"2\",\"9\":\"3\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"57.9\",\"18\":\"21.90\",\"19\":\"77.0\",\"20\":\"0.794\",\"21\":\"101\",\"22\":\"58.9\",\"23\":\"22.28\",\"24\":\"76.5\",\"25\":\"0.757\",\"26\":\"92\",\"27\":\"214\",\"28\":\"150.6\",\"29\":\"42\",\"30\":\"107\",\"31\":\"235\",\"32\":\"172.2\",\"33\":\"35\",\"34\":\"139\",\"35\":\"175\",\"36\":\"95\",\"37\":\"6.5\"},{\"1\":\"1\",\"2\":\"68\",\"3\":\"2\",\"4\":\"1\",\"5\":\"0\",\"6\":\"1\",\"7\":\"0\",\"8\":\"3\",\"9\":\"3\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"60.9\",\"18\":\"29.05\",\"19\":\"96.0\",\"20\":\"1.000\",\"21\":\"116\",\"22\":\"57.7\",\"23\":\"27.52\",\"24\":\"86.0\",\"25\":\"0.910\",\"26\":\"115\",\"27\":\"212\",\"28\":\"137.8\",\"29\":\"52\",\"30\":\"111\",\"31\":\"202\",\"32\":\"126.6\",\"33\":\"53\",\"34\":\"112\",\"35\":\"174\",\"36\":\"98\",\"37\":\"6.8\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\n---\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nn.obs = 100 # we're going to take a small subset of the data to look at; \n# if we took the whole data set, the likelihood function would be hard to \n# graph nicely\n\nlibrary(dplyr)\ndata1 = \n  hers |> \n  filter(\n    diabetes == 0,\n    exercise == 0) |> \n  head(n.obs)\n\nglucose_data = \n  data1 |> \n  pull(glucose)\n\nlibrary(ggplot2)\nlibrary(ggeasy)\nplot1 = \n  data1 |> \n  ggplot(aes(x = glucose)) +\n  geom_histogram(aes(x = glucose, after_stat(density))) +\n  theme_classic() +\n  easy_labs()\n\nprint(plot1)\n```\n\n::: {.cell-output-display}\n![](intro-MLEs_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nLooks somewhat plausibly Gaussian. Good enough for this example!\n\n### Find the MLEs\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nmu_hat = mean(glucose_data)\nsigma_sq_hat = mean((glucose_data - mean(glucose_data))^2)\n\n```\n:::\n\n\n\n\n\n\n\nOur MLEs are:\n\n* $\\hat\\mu = 98.66$\n\n* $\\hat\\sigma^2 = 104.7444$\n\nHere's the estimated distribution, superimposed on our histogram:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nplot1 +\n  geom_function(\n    fun = function(x) dnorm(x, mean = mu_hat, sd = sqrt(sigma_sq_hat)),\n    col = \"red\"\n  )\n```\n\n::: {.cell-output-display}\n![](intro-MLEs_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nLooks like a somewhat decent fit? We could probably do better, but that's for another time.\n\n### Construct the likelihood and log-likelihood functions\n\n::: notes\nit's often computationally more effective to construct the log-likelihood first and then exponentiate it to get the likelihood\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nloglik = function(\n    mu, # I'm assigning default values, which the function will use \n    # unless we tell it otherwise\n    sigma = sd(x), # note that you can define some defaults based on other arguments\n    x = glucose_data, \n    n = length(x)\n)\n{\n  \n  normalizing_constants = -n/2 * log((sigma^2) * 2 * pi) \n  \n  likelihood_kernel = - 1/(2 * sigma^2) * \n    {\n      # I have to do this part in a somewhat complicated way\n      # so that we can pass in vectors of possible values of mu\n      # and get the likelihood for each value;\n      # for the binomial case it's easier\n      sum(x^2) - 2 * sum(x) * mu + n * mu^2\n    }\n  \n  answer = normalizing_constants + likelihood_kernel\n  \n  return(answer)\n  \n}\n\n# `...` means pass any inputs to lik() along to loglik()\nlik = function(...) exp(loglik(...))\n\n```\n:::\n\n\n\n\n\n\n\n### Graph the Likelihood as a function of $\\mu$\n\n(fixing $\\sigma^2$ at $\\hat\\sigma^2 = 104.7444$)\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nggplot() + \n  geom_function(fun = function(x) lik(mu = x, sigma = sigma_sq_hat)) + \n  xlim(mean(glucose_data) + c(-1,1) * sd(glucose_data)) +\n  xlab(\"possible values of mu\") +\n  ylab(\"likelihood\") + \n  geom_vline(xintercept = mean(glucose_data), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](intro-MLEs_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n### Graph the Log-likelihood as a function of $\\mu$\n\n(fixing $\\sigma^2$ at $\\hat\\sigma^2 = 104.7444$)\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nggplot() + \n  geom_function(fun = function(x) loglik(mu = x, sigma = sigma_sq_hat)) + \n  xlim(mean(glucose_data) + c(-1,1) * sd(glucose_data)) +\n  xlab(\"possible values of mu\") +\n  ylab('log(likelihood)') + \n  geom_vline(xintercept = mean(glucose_data), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](intro-MLEs_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n### Likelihood and log-likelihood for $\\sigma$, conditional on $\\mu = \\hat\\mu$:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n\nggplot() + \n  geom_function(fun = function(x) lik(sigma = x, mu = mean(glucose_data))) + \n  xlim(sd(glucose_data) * c(.9,1.1)) + \n  geom_vline(\n    xintercept = sd(glucose_data) * sqrt(n.obs - 1)/sqrt(n.obs), \n    col = \"red\") +\n  xlab(\"possible values for sigma\") +\n  ylab('Likelihood')\n```\n\n::: {.cell-output-display}\n![](intro-MLEs_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\nggplot() + \n  geom_function(\n    fun = function(x) loglik(sigma = x, mu = mean(glucose_data))\n  ) + \n  xlim(sd(glucose_data) * c(0.9, 1.1)) +\n  geom_vline(\n    xintercept = \n      sd(glucose_data) * sqrt(n.obs - 1) / sqrt(n.obs), \n    col = \"red\") +\n  xlab(\"possible values for sigma\") +\n  ylab(\"log(likelihood)\")\n```\n\n::: {.cell-output-display}\n![](intro-MLEs_files/figure-html/unnamed-chunk-31-2.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n### Standard errors by sample size:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nse.mu.hat = function(n, sigma = sd(glucose_data)) sigma/sqrt(n)\nggplot() + \n  geom_function(fun = se.mu.hat) + \n  scale_x_continuous(trans = \"log10\", limits = c(10, 10^5), name = \"Sample size\") +\n  ylab(\"Standard error of mu (mg/dl)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](intro-MLEs_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n### Simulations\n\n#### Create simulation framework\n\nHere's a function that performs a single simulation of a Gaussian modeling analysis:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndo_one_sim = function(\n    n = 100,\n    mu = mean(glucose_data),\n    mu0 = mean(glucose_data) * 0.9,\n    sigma2 = var(glucose_data), \n    return_data = FALSE # if this is set to true, we will create a list() containing both \n    # the analytic results and the vector of simulated data\n)\n{\n  \n  # generate data\n  x = rnorm(n = 100, mean = mu, sd = sqrt(sigma2))\n  \n  # analyze data\n  mu_hat = mean(x)\n  sigmahat = sd(x)\n  se_hat = sigmahat/sqrt(n)\n  confint = mu_hat + c(-1, 1) * se_hat * qt(.975, df = n - 1)\n  tstat = abs(mu_hat - mu0) / se_hat\n  pval = pt(df = n - 1, q = tstat, lower = FALSE) * 2\n  confint_covers = between(mu, confint[1], confint[2])\n  test_rejects = pval < 0.05\n  \n  # if you want spaces, hyphens, or characters in your column names, use \"\", '', or ``:\n  to_return = tibble(\n    \"mu-hat\" = mu_hat,\n    \"sigma-hat\" = sigmahat,\n    \"se_hat\" = se_hat,\n    \"confint_left\" = confint[1],\n    \"confint_right\" = confint[2],\n    \"tstat\" = tstat,\n    \"pval\" = pval, \n    \"confint covers true mu\" = confint_covers,\n    \"test rejects null hypothesis\" = test_rejects\n  )\n  \n  if(return_data)\n  {\n    return(\n      list(\n        data = x, \n        results = to_return))\n  } else\n  {\n    return(to_return)\n  }\n  \n}\n\n```\n:::\n\n\n\n\n\n\n\nLet's see what this function outputs for us:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndo_one_sim()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mu-hat\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma-hat\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"se_hat\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint_left\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint_right\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tstat\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pval\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint covers true mu\"],\"name\":[8],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"test rejects null hypothesis\"],\"name\":[9],\"type\":[\"lgl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"99.1\",\"2\":\"9.564\",\"3\":\"0.9564\",\"4\":\"97.21\",\"5\":\"101\",\"6\":\"10.78\",\"7\":\"2.195e-18\",\"8\":\"TRUE\",\"9\":\"TRUE\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\nLooks good!\n\nNow let's check it against the `t.test()` function from the `stats` package:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nset.seed(1)\nmu = mean(glucose_data)\nmu0 = 80\nsim.output = do_one_sim(mu0 = mu0, return_data = TRUE)\nour_results = \n  sim.output$results |> \n  mutate(source = \"`do_one_sim()`\")\n\nresults_t.test = t.test(sim.output$data, mu = mu0)\n\nresults2 = \n  tibble(\n    source = \"`stats::t.test()`\",\n    \"mu-hat\" = results_t.test$estimate,\n    \"sigma-hat\" = results_t.test$stderr*sqrt(length(sim.output$data)),\n    \"se_hat\" = results_t.test$stderr,\n    confint_left = results_t.test$conf.int[1],\n    confint_right = results_t.test$conf.int[2],\n    tstat = results_t.test$statistic,\n    pval = results_t.test$p.value,\n    \"confint covers true mu\" = between(mu, confint_left, confint_right),\n     `test rejects null hypothesis` = pval < 0.05\n  )\n\ncomparison = \n  bind_rows(\n    our_results,\n    results2\n  ) |> \n  relocate(\n    \"source\",\n    .before = everything()\n  )\n\ncomparison\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"source\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"mu-hat\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma-hat\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"se_hat\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint_left\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint_right\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tstat\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pval\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint covers true mu\"],\"name\":[9],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"test rejects null hypothesis\"],\"name\":[10],\"type\":[\"lgl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"`do_one_sim()`\",\"2\":\"99.78\",\"3\":\"9.239\",\"4\":\"0.9239\",\"5\":\"97.95\",\"6\":\"101.6\",\"7\":\"21.41\",\"8\":\"6.231e-39\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"`stats::t.test()`\",\"2\":\"99.78\",\"3\":\"9.239\",\"4\":\"0.9239\",\"5\":\"97.95\",\"6\":\"101.6\",\"7\":\"21.41\",\"8\":\"6.231e-39\",\"9\":\"TRUE\",\"10\":\"TRUE\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\nLooks like we got it right!\n\n#### Run 1000 simulations\n\nHere's a function that calls the previous function `n_sims` times and summarizes the results:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndo_n_sims = function(\n    n_sims = 1000,\n    ... # this symbol means \"allow additional arguments to be passed on to the `do_sim_once` function\n)\n{\n  \n  sim_results = NULL # we're going to create a \"tibble\" of results, \n  # row by row (slightly different from the hint on the homework)\n  \n  for (i in 1:n_sims)\n  {\n    \n    set.seed(i)\n    \n    current_results = \n      do_one_sim(...) |> # here's where the simulation actually gets run\n      mutate(\n        sim_number = i\n      ) |> \n      relocate(sim_number, .before = everything())\n    \n    sim_results = \n      sim_results |> \n      bind_rows(current_results)\n    \n  }\n  \n  return(sim_results)\n}\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsim_results = do_n_sims(\n  n_sims = 100,\n  mu = mean(glucose_data),\n  sigma2 = var(glucose_data),  \n  n = 100 # this is the number of samples per simulated data set\n)\n\nsim_results |> head(10)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"sim_number\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"mu-hat\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma-hat\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"se_hat\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint_left\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint_right\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tstat\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pval\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"confint covers true mu\"],\"name\":[9],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"test rejects null hypothesis\"],\"name\":[10],\"type\":[\"lgl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"99.78\",\"3\":\"9.239\",\"4\":\"0.9239\",\"5\":\"97.95\",\"6\":\"101.61\",\"7\":\"11.891\",\"8\":\"8.776e-21\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"2\",\"2\":\"98.34\",\"3\":\"11.934\",\"4\":\"1.1934\",\"5\":\"95.98\",\"6\":\"100.71\",\"7\":\"8.003\",\"8\":\"2.368e-12\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"3\",\"2\":\"98.77\",\"3\":\"8.806\",\"4\":\"0.8806\",\"5\":\"97.03\",\"6\":\"100.52\",\"7\":\"11.333\",\"8\":\"1.390e-19\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"4\",\"2\":\"99.65\",\"3\":\"9.400\",\"4\":\"0.9400\",\"5\":\"97.79\",\"6\":\"101.52\",\"7\":\"11.552\",\"8\":\"4.694e-20\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"5\",\"2\":\"98.99\",\"3\":\"9.723\",\"4\":\"0.9723\",\"5\":\"97.06\",\"6\":\"100.91\",\"7\":\"10.482\",\"8\":\"9.765e-18\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"6\",\"2\":\"98.56\",\"3\":\"10.630\",\"4\":\"1.0630\",\"5\":\"96.45\",\"6\":\"100.66\",\"7\":\"9.182\",\"8\":\"6.648e-15\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"7\",\"2\":\"100.09\",\"3\":\"9.862\",\"4\":\"0.9862\",\"5\":\"98.13\",\"6\":\"102.04\",\"7\":\"11.450\",\"8\":\"7.766e-20\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"8\",\"2\":\"97.70\",\"3\":\"11.095\",\"4\":\"1.1095\",\"5\":\"95.50\",\"6\":\"99.90\",\"7\":\"8.028\",\"8\":\"2.096e-12\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"9\",\"2\":\"98.11\",\"3\":\"9.862\",\"4\":\"0.9862\",\"5\":\"96.15\",\"6\":\"100.07\",\"7\":\"9.446\",\"8\":\"1.773e-15\",\"9\":\"TRUE\",\"10\":\"TRUE\"},{\"1\":\"10\",\"2\":\"97.26\",\"3\":\"9.682\",\"4\":\"0.9682\",\"5\":\"95.33\",\"6\":\"99.18\",\"7\":\"8.740\",\"8\":\"6.091e-14\",\"9\":\"TRUE\",\"10\":\"TRUE\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\nThe simulation results are in! Now we have to analyze them. \n\n#### Analyze simulation results\n\nTo do that, we write another function:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsummarize_sim = function(\n    sim_results, \n    mu = mean(glucose_data),\n    sigma2 = var(glucose_data), \n    n = 100)\n{\n  \n  \n  # calculate the true standard error based on the data-generating parameters:\n  `se(mu-hat)` = sqrt(sigma2/n)\n  \n  sim_results |> \n    summarize(\n      `bias[mu-hat]` = mean(`mu-hat`) - mu,\n      `SE(mu-hat)` = sd(`mu-hat`),\n      `bias[SE-hat]` = mean(se_hat) - `se(mu-hat)`,\n      `SE(SE-hat)` = sd(se_hat),\n      coverage = mean(`confint covers true mu`),\n      power = mean(`test rejects null hypothesis`)\n    )\n  \n}\n\n```\n:::\n\n\n\n\n\n\n\nLet's try it out:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsim_summary = summarize_sim(\n  sim_results, \n  mu = mean(glucose_data), \n  # this function needs to know the true parameter values in order to assess bias\n  sigma2 = var(glucose_data), \n  n = 100)\n\nsim_summary\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"bias[mu-hat]\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SE(mu-hat)\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"bias[SE-hat]\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SE(SE-hat)\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"coverage\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"power\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-0.0334\",\"2\":\"0.9992\",\"3\":\"-0.008259\",\"4\":\"0.07517\",\"5\":\"0.98\",\"6\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\nFrom this simulation, we observe that our estimate of $\\mu$, $\\hat\\mu$, has minimal bias,\nand so does our estimate of $SE(\\hat\\mu)$, $\\hat{SE}(\\hat\\mu)$.\n\nThe confidence intervals captured the true value even more often than they were supposed to, and the hypothesis test always rejected the null hypothesis.\n\nI wonder what would happen with a different sample size, a different true $\\mu$ value, or a different $\\sigma^2$ value...\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/table1-1.0/table1_defaults.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}