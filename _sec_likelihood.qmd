{{< include latex-macros/macros.qmd >}}

### The likelihood function

:::{#def-lik-obs}
#### Likelihood of a single observation

Let $X$ be a random variable and let $x$ be $X$'s observed data value.
Let $\p_{\Th}(X=x)$ be a probability model for the distribution of $X$, with parameter vector $\Th$.

Then the **likelihood** of parameter value $\th$, for model $\p_{\Th}(X=x)$ and data $X = x$, is simply the probability of the event $X=x$ given $\Th = \th$:

$$
\ba
\Lik(\theta) &\eqdef \P_{\theta}(X = x)
\ea
$$
:::

---

{{< include _def-lik.qmd >}}

::: callout-note

#### Notation for the likelihood function

The likelihood function can be written as:

- $\Lik(\theta)$
- $\Lik(\vec x;\theta)$
- $\Lik(\theta; \vec x)$
- $\Lik_{\vec x}(\theta)$
- $\Lik_{\theta}(\vec x)$
- $\Lik(\vec x | \theta)$

All of these notations mean the same thing.
The parameter vector $\theta$ is often listed first or solely,
either to emphasize that we are interested in how this function varies with
the parameters, given the data,
or possibly to make the likelihood resemble
the Bayesian posterior probability $\p(\theta | \vx)$,
hinting at the fact that if the prior probability $\p(\theta)$ is uniform
over some finite parameter space,
the posterior probability is proportional to the likelihood:

$$
\ba
\p(\theta | \vx) &= \frac{\p(\vx | \theta)\p(\theta)}{\p(\vx)}
\\
&= \frac{\Lik(\vec x | \theta) \p(\theta)}{\p(\vx)}
\\
&= \Lik(\vec x | \theta) \frac{\p(\theta)}{\p(\vx)}
\ea
$$



:::

::: notes
The likelihood is a function that takes $\theta$ (and implicitly, $\vec X$) as inputs and outputs a single number, the joint probability of $\vec x$ for model $p_\Theta(\vX=\vx)$ with $\Theta = \theta$.
:::

---

:::{#thm-lik-iid}

#### Likelihood of an independent sample

For [mutually independent](probability.qmd#def-indpt) data $X_1, ..., X_n$:

$$\Lik(\vec x|\theta) = \prod_{i=1}^n \p(X_i=x_i|\theta)$$ {#eq-Lik}

:::

---

:::{.proof}

$$
\ba
\Lik(\vec x|\theta)
&\eqdef \p(X_1 = x_1, â€¦,X_n =x_n|\theta)
\\&= \prod_{i=1}^n \p(X_i=x_i|\theta)
\ea
$$
The second equality is by the definition of statistical independence.

:::

---

:::{#def-lik-factor}
#### Likelihood components

Given an $\iid$ dataset $\vec x$, the **likelihood component** or **likelihood factor** of observation $X_i=x_i$ is the marginal likelihood of $X_i=x_i$:

$$\Lik_i(\theta) = \P(X_i=x_i)$$

:::

---

:::{#thm-ds-lik-obs-lik}

For $\iid$ data $\vx \eqdef \x1n$,
the likelihood of the dataset is equal to the product of the observation-specific likelihood factors:

$$\Lik(\theta) = \prodin \Lik_i(\theta)$$
:::

---

{{< include _sec_binary_likelihood_no_covs.qmd >}}
